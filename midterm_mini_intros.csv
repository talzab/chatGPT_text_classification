doc_id,text
student_001,"Since 2005, Youtube has greatly influenced the lifestyle of people in this age as a greater percentage of people are growing up and are surrounded by technology. As a result of a more technology-enhanced lifestyle, it is rare to see a person without a phone or computer which has led to many new pastimes such as watching YouTube videos as a viewer or posting videos on YouTube as a content creator. As a content creator, creating new videos to draw in new subscribers and views is important if you want to keep your channel alive and generate revenue. Thus, our goal in this work is to better understand how channels obtain subscribers and hopefully to provide future content creators information on how to attract new subscribers."
student_002,"In the past years, we have seen a great spike in social media platforms. We have realized the impact of social media through events such as the 2020 Presidential Election. More and more individuals, from toddlers to seniors, are consuming content. With increased demand, there has to be an increase in supply. A specific platform, Youtube, has been key in the way humans receive information. With its incline, it has seen more and more content creators. People take Youtube seriously and even quit their full-time job to become content creators on Youtube. With this in mind, how exactly can creators get popular enough that they can make revenue? Is there a certain genre that appeals to audiences? Over time, are Youtubers seeing a dramatic increase in views? These are some of the questions that can be answered through data."
student_003,"Internet has developed swiftly during the recent decades, and online medias have received more and more recognition. Thus, publishers are getting increasingly curious about improving shares of their articles. Mashable is one of the most popular platforms, as statistics show that the Mashable community is comprised of 25 million social media followers and 45 million monthly unique visitors (as reported on the website https://www.referralcandy.com/blog/mashable-marketing-strategy/). In that case, it would be valuable to understand the issue of share numbers of articles published on Mashable. This work may contribute to those who intend to make their articles more popular."
student_004,"New York City houses roughly 8.5 million people of different ages, backgrounds, and income levels. It is a melting pot of cultures but there is increasingly more worry for the living conditions that people have to endure, especially those in the lower class. There is a clear shortage in available households for citizens, granted that they are expected to spend a considerable amount of income to put a roof over their heads (reported by NYC.gov: https://www1.nyc.gov/site/housing/problem/problem.page). With an increasing amount of money spent on rent, there is a little left to spend on basic needs. Many citizens are desperate to see the next day, which may mean sacrificing health. In particular, nearly half of the city's households are considered poor (explained by the Center of NYC Affairs:http://www.centernyc.org/inequality-poverty). The goal of the study is to gain a deeper understanding of the underlying reasons for poor health conditions in NYC. Relating income level, ethnicity, and borough to self- assessed health may be of help to any person who wants to move into the city by providing a source of information and advice."
student_005,"As the most popular sporting event held every 4 years (as highlighted on Sports News, January 25, 2020: https://www.sportsnewsireland.com/soccer/soccer_world/the-worlds-most-watched-sporting-events/), the World Cup is known for having countries from all over the world compete in a tournament, where one remaining, undefeated, team stands at the end of the competition and is crowned a winner. Each team comprises of players who play different positions and have different purposes, or roles. Our goal is to examine whether the number of minutes a player receives (playtime) is correlated to their position, as well as understand how their playtime influences their performance with respect to the total number of passes they complete."
student_006,"As the world increasingly moves online due to both the progression of technology and, currently, due to the spreading pandemic, more and more people are seeking relationships through online means. OkCupid is one such dating website, and it collects many forms of data about its users. Among many websites and apps, correlations have been made regarding user data to conclude that certain types of people- that is, certain ages, sexes, etc.- are more likely to use one online dating service over another (according to a report by SurveyMonkey: https://www.surveymonkey.com/curiosity/dating-apps-and-sites-are-almost-as-common-as-they-are-disliked/). The existence of these patterns on other dating websites begs the question: does OkCupid have any major correlations in user data between demographics? Our goal in this report is therefore to better explore the user data in search of potential patterns."
student_007,"As technology is rapidly growing, social media platforms are becoming a huge part of an individual’s life. One of the most popular social media platforms visited is YouTube. With our world adapting to a new norm of online life, people all over the world use YouTube as a means of receiving daily news, entertainment, knowledge, listening to music, and many more contents. An important aspect of YouTube is the number of views and subscribers an individual or channel has. This statistical analysis of YouTube categories may be of help to influencers who are looking for a way to impact a larger audience or to those who are looking to successfully create a YouTube account."
student_008,"When insurance companies are determining the premiums, they need to take into consideration of many factors, such as the subscriber's medical history, or age, etc. It is significant that they use these factors to predict the subscriber's medical costs in the future. If they fail to predict the cost and then set the premiums accurately, they may encounter financial difficulties when a low level of premium is matched with a high cost of claims. The aim of our study is to investigate and better understand the relationship between factors like medical history and the total cost of claims of a group of subscribers who were treated for ischemic heart disease. To put it in another way, we would like to know what characteristics can we spot in subscribers that could potentially give rise to a higher total cost of claims, and can these variables also be associated with each other. Our findings could help insurance companies to make better predictions and set adequate premiums in the future. Therefore, insurance companies are more likely to maximize profits and avoid significant losses."
student_009,"According to a study by The Sun, nearly three fourths of children aged between 6 and 17 want to become online video creators. It is also a fact that the largest video streaming platform is YouTube. Founded in 2005, YouTube boasts 2 billion users and 2 billion hours of content consumed on their platform each day. Naturally, it is common for individuals to ponder what it for a YouTube channel to be successful. With this question in mind we will be exploring the lesser known factors surrounding a YouTube channels' success (other than channel specific content or the creator)."
student_010,"Food, water, and shelter are necessary for human survival. However, the idea of shelter is one often neglected in the consideration of social challenges. Housing is a human right, but it also makes up one of the crucial issues facing contemporary society. There is a widening disparity in housing, especially in large cities like New York. As house prices continue to rise the city faces a housing crisis (as presented in the New York Times, June 13, 2021: https://www.nytimes.com/2021/06/13/nyregion/affordable-housing-nyc-mayor.html). People call upon democratically-elected leaders to push forth change, yet affordable and adequate housing is still disappearing, leading to a greater housing imbalance. This form of inequality further has an impact in social mobility since homeownership is a critical step in building generational wealth (as presented in the Washington Post, July 23, 2020: https://www.washingtonpost.com/business/2020/07/23/black-homeownership-gap/). The motivation behind the current work is to facilitate the understanding of causes behind ongoing housing inequality in New York, which aids in the alleviation of and, ultimately, the solution of the housing crisis."
student_011,"Forest Fires are a pervasive and potentially devasting natural disaster than can cause numerous mortalities and extensive financial costs to those whose property is damaged. Therefore, the prediction of fires is an issue of enormous importance, especially with the increasing rates of fires due to the onslaught of Global Warming. Many methods of analysis used to predict the severity of conditions conducive to fires are expensive, especially those having to do with human analysis or imagery (by satellite or infrared). The most efficient methods of fire prediction in terms of timing and price tend to be meteorological. Therefore, we will attempt to determine the level of influence which a few key contributing meteorological factors have on the area that fires burn."
student_012,"The World Cup occurs every four years, which means each player’s statistics are that much more important. The stats of a player for any sport are crucial because that is what differentiates a good player from a bad player. In soccer there are many different positions and each player in that position focuses on different aspects of the game, however, at the end of the day, the team that scores the most goals is the winner. A player is able to showcase their skills when they are in the game longer. In this analysis, we will be focusing on playtime, position played, and shots attempted to better understand how a player can improve their stats."
student_013,"In the current world we live in we constantly are scouring the internet for the slightest bit of interesting content that we can sink our teeth in. For the people who are creating the articles, the more times someone clicks on their article and shares it, the more money the author will make. The goal of this report is to determine what factors cause us to choose an article and share it and what causes us to just pass by. This analysis could be of great help to people who are authors and post articles on the internet because it could allow them to maximize efficiency and gain more shares by focusing on the things that are important to gaining shares."
student_014,"For health insurance companies, an accurate prediction of client's healthcare cost during the claim period is the key to a reasonable premium subscription price. Hence, our goal is to dig out clues for estimation of costs of claims based on medical history provided by the client."
student_015,"With over 2.6 billion active users, Youtube is one of, if not, the most popular video streaming platforms [1]. The rise in popularity of Youtube gives many a new means to make money. Through advertisements, Youtubers can earn around $5 every 1,000 views [2], which can accumulate to a lot of money; for instance, Youtubers with around 1 million subscribers typically earn around $50,000 a month [3], which is almost equal to the average yearly salary in the United States of $54,132 [4]. This lucrative figure shows how crucial viewership is to Youtubers. Our goal is to analyze factors like channel category, location, and age to see if they affect viewership and monetization on Youtube to test its viability as a potential career path for many to earn more money, especially in an era where the Internet is prevalent."
student_016,"Health insurance premiums can be expensive. The total cost of health insurance for subscribers/patients is dependent on different factors in their life. For example, smoking habits and diet, personal information such as age or gender, and the patient's medical history. It is believed that higher-risk patients have higher health insurance premiums due to the additional costs presumably added due to their poorer health conditions and the extra care needed. For people with ischemic (coronary) heart disease, the expenses are drastic. This may include hospital stays, emergency room visits, prescribed drugs, etc. In this study, we are going to try to comprehend the relationship between the Total Cost of Insurance Claims from patients and the reported medical history within the insurance claim period, specifically for subscribers with ischemic (coronary) heart disease."
student_017,"Since its launch in 2005, Youtube has steadily hosted millions of users accessing the platform for long-form video content. Along with the platform's increasing amount of users came the rise of 'Youtubers,' people who dedicate their lives to the production of video content on the site. Youtubers have since been able to generate incomes totaling into the millions of dollars for themselves and have been able to support themselves through ad revenue and sponsored brand content. The job of a Youtuber is highly lucrative and dependent on amassing a loyal and large subscriber base. However, there is no clear-cut methodology to gaining subscribers and youtuber hopefuls are eager to crack the code. The goal of our work is to examine patterns related to subscriber count of popular Youtube channels, and identify any patterns and relationships that might shed light on how Youtubers are able to gain such large amounts of subscribers."
student_018,"Cardiovascular disease is currently the top cause of death globally (as reported for instance in World Health Organization https://www.who.int/en/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)) and is an ongoing area of concern in the medical field. To reduce the out-of-pocket expenditure of such a chronic disease, patients usually choose health insurance premiums. Health insurance companies plan their premiums according to the prediction of subscriber’s potential medical costs in the future. To better plan the premiums based on the patient’s condition, it is essential to thoroughly analyze what are the characteristics associated with higher medical claims with existing data. Thus, our goal in the current work is to better understand the total cost of claims by subscribers of heart disease during the claim period. This work can be a useful reference for health insurance companies and subscribers for the future design of their premiums."
student_019,"Health insurance premiums are decided by insurance companies through predictions about a subscriber’s potential medical costs in the future. These predictions are based on the subscriber’s age, medical history, and lifestyle habits. Hospital stays or visits for ischemic (coronary) heart disease—which is the leading cause of death for both men and women in the United States (as reported by the Centers for Disease Control and Prevention, Sept. 8, 2020: https://www.cdc.gov/heartdisease/facts.htm)—are costly and associated with complications and emergency room visits, etc. Our goal is to better understand the relationship between the medical history of those with ischemic heart disease and total cost of claims they made to their insurance company. This will allow patients with the condition to understand how their premiums will change."
student_020,"Natural disasters have always been a major issue, endangering both humanity and nature. In particular, forest fires bring with them a variety of grave problems. They cause devastating damage to ecosystems, destroying wildlife habitats. Forest fires can also be dangerous to humans, as Portugal reported 39 human deaths from the wildfires of 2003 and 2005. Forest fires also damage the environment, releasing massive amounts of carbon dioxide as well as emissions that are harmful to humans. They even cause major economical damage, due to the property damage and lack of tourism (as reported in 2021 Investopedia report https://www.investopedia.com/how-fire-season-affects-the-economy-5194059). All of these factors emphasize the importance of research on forest fires, specifically what causes them. Our work may help to both prevent and provide quicker responses to forest fires, saving both human life and the environment."
student_021,YouTube is always trying to find out how to maximize their profit through advertisements on videos. Understanding which user demographics to appeal to and where to put ads for them is essential for keeping viewers happy and making money. YouTube conducts research on who is watching what and from where.
student_022,"Diamond, as one of the most expensive consumer goods and collector's items in the human world, has many consumers' favor. And therefore, the pricing of diamonds is a matter of concern for many consumers. Often the slightest difference in a diamond can lead to a large price fluctuation. The goal of our work is to better understand the rules of the diamond pricing market. This work will help consumers understand the psychological value of different grades of diamonds and create a healthier, more reasonable diamond market."
student_023,"The residential environment, as a crucial factor in people's daily life, serves as a powerful indicator of the happiness of citizens. In recent years, as the overall technological and economic strengths of the U.S. developed rapidly, it seems to imply the increased living conditions of citizens. However, in Jacob Riis's 'How the Other Half Lives' (https://mymodernmet.com/jacob-riis-how-the-other-half-lives/), it shows that in New York, the demand for buildings is always so high that even if the environment is poor there'll still be people willing to pay for it. Our work will focus on different factors that are related to the life of New York residents. This study may help people in other cities who want to move to NYC to have a better understanding of the lifestyle in NYC and make the decision."
student_024,"Youtube is a platform that allowed a hobby of taking videos to become a career. People who are invested in this path are called 'creators' because the range of what individual channel does is truly diverse: some are made for educational purposes and others more for pure entertainment or somewhere in between. The increase in upcoming generation's inclination to become a creator compared to more traditional careers is also quite noticeable. In order monetize their channels, creators first need to have at least 1,000 subscribers. [1] Therefore, it is crucial to ask the question of how to become a successful creator with many subscribers. Our goal in this current work is to identify and better understand which factors affect the amount of subscribers to determine a successful path for rising creators."
student_025,"While internet communities and forums allow for the fast exchange of ideas and information, it also provides a free space for posting disturbing, inappropriate content. Multiple reasons are behind this phenomenon. Albeit the effort to discourage anonymity in some of the major only forums, such as Facebook, users can still remain anonymous as they wish. This anonymity allows users to post disturbing content at low or even no cost, and encourages users psychologically to take a more extreme position and make hateful comments in the process of deindividualization. Secondly, the internet is the most effective medium to spread information, as any images can be freely uploaded and viewed by thousands of other users in an internet community. The spread of the inappropriate content may have grave outcomes. It may disturb the viewers and cause discomfort. More importantly, it may frustrate some users and lead to unhealthy online arguments. Therefore, an investigation into the profiles of users who tend to create inappropriate content and the category in which inappropriate content most frequently occurs is needed. A measure of whether moderator shut down is a good strategy is also warranted."
student_026,"ScienceForums.Net is a discussion forum that has been open since 2002 and allows users to communicate with each other via posts and responses, creating a series of posts called a 'thread'. In platforms such as ScienceForums.Net, there is little to no guidance on the type of content that is discussed, nor is there any formal instruction on the conventions and etiquette that are used on the platform. Experienced users in forum sites more or less learn about the forum's culture solely through use and participation over time. This is why it is the moderators job to oversee these posts and threads to ensure that they contain appropriate content, and to intervene if they see that a user has posted something that warrants a deletion of a post. In this study we will be looking at what may make a thread more popular or less popular, especially in regards to posting inappropriate content and its consequences."
student_027,"Over the past decade, YouTube has become a dominant social media platform many users being able to post a variety of videos online, with the potential of reaching internet fame. The website attracts millions of people to the website, and to many, it has become a part of their routine to check the new content out for a brief intermission from their day. However, the beloved site has also found itself many practical uses as well, such as having a space for educational videos to be found, finding highlights of sports games that were missed, or finding out the latest scoops on politics or even just their favorite games. There seems to be an endless list of what users can post and view on the platform, at least within their guidelines, but what are the most profitable things to post? Even more apparent over the recent years, being one of YouTube's many content creators may seem like a viable means of living if an individual, or company, puts in enough effort to make it a full-time job. The study being held is to examine the most effective category of media to produce by continental region, which may guide prospective users to choose a certain category over their time creating content."
student_028,"In the past decade Youtube has become worldwide video streaming and sharing website. Its accessibility and variety has made it a global phenomenon, producing new career aspirations, celebrities and a continuously growing encyclopedia of visual knowledge on a plethora of topics. Since it is a relatively new concept, there is still much to learn about YouTube and its creators. The goal of this research is to explore view count, subscriber count, continent of origin and category and possible relationships between them. Through exploring this, we hope to learn more about Youtube, its creators and how videos succeed on this relatively young platform."
student_029,"To what extent does medical history matter when determining the rate and cost of health insurance premiums for an individual person? Usually when insurance companies set health premiums, they look at a variety of different factors such as age, location, substance usage, etc. [1] The most important one, however, would arguably be one's total medical costs; if an individual's previous medical cost is higher, they should pay more for their health premium as they have a higher chance of needing it. Therefore, our main goal is to figure out what specific factors (if any) have a stronger association with a person's total cost of claims. And if we are unsuccessful in finding the factors, what might be some statistical obstacles in discovering them? That way, perhaps, the total cost of insurance claims can be adjusted according to an individual's financial situation, a notable benefit for families with greater medical conditions yet lacking financial stability."
student_030,"Youtube is one of the most popular social media platforms with millions of daily users actively searching for some of their favorite topics as well as sharing their videos. A growing focus of many channels seems to be increasing their subscriber count. Earning lots of subscribers leads to increased views and unlocks the many benefits of monetization such as placing ads in videos and earning revenue per view. However, subscriber count may seem simple on the surface but is affected by many other variables and also has its own effects on other variables."
student_031,"Football is a game of extreme physical activity, characterized by brute force and flashy athletics. The dynamic nature of the game makes it easy to forget that football is a game of numbers, probabilities, and strategy as much as it is a display of pure athleticism and skill. Much time (on the parts of coaches, betters, and spectators) is dedicated to the study of football statistics, as an understanding of this can be the difference between winning and losing. Therefore the goal is to be able to gain insight into football statistics and make predictions about certain events and outcomes."
student_032,"Nearly everybody on earth wants to feel loved and the majority would like to end up married to the person of their dreams. This has led hundreds of thousands of hopeful romantics to turn to dating apps such as hinge, tinder, and OkCupid. Today we will be investigating some of the factors that could lead to people being single. The relationships we will be investigating through this paper are if there is a relationship between drinking and relationship status, body type and relationship status, and finally average income and relationship status."
student_033,"We live in a world that revolves around technology and video sharing platforms do not stay behind. Youtube is the most visited platform for video sharing on the web since it allows users to upload original content and is an indispensable tool for modern connectivity and online presence/entertainment. This platform allows people to become world wide known youtubers that can make a profit from creative use of the platform which is why it is important to understand trends that help the success of youtubers, such as the relationship between how long an account has been active and how many subscribers it has, how subscribers vary by category and how the amount of videos uploaded relates to subscriber count. This work might help youtube users who have an aspiration for becoming content creators understand and structure a strategy on how they can be more successful as well as current youtubers understand their statistics and online presence."
student_034,The past decade of the NFL has been the decade of the quarterback with the proliferation of the passing game with more and more of a teams offense built around a lethal quarterback with the run game as an afterthought. But how effective is this strategy? With our research we hope to examine and analyze the effectiveness of this proliferation and what impacts it has on the game today.
student_035,"Maintaining the wellbeing of our planet is essential to the sustainability of life and the general future on Earth. Unfortunately, climate change and rising global temperatures have become increasingly urgent problems in our current world, and have amplified the occurrence of forest fires (as reported by the Center for Climate and Energy Solutions, https://www.c2es.org/content/wildfires-and-climate-change/). Climate change has specifically affected factors such as humidity, soil moisture content, and temperature that directly correlate to the occurrence of forest fires. Our goal in this project is to better understand which factors greatly affect the frequency and intensity of forest fire spread, the results of which may help environmentalists and scientists determine the best steps to take to protect our forests from further destruction."
student_036,"The dangers of wildfires have been brought to the public's attention in recent years after thousands of fires worldwide left record-breaking damage to property and loss of life in their wake. According to Portugal Wildfire data from 1980-2005, 2.7 million Hectares of forest was destroyed, killing 21 people in 2003 alone. Demand for a wildfire detection method is needed to predict the likelihood of fires, so the characterization and investigation of different techniques' pros and cons are important consideration when considering the time pressure firefighting teams are under. One such equipment detection service is Meteorological Satellites which use factors such as temperature, wind, and rain to help determine chances of fire in an accessible and relatively inexpensive way. The goal of this work is to see how using patterns in meteorological data combined with data from the Fire Weather Index can best inform the decisions of firefighters."
student_037,"Technology development overtime has always had a large impact on humanity. YouTube in particularly, has managed to have an impact on individuals of all ages, as it has managed to become a source of information for those in need. Specifically, the amount of views received by YouTube channels has become a major benefactor as YouTubers attempt to expand both their wealth and familiarity. This work may be able to inform beginning YouTubers of the most relevant means of expansion."
student_038,"Dating is one of many areas of life that has significantly changed as the world has digitized. One of the largest innovations in the field is the creation of dating apps, which allow people to find potential partners from around the world without even meeting them. Users fill out fields to describe themselves, and others can view that information and reach out to those they are interested in. This raises the question: what criteria do people choose potential partners based on? Many would argue that physical appearance plays a large part in these decisions. This report aims to shed some light on the users of dating apps and the part appearance plays in relationships."
student_039,"Diamond is one of the most valuable gemstones found on Earth. The most familiar uses of diamonds today are as gemstones used for adornment, and as industrial abrasives for cutting hard materials. The markets for gem-grade and industrial-grade diamonds value diamonds differently [1]. Diamonds are recognized as luxury gifts. It is used in engagement rings, necklaces and other luxury items. An intriguing feature of diamonds is that there are many types of diamonds based on carat, cut, clarity, etc. Our goal in this project is to better understand the relationship between these features of diamonds."
student_040,"With the rise of global warming and climate change, the threat of forest fires is more imminent than ever before as it has the potential to devastate the globe both economically and ecologically. From lighting to gender reveal parties sparking the flame, the area of forest fire damage has only increased over the years (as reported by the EPA https://www.epa.gov/climate-indicators/climate-change-indicators-wildfires). The goal of this study is to look into and find patterns of when exactly these forest fires occur. This work has the potential to help us implement measures that can control these fires before they get too large by dispatching and preparing responders before the fire occurs."
student_041,"As we progress further into the Digital Age, more and more of our consumed content is being curated through online platforms. YouTube is one of the biggest said platforms. It then stands to question what kind of content is on YouTube and what is the most popular. Our goal is to understand YouTube statistics to determine what does well on this platform."
student_042,"Forest Fires are an ever-growing problem in today's world with the uncertainty of our climate. This Forest Fire season has been devastating, completely changing the landscape of the west coast of the United States. Forest Fires are a great threat to the entire ecosystem, including humans. Just this year over one million acres were burned and still are burning in California. The remnants of the smoke cloud can be seen millions of miles away by satellites as a blemish on the Earth. We need to act fast to combat the climate change that is igniting the Earth. The number of fires this year is well above the 10-year average (as nifc.gov reports). A continual increase in fires could affect not only the west coast of our country but affect the entire country in the future in terms of, air quality, temperature, and safety. Our goal is to get a better understanding of the conditions causing wildfires. Once we can identify the perfect conditions for wildfires to spread our work can be used to prevent these conditions from occurring."
student_043,"The world cup is a huge sporting event that gathers millions of fans together to support their country. Every four years the world cup is held and varies in location with each country sending forth the best soccer players in their country. The process behind choosing each national team is by looking through individual leagues within that country and finding those who work best together. Usually the stats looked at are: shots attempted, number of passes, total time played, position they play, and chemistry with other teammates. All of these variables are accounted for in making an official national team. After this process your nation will play in a tournament which usually varies on the area in the world you are. This process qualifies those national teams to play in the world cup. Once you qualify for the world cup there are the group stages which are 32 teams and the knockout rounds of quarter finals, semi finals and finals. This project will focus on the positions of the players and their role on the team."
student_044,"Diamonds are always regarded as a precious jeweler as well as a symbol of wealth. However, the quality of natural diamonds can vary significantly. Meanwhile, the cutting techniques and the weight of the diamonds served as scales of evaluation of diamonds as well. In general, the value of diamonds can be affected by their weight, cut, clarity, and color. Different characters influence the price of diamonds in diverged degrees. To better comprehend the process of value assessment, it’s important to know these related factors. This work may be helpful for potential consumers of diamonds to lock down on their targets at cost-efficient prices. The dataset includes a collection of 5000 diamond samples. To be specific, each sample confines a diamond with its price( in $), its weight(in carat), its cut( fair, good, very good, premium, ideal), its color( range from J to D, while J is the worst and D is the best), its clarity( I1=worst, SI1, SI2, VS1, VS2, VVS1, VVS2, IF=best), and its body(percent). The mean price of diamonds in our sample is 3859.478$. The standard deviation of the diamond prices is 3906.035$. The prices of diamonds in our sample vary enormously. The most expansive diamond cost more than 18k, while the lowest price is as low as about 300$. Such a gigantic price difference shows that the variables mentioned about have a decisive influence on the diamond prices Our first question is to generate a distribution pattern of the price of diamonds. As a hypothesis, we predict that most of the prices will be converging on the mean price (3859.478$) To begin with, we introduce a histogram of the prices of diamonds (Figure 1). As observed, the distribution of the prices is a unimodal histogram. The column of 500-999$ contains the most price samples, which is 1159 out of the 5000 samples. The mode of this sample, obviously, is in the range of 500-999$. Thus, approximately 25% of the sample prices are actually below 1000$. The median of sample prices is 2405.500, which means that roughly 50% of the diamonds cost lower than 2400$. Compared with the actual fact, our hypothesis is obviously unprecise. According to what we expected, the histogram should be unimodal, skewed to the left (as we expected the mode equals the mean, which is greater than the median). However, the fact shows that the common diamonds cost lower than we expected. As we mentioned before, the standard deviation is in the sample prices is as large as 3906.035$, which means the difference between prices is drastic. In conclusion, the distribution pattern of the diamond price is unimodal and right-skewed. Our next question is to find out the relationship between the weight of the diamonds (measure in carat) and the prices of them. Intuitively, we hypothesize that the great weights (larger the carats) predict higher values of the diamonds. Thus, the weights and prices should be positively associated. To start, we created a scatterplot to show the potential relationship between the prices and the weights (figure 2). According to the plot, there is a roughly positive association between the prices and the weights. This illustrates a prediction that generally, the larger the weight, the higher the price. To clarify this, we introduced 2 regression lines. The first regression line is linear. The slope of the linear regression line is positive. With the emergence of the line, it is obvious that the higher-priced diamonds fit better on the regression line. The less expensive samples seem not to depend that greatly on their weights. As we mentioned, the number of carats is only one of the scales to evaluate the price of diamonds. To better elaborate on this association, we also introduced the smooth regression line. We set our smooth parameter to 0.66. The smooth regression line performed sightly better than the linear one in predicting the expensive samples. The coefficients of the linear regression function are also listed below: It is obvious that the samples do not fit perfectly on our regression line. Therefore, even though this linear regression line can predict the price relatively well, its coefficient cannot be 1. As we listed below , the coefficient of our linear regression line is 0,.922, which is very close to 1. Thus, a strong positive linear association exists between the weights and the price. Also, the residuals between the real prices with the predicted prices are listed below (figure 3.) Apparently, the residuals in the higher-priced section are relatively smaller than the residuals in the lower-priced section. This indicates that the linear regression linear predicts better when the prices of the samples are indeed high. Our third question is to find out the association between the color of the diamonds and their price. The color of the diamonds also has influences on their prices. As a hypothesis, we speculate that the better color (level closer to” D”) refers to a higher price. To start, we created a boxplot to show the possible association between the prices and the colors (figure 4). As an explanation, we considered that no diamond is perfect. The D level represents the best color, so these diamonds are extremely rare. In that case, their median price can be significantly affected by other factors. For instance, so D level diamonds may not be large enough. However, the D level has more outliers. This means that if those most precious diamonds may be of D level. So, suppose 2 samples are both the same size (large and heavy), ideally cut, same clarity (IF, best), same body, then if one of them is D level, its price will be incredibly higher than the other one because of its multiplied rareness. The fact actually refuted or prediction. On the other hand, we still managed to give a reasonable explanation for this phenomenon. In conclusion, all color levels have similar median prices, but the better levels have more outliers, which represent those most expensive top-levels."
student_045,"As the boundaries of the digital world expand, so too do the opportunities to earn a living by entertaining viewers on social media and video platforms such as TikTok, Instagram, and YouTube. YouTube is currently the most popular online video streaming site for young adults, with new channel registration on the rise each subsequent year. YouTube content creators have the opportunity to earn money by running ads on their videos, with the yearly revenue of the highest-paid creators going well into the eight figures (as reported by Forbes in Dec. 19, 2019: https://www.forbes.com/sites/maddieberg/2019/12/18/the-highest-paid-youtube-stars-of-2019-the-kids-are-killing-it/#8342b8638cd4). YouTube reports that channels generating more than six figures for their creators are on a 40% year-by-year rise (https://www.youtube.com/intl/en-GB/about/press/). Since the amount of money a YouTube channel makes is dependent on its engagement, understanding the insight behind what makes a YouTube channel achieve views can be helpful for budding creators, and perhaps catalyze their success in trekking this relatively new job field."
student_046,"In a world where over half of the population are active Internet users, it is evident that technology has impacted our lives tremendously. In fact, one of the most popular applications used today, Youtube, has over 2 billion users worldwide, according to a report released by Youtube: https://www.youtube.com/about/press/. Lately, there has been a lot of controversy with Youtube’s algorithm. By the term “algorithm,” we mean the process of which certain videos end up on the trending or recommended page. Creators are constantly worried about whether or not their channel will be relevant because of the unusual algorithm, and the viewers complain about what videos Youtube recommends to them (there are more problems than this). According to an article talking about the algorithm (https://www.wsj.com/articles/youtube-created-a-generation-of-young-stars-now-they-are-getting-burned-out-11576762704), this leads to burnout and a lot of distress to the users. With such controversy, our goal is to further examine the Youtube algorithm, seeing what could be affecting engagement, and explore possible relationships between different variables."
student_047,"Since its launch in 2005, Youtube has steadily hosted millions of users accessing the platform for long-form video content. Along with the platform's increasing amount of users came the rise of 'youtubers,' people who dedicate their lives to the production of video content on the site. Youtubers have since been able to generate incomes totaling into the millions of dollars for themselves and have been able to support themselves through ad revenue and sponsored brand content. The job of a youtuber is highly lucrative and dependent on amassing a loyal and large subscriber base. However, there is no clear-cut methodology to gaining subscribers and youtuber hopefuls are eager to crack the code. The goal of our work is to examine patterns related to subscriber count of popular Youtube channels, and identify any patterns and relationships that might shed light on how youtubers are able to gain such large amounts of subscribers."
student_048,"Nowadays, plenty of websites offer users the platform to express various things in video format. Youtube, one famous website with free-watching videos, seemingly becomes a tool to advertise and raise money (as reported in Intuit Mint, Apr. 23, 2021:https://mint.intuit.com/blog/relationships/how-much-do-youtubers-make-5035/). In YouTube, different channels upload different numbers of videos in several types of categories and obtain different amounts of views. In the current research, we aim to provide deeper and more comprehensive information about Youtube, especially the possible connection among videoUploads, categories, and views. This work may act as a helper by enabling users to manage their accounts better."
student_049,"The growth of streaming services and digital media platforms over the past few years has been tremendous, with Youtube being one of the most prominent video-sharing platforms in the world. Youtube is a not only a source of entertainment, but also a resource for those looking to catch up on the latest news or those hoping to learn various math skills (as reported for example in Grand View Research, June 2020: https://www.grandviewresearch.com/industry-analysis/video-streaming-market). The platform provides many opportunities to its users, so much so that people can work full-time as a '“youtuber.” Of course, this is only possible given that the channel is successful to a certain extent. Subscribers heavily contribute to a channel’s monetary growth and channels with many subscribers are generally viewed as successful. Our goal is to better understand Youtube subscribers, such as how they are distributed based on category. This study may provide some insight as to how a channel should operate and what some viable goals could be, specifically if the channel is looking to grow its subscriber count."
student_050,"Heart disease is widely known to be the leading cause of death for people in the United States for decades. More specifically, ischemic heart disease is the most prevalent type of heart disease in the U.S. [1]. An individual's cost of insurance highly depends on how likely they are to experience further complications and how often they're expected to use medical resources offered by their insurance companies. The data will be further analyzed to help determine whether there are any relationships between the total cost of claims of insurance subscribers, their age, number of comorbidities, number of interventions, and their gender."
student_051,"Life in New York City has been romanticized for years. It's chosen as the setting for countless romantic comedies, thrillers, and inspirational coming-of-age films, capturing the hearts of millions of Americans who dream of living there. Who wouldn't love the thought of living amongst dazzling skyscrapers, world-class restaurants, and people of all cultures? While this is the dream for most, the cost of living in NYC is one of the most expensive in the world [1]. With prices on the rise, it brings about the question: what is the reality of living in NYC? Do the living conditions live up to their expectations? Do New Yorkers actually make enough to fund a lavish lifestyle? Who is, and who isn't? This report will seek to answer this question for aspiring New-Yorkers, and provide some insight on the reality of living in the city."
student_052,"Forest fires can easily destroy tens of thousands of homes, businesses, and hectares of land (1 hectare = roughly .47 acres). They are also a threat to wildlife, and obviously, have a drastically negative impact on the overall environment. In 2003, forest fires destroyed 4.6% of Portugal and claimed 21 human lives according to a 2007 report by Morais Cortez. Cortez also reported in 2005, fires destroyed 3.1% of Portugal and claimed 18 human lives. Our research group aims to pinpoint the exact month, weekday, and coordinates where the most frequent fires occurred at Portugal's Montesinho Park given a certain time period (2000-2003). This experiment may provide individuals who regularly visit Montesinho Park with insight as to whether their behavior is negatively impacting their environment over time."
student_053,"Since its creation in 2005, Youtube has emerged as one of the most popular video-sharing platforms. While Youtube provides free videos, 'content creators' have been able to make money through the videos they post by inputting advertisements in their videos. Due to this, many content creators compete for subscribers from the millions of Youtube users. With such a large variety in the types of Youtube videos and content available, the goal in this report is to determine what factors may play a role in gaining subscribers. This report has the potential to help both account owners and Youtube watchers learn about the nature of Youtube subscribers and see what factors have a relationship with the number of subscribers an account may have."
student_054,"Since Youtube was founded in 2005, it has become a great source of income for many content creators. Users have access to multiple monetization features such as advertising, brand sponsorship, etc. Up to today, top Youtube stars make close to $20 million a year. (as detailed in a 2021 Investopedia report, https://www.investopedia.com/ask/answers/012015/how-do-people-make-money-videos-they-upload-youtube.asp) However, making money from Youtube is never quick and easy: most content creators earn far less or nothing at all. Therefore, how to attract a great number of loyal subscribers is what most content creators strive to figure out. Our goal in the current work is to help Youtube users better understand youtube channel statistics, what could make a channel successful, and get answers to how to make a substantial income from Youtube."
student_055,"Heart Disease is a serious condition that affects millions of people every year and is a leading cause of death for adults (1). This disease's effects are broader than a person's health and can be studied regarding the cost of treatment covered by insurance companies. Treatments for heart disease can prove costly, not only for a patient, but also as a strain on a country's economy (2). Our goal is to study specific characteristics of a patient population and how they affect the cost implications of treating heart disease. This study may help insurance companies better understand and predict the effects of this disease to allow for a better allocation of the cost of treatment."
student_056,"Forest fires are an ever increasing issue exacerbated by climate change. As forest fires increase in both frequency and size, they pose more issues to both cities and ecosystems [1]. Fires are burning closer to cities than they have ever before, damaging properties and harming more individuals [2]. Many ecosystems also struggle to recover after being severely damaged by intense forest fires. Studies have shown that rates of regrowth in forests following fires has sharply declined in recent years [3]. One large consequence of climate change in many environments around the world has been an increase of extreme weather, particularly droughts [4]. This study explores the relationships between droughts, forest fires, and other factors to help predict and control forest fires in the future."
student_057,"Now more than ever, people are living longer and healthier lives, however, since the very beginning of time, humans have been plagued with diseases. It is an unavoidable fact that holds true for every generation, therefore, collecting data on diseases, such as heart disease, is a necessity. Heart disease is a leading cause of death in the US and being at the forefront of heart disease research and experimental studies will undoubtedly benefit society. (As stated by the CDC, heart disease was the cause of death in 1/4 deaths. https://www.cdc.gov/heartdisease/facts.htm#:~:text=Heart%20disease%20is%20the%20leading,1%20in%20every%204%20deaths.) By obtaining experimental data from individuals with heart disease, we can then determine many characteristics about variables (In this case, interventions) to see if there is correlation or causation with other variables, namely gender and costs."
student_058,"In this day and age, YouTube is one of if not the biggest media platforms in the world. The viewer base is massive, with pretty much the entire globe having access to any video on the platform, as well as the ability to upload videos themselves. With all this choice, what makes one video more likely to receive more views than another? That is a question our work will try to answer in this report."
student_059,"New York city is notorious for having extremely rich and severely poor populations. The wealth disparity in the city and different boroughs is very apparent. There are many different ways of measuring living conditions, but one of the most important living conditions is access to heat. New York winters can be very cold with temperatures dropping into the low twenties. Heaters are essential to staying warm. Income has a connotation of quality of living, but in reality does it prevent average income problems such as heater breaks? Even further, does income have a relation to ethnicity specifically in New York City? After answering these questions there should be a defined relationship, or whether or not they are related, between income, ethnicity, and heater breaks. This dissection of data may be able to help understand the relationship between wealth and living conditions in New York City, as well as ethnicity."
student_060,"In today’s times, many young people all around the world have began to pursue YouTube as their full-time job in hopes of becoming famous, even if it requires them to relocate and move to a more popular, trending area. The one common, burning question between them all is how does one make a name for themselves on a platform as large and broad as YouTube? With YouTube awarding YouTubers with specific plaques called “YouTube Play Buttons” after hitting a “milestone” number in subscribers, many videographers are motivated and drawn to discovering just what the key is to becoming so well-liked and relevant enough to earn such an award. The goal of this study is to discern and uncover what factors might lead to YouTube accounts gaining a large following on the expansive, saturated platform. While the data being used in this analysis is roughly two years old, it may hold valuable information that can help YouTubers in 2020 to achieve their goal of becoming a top-tier videographer."
student_061,"Health insurance operating costs continue to grow, driven in large part by an increase in medical bills and utilization [1], leading to insurer consolidation and increased costs for consumers [2]. As such, insurers are looking for ways to reduce operating costs in order to stay competitive. This includes targeting patients for earlier interventions for heart disease, which would improve health outcomes and save insurers money. Insurers rely on their own health data to build models of risk for certain health events [3]. This investigation of health insurer data seeks to identify patters between the utilization of certain heart disease treatments and long-term cost related to treatment."
student_062,"As the world becomes more and more dependent on the internet, more and more information is becoming available to people at their fingertips. Gone are the days where people get their news from the paper. Now, most look to social media, or the internet whenever they need information. But not all content on the internet is created equally. Because of this rise in popularity of internet media and information, creators of this media need to understand who and what content is consumed the most, in order to maximize what they are putting out on the internet. The goal of this project is to better understand how content is consumed. This project may be valuable to producers of online content, so that they can ensure that the content they are creating is the most valuable content to their viewers."
student_063,"YouTube is one of the most popular websites in the world. Everyday, millions of people watch videos on YouTube and some YouTube Channels have gain more than ten billion total views. But what factors affect the views of a YouTube Channel? Our goal in the current work is try to understand the views of certain channels. This work may be helpful for people who want to become a YouTuber in the future or new YouTubers who try to raise their total views."
student_064,"As climate change is starting to affect the world and the forests, understanding and predicting forest fire behavior is critical to saving the few forests that are left. One of the key factors in the spread of fires is ISI or Initial Spread Index which, according to the National Wildfire Coordinating Group (NWCG), measures the potential for a fire to spread using windspeed and Fine Fuel Moisture Code (FFMC). By using other more simple factors such as day, temperature, and region in the Montesinho National Park in relation to ISI, scientists may have more insight on the behavior of fire and be able to predict the fires that have a higher potential to spread."
student_065,"Football (or commonly known as soccer in North America), is widely regarded as the most viewed sport in the world with an annual viewership upwards of 4 billion [1]. With such a large fan base and influence, it is common for many individuals to ponder the factors behind the success of their favorite football players. Factors such as shots taken, passes made, and positions could indicate and even predict the success of a football player. These statistics could also prove useful in simplifying decision making for team managers when making player choices. Therefore, identifying the factors and their relationships with the success of football players would be crucial information that could not only support passionate fans, but also teams as a whole. This relationship between a player's statistics and their success in the sport is exactly what we aim to study in this project through the use of the 2010 world cup players dataset."
student_066,"Nutrition has become increasingly emphasized in our lifestyles especially with the rise of different diets. While we have established diets such as vegan, pescatarian, etc. there is still a lot of room for experimentation to find a diet that can work for a certain person. With this in mind, we would like to explore how the nutritional value of different types of food have been affected due to how they were processed. Specifically, we would like to explore the different relationships between different nutrients in relation to the energy it provides. Through this research, we hope to better understand the kinds of food that should or should not be avoided when someone is considering their dietary needs."
student_067,"While NYC is known as one of the greatest cities in the world with a multitude of businesses and illustrious city culture to offer, it is also known for its sky-high prices and small living spaces. However, even outside of the monetary cost of living, the poor living conditions in NYC have been leading to health conditions and even death. With the high demand of housing in NYC and low supply of houses, many places are illegally converted into living spaces (as mentioned in the 2021 CNN article: https://www.cnn.com/2021/09/03/us/new-york-city-flooding-illegal-basement-apartments/index.html). As the article details, the people who generally end up living in these deadly conditions are those who cannot afford other living options. Thus, our goal is to further understand the income and living conditions of people in NYC. This work may be of help to the government for finding the groups of people particularly suffering from the poor living conditions in order to aid them."
student_068,"At present, the online entertainment industry seems to be booming. Ever since sites such as YouTube began hosting domains to post videos in the mid-2000s, people have had an avenue through which to express themselves. But more than that, the existence of these sites has allowed aspiring entrepreneurs to gain profits on their videos. These profits typically come from advertisement revenue, yet it still begs the question: can money made from YouTube videos be considered an effective source of income?"
student_069,"Over the past 30 years income inequality in the United States has steadily increased and is expected to continue growing. From 1968-2018 the top 1% income share has doubled while the poverty rate has held steady (https://inequality.org/facts/income-inequality/). New York City - the most populous city in the country has consistently been one of the states with the greatest gap between rich and poor. Hence, the goal of this paper is to investigate income inequality across New York City. We aim to look at income distribution across the city's residents along with some of the social demographical features that potentially affect income inequality. Understanding income inequality trends within the city can be hugely beneficial to future efforts of tackling systemic issues of income inequality, not only within NYC but also the entire country."
student_070,"Over 100 years ago New York city was a place filled with the richest of the rich and the poorest of the poor. Today we wonder if that is still true. Over the past couple of years many New Yorker have lived in some unpleasant conditions, and recent have struggled during the pandemic because staying in their apartments is uncomfortable (as talked about by the New York Times https://www.nytimes.com/2020/12/15/nyregion/nyc-public-housing-coronavirus.html). Our purpose is to better understand how and if different factors about the people living in different boroughs could affect housing conditions. This information could help to find ways to improve housing conditions across New York."
student_071,"Youtube is perhaps the most popular online video sharing and social media platform in present-day society. Nowadays, a significant number of people from the new generation want to be full-time video bloggers, and youtube is one of their top choices. The purpose of this study is to determine what are the factors that constitutes a popular video blogger on youtube, so that beginner video bloggers can have a better understanding of their careers and figure out what kind of video blogger they want to be."
student_072,"Income can be influenced by a multitude of factors. One of which is relationships; the way people interact, approach one another, and date are all factors that influence lifestyle choices. These lifestyles likely yield a variety of results on income. To learn more about this question, this report will study how one's sex, sexual orientation, and relationship status relate to their income. The data will be taken from the OkCupid platform and help to either validate or invalidate the research hypotheses in this report."
student_073,"Online forums have taken the internet by storm. A new way to communicate with anyone around the globe, you have access to people with similar hobbies and interests, no matter how niche. However, users in online forums are halfway between anonymous and not. Although usernames tie users to a specific identity, with post histories and dates of registration, this identity is relatively weak. In order to keep anonymity, users can create whole new accounts to stir trouble without the consequences. Understanding the relationship between author experience and punishment (closed posts or banned users) is important to make online communities a safer, more welcoming environment."
student_074,"Climate Change, now more than ever, is becoming more evident in society. From unusual weather patterns to uncontrollable forest fires, climate change has effected everyone society in one way or another. This year, forest fires alone burned 6.7 million acres along the west coast(as reported in BBC, Sept. 18,2020 https://www.bbc.com/news/world-us-canada-54180049). This has changed the lives of millions, from jobs being lost to homes being completely demolished. Understanding the conditions of the forests prior to the fires may help firefighters, in the short term, monitor the land to initiate precautions to minimize the damage of future wildfires."
student_075,"Soccer, or football as many call it, has been universally coined as the “The Beautiful Game.” Many components, such as athleticism, tactics, or skill with the ball are involved in the sport. However, one of the aspects which is perhaps most key to the game of soccer is the act of passing the ball from one teammate to another. Total passes, as we will describe this act in the following research, is integral in understanding how the game is played, and can be very indicative of which team is performing well, or which players are doing their jobs correctly. Having a proper understanding of total passes in the game of soccer undoubtedly enhances one’s understanding and interpretation of the events throughout a soccer game. Therefore, the research we will conduct will help us delve deeper into understanding the statistic that is total passes, and thus, the game of soccer."
student_076,"Health insurance is a prevalent problem in our society and has been for years before. Health insurance prices are determined on how at risk you are given your current state in life. Health insurance premiums are very costly and can be a burden on families financially so it is important to take all factors into consideration. The goal of this paper is to help better the understanding of the correlation between cost of claims and the amount of care they receive while in the hospital. In some cases the cost for the procedure may be more than the health insurance premium so, this should help people when deciding if or how the current health care plan is benefiting you as much as possible."
student_077,"New York City is considered by many to be the hub of the world. While being the hub of the world definitely has its perks, there are significant drawbacks as well. Rodents, poor maintenance, and dilapidated buildings plague the city. The dense population leads to large income inequality, and subsequently, vastly different living conditions. Differences in income and living conditions may affect the overall health of New York City's inhabitants, so it is important for us to understand the relationships between these variables, in order to make New York City a healthier place for all."
student_078,"As a cultural and financial center of the world, New York City is one of the most famous cities in the United States. Under such circumstances, it has a high percentage of renters, and its housing is costly though many of the buildings are old and have undergone renovations and staff turnover. Therefore, New York housing rents and housing conditions are always in the spotlight, which is essential for enormous New York residents and governments to ensure social well-being and stability of macro-economic markets. Our goal in the current work is thus to understand better the relationships between housing conditions with various demographic characteristics, including ethnic, income, and borough. This work may be helpful for governments to understand the current living conditions of New York residents and implement relevant policies to improve social welfare for minorities."
student_079,"With over 8.4 million residents, New York City is one of the largest metropolitan areas in the world. The city also encompasses a significant portion of immigrants, with over 3.1 million as of 2018, which is approximately 37% of the population: (https://www1.nyc.gov/assets/immigrants/downloads/pdf/moia_annual_report_201final.pdf). Despite the allure of the city, however, New York also possesses a deep-rooted economic disparity among its constituents. According to a 2018 study by Inequality.org, the top 1 percent of people in New York occupy more than 40 percent of the city's total income. Furthermore, the study revealed that one in five people in New York are officially considered poor: (https://inequality.org/great-divide/new-york-new-york-less-wonderful-town/). Minority groups in particular, specifically African-Americans and Latinos, are among the majority of lower-income households in New York. A 2017 study from fiscalpolicy.org states that 63% of African-American and 70% of Latino people are in the bottom half of household incomes: (http://www.fiscalpolicy.org/wp-content/uploads/2017/03/Racial-Dimension-of-Income-Inequality.pdf). Our goal is to understand if there is an underlying correlation between ethnicity and income in New York's five regions. Based on this, we can see if there is also a relationship between the above variables and overall quality of living."
student_080,"Jacob Riis was a famous muckraking journalist from the 1800s. He was able to photograph the poor living conditions in New York City, which called a lot of attention to urban reform and income inequality. Looking at NYC over a hundred years later we still have an opportunity, using data from The New York City Housing and Vacancy Survey, to looking at living conditions there. In this paper, the goal is to better understand living conditions and patterns in New York City based off of household incomes."
student_081,"New York City, also known as 'The Big Apple', is currently one of the most populated cities in the United States. It is known to be extremely rich in culture as many people with different backgrounds and ethnicities settled down starting from the middle 1900s. Although living in New York City (NYC) has its benefits, it also has its disadvantages, like the outrageous rent and living costs for a small one-bedroom apartment. Even those with a substantially high income are unable to afford high-quality places within NYC. Another main issue in NYC are health and sanitary concerns. NYC is known to have a large population of mice and rats, who live underground near the subways and sewer system. These rodents carry lots of bacteria that are found in the sewer, which can cause further health issues for humans who come in contact with these rats. Additionally, when it rains and the sewer system floods, the sewer water can float upwards into the streets and public waterways, like rivers and lakes. Our goal in this analysis is to investigate and understand how living in NYC can affect one's health, and if it has been an ongoing crisis since the mid-1900s. We will be specifically researching how mice/rats and water leakage are affecting NYC residents' health and if having a higher income can improve their health with the ongoing health crisis in NYC."
student_082,"People around the world enjoy watching and playing sports. Soccer is one such sport that is loved by millions of people around the world. Just in 2018, the final match of the soccer world cup was watched by over 1.1 billion people in just under 90 minutes. (https://www.statista.com/chart/16875/super-bowl-viewership-vs-world-cup-final/). Similarly, soccer is also played by millions around the world; there are over an astounding 21 professional soccer leagues around the world, collectively worth billions of dollars (https://www.reference.com/world-view/many-professional-soccer-leagues-e068e44267ad1904). With all this craze and passion around this sport, this project hopes to uncover how the best soccer players in the world play, to try to uncover what components of the game such as passing, or shooting are most used by which types of players."
student_083,"The 2010 world cup saw at least 909.6 million people tune in to watch part of the finals (according to the 2011 report by ESPN). 32 countries participated in this tournament which took place in South Africa, with all of the best soccer players in the world competing to win their country a championship. In soccer there are 11 field players, each playing a different position. Each position is different and they all work together to both score goals and prevent the other team from scoring. The main position groups are goalkeeper, defenders, midfielders, and attackers. In this project we will be researching whether there is a relationship between position and performance and seeing if there is a relationship between ShotsAttempted and Playtime."
student_084,"Insurance companies have to account for a wide variety of factors in a subscriber when deciding on that subscriber's premiums. While it can never be known how costly a client will be to a company in the future, the company can make statistical predictions based on their client's medical histories when determining premiums. In the case of those who suffer from ischemic heart disease, they are at higher risk of needing hospital visits and medical procedures done and so I will be analyzing compiled data on such patients to determine which factors of their medical history affected their insurance claim amounts and the average expected increases. This data analysis may help insurance companies better evaluate a potential client's premiums based on the factors seen in their past medical history."
student_085,"As sharing articles on social media has become a common method to spread information, many researchers have investigated how to promote readers' intentions to share the articles [1]. For example, researcher Veronika Karnowski examined how sharing performance is affected by the content characteristics and the availability of other articles associated with similar topics on Facebook and Twitter [2]. Thus, in this research paper, we intend to explore the number of shares of articles published on social media. We will investigate the relationship between shares with three variables, content, channel, and daypublished then contribute a better understanding for authors to write the article with higher article views and sharing times and to determine when and where they should publish their articles on."
student_086,"In an increasingly globalized world, different ethnicities are mixing like never before, and New York City (NYC) is one of the largest and most diverse cities in the world. However, this ethnic melting pot is filled with vast disparities in treatment and living conditions. Racism has long been a problem in the United States, and many worry over the effects of structural racism in the American housing system (as discussed in a report from the Center for American Progress on Aug. 7, 2019: https://www.americanprogress.org/issues/race/reports/2019/08/07/472617/ systemic-inequality-displacement-exclusion-segregation/). Specifically, focusing on examining data for a link between maintenance deficiencies and ethnicity can help add to the discussion on how deficiencies in housing may or may not be related to race, and examining the implications of these results can help activists understand what issues to focus on. Similarly, the same circumstances and structural inequalities that create housing problems for minority ethnicities may also be associated with health problems (as described in an article by the Brookings Institute, Oct. 19, 2016: https://www.brookings.edu/research/time-for-justice- tackling-race-inequalities-in-health-and-housing/). Both housing problems and health problems are often associated with income disparity, and so looking at the variation in income based on ethnicity will help to further understand how ethnicity can be correlated with various disparities in life outcomes. The aim of this report is not to establish any causal relationships, but rather to better contextualize these issues through summarizing how diverse NYC truly is (in terms of ethnicity) and by displaying correlations between ethnicity and maintenance deficiencies as well as ethnicity and health to examine if disparities exist and how deep they may lie. This research will be of use to public health officials and activists in deciding what issues to devote time, money, and political will towards."
student_087,"In an increasingly interconnected world in digital communication, YouTube has swiftly amassed popularity and users as the top video sharing platform of our generation. Its appeal is powerful yet simple: anyone and everyone has the ability to build their personal brand via sharing and communicating videos to the world (https://www.elon.edu/u/academics/communications/journal/wp content/uploads/sites/153/2017/06/06_Margaret_Holland.pdf). As a result, everyone from musical artists to home cooks has taken advantage of the platform to spread their message and earn significant monetary compensation as well. This research aims to focus on the most popular YouTube channels around the world (in terms of subscriber count) to understand if any relationships exist between certain factors and channel popularity. The insights of such research may help YouTube or YouTubers predict the popularity and success of a channel/their channel."
student_088,"While large cities in the U.S. tend to have a plethora of professional opportunities, incredibly unique art and architecture, and their own distinct cultural scene, the large size ends up bringing a vast disparity in living conditions for their populations. Access to sustainable housing without maintenance deficiencies is a major concern today especially within cities like New York City, where many poorer city dwellers had to wait out the pandemic in debilitating public housing (as reported in the New York Times, Dec. 15, 2020: https://www.nytimes.com/2020/12/15/nyregion/nyc-public-housing-coronavirus.html). The goal of this examination is to better comprehend maintenance deficiencies in New York City housing. The discussion of these deficiencies will potentially help New York politicians create a comprehensive plan of action in dealing with this current crisis, hopefully benefitting the citizens of New York City."
student_089,"While modern medicine continues to develop at an exponential pace, heart disease remains to be indisputably deadly. The cause of 1 in 4 deaths in the United States (https://www.cdc.gov/heartdisease/facts.htm), cardiovascular diseases kill an estimated 17.9 million people worldwide (https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1). Therefore, companies specialising in health insurance should pay special attention to the predictive measures of heart disease, as if treated early, not only will the chance of survival increase, but the financial burden onto the patient (and as a result the financial burden onto the company) will also decrease. This report will aim to mainly explore the relationship between patients medical history, during their claim period and before, and the total cost of claims made to the insurance company"
student_090,"Given the exponentially growing popularity of virtually consumed entertainment and media, identifying viral potential or content that will consistently reach high numbers of viewers is an increasingly important factor of digital entertainment. Platforms such as YouTube are conduits for such content, and maximizing one's understanding of its algorithm allows advertisers to reach larger populations and creators to gain greater profit from monetized videos. Thus, being able to identify ways to take advantage of algorithms and resultantly reach more people through digital content is key to achieve success in the modern, internet-driven world."
student_091,"In today’s world, there are many situations in which people need to go to court because of injustices. In our justice system, there are many types of civil cases and monetary compensation is often given to plaintiffs in those court cases. However in recent years, there have been complaints regarding the amount of damages awarded in certain civil cases. In some cases, plaintiffs have received large amounts of money while in other cases, plaintiffs have received little. The goal of this examination is to determine whether various factors have an effect on damages awarded."
student_092,"Over the past few decades, the internet has become an increasing pervasive aspect of human life. Amongst the many changes that the technological information age has brought to the world, one important development was the conception of social platforms. Besides interpersonal connections, these platforms have led to the ability to create job connections and new forms of advertisement. YouTube is one such social platform out of which a new way of making money has sprung. Due to video monetization, it is now easy for popular YouTubers to make a livelihood off of the content they create. With over 2 billion active users a month as of May 2019 (according to Business Of Apps in a 2020 report: https://www.businessofapps.com/data/youtube-statistics/), the potential for success through YouTube exposure is high. Our goal in this report is therefore to better explore possible relationships between YouTube’s user data and video success. Because video creation on YouTube is accessible to most people who have a smartphone or computer, it may be of value to YouTube users for us to explore the analytics behind successful YouTubers."
student_093,"Formed only in extreme conditions deep underground, diamonds are one of the most rare and sought after materials in the world due not only to their association with wealth and beauty but also due to their unique physical properties. For these reasons, in addition to a dwindling supply, diamonds command high prices in the market, with some prices ballooning by 10% in the last De Beers auction (as reported on Fortune.com, August 17, 2021: https://fortune.com/2021/06/09/diamond-shortage-de-beers-supply-chain/). Additionally, there are certain metrics which factor into the appraisal of every diamond. Known as the '4 C's,' every diamond is graded based on their carat, clarity, cut, and color (additional elaboration provided at forgejewelryworks.com, 2021: https://www.forgejewelryworks.com/determine-diamonds-value/). Therefore, our goal in the current work is to further research the factors which affect the process of assigning monetary value to diamonds. The results of this work might provide insights for the common consumer regarding expectations of diamond quality within certain price ranges as well as the possible formation of certain ideal archetypes of affordable diamonds."
student_094,"With the advent of social media and the constant technological advancements being made in that field, it is crucial to note what content the public is consuming. Youtube, one of the biggest social media outlets with over '500 hours of content being uploaded' every minute' (https://www.tubefilter.com/2019/05/07/number-hours-video-uploaded-to-youtube-per-minute/). My goal is to better understand which categories are consumed the most. This will help determine which creators are making the most revenue and how we can use their influence for the better."
student_095,"With the increase access to technology and the advent of social media, many superstars have appeared in society. Many of these people have created a career for themselves by posting content, which range from video games to lifestyle videos (vlogs). This has inspired many people to do the same, sparking furious competition among social media websites. In such a competitive market, every advantage helps, thus bringing rise to the importance of finding ways to receive more interest from the public. This analysis will seek correlations in order to better understand the interests of the population base."
student_096,"Beginning as a simple video sharing site in 2005, YouTube has grown to become a platform where creative individuals can share their talents with over two billion people who use the site, which is 1/3 of the people who use the internet (as reported, for instance, on the YouTube about page https://www.youtube.com/about/press/). When someone chooses to subscribe to a channel, they are stating that they enjoy the creator’s content and wish to become a part of their community. Our goal in this analysis is to better understand the number of subscribers and how it varies between various channels. This investigation could help creators figure out what successful communities have in common and what the number of subscribers they have mean in the context of the entire platform."
student_097,"Over the past decade, YouTube has become a dominant social media platform many users being able to post a variety of videos online, with the potential of reaching internet fame. The website attracts millions of people to the website, and to many, it has become a part of their routine to check the new content out for a brief intermission from their day. However, the beloved site has also found itself many practical uses as well, such as having a space for educational videos to be found, finding highlights of sports games that were missed, or finding out the latest scoops on politics or even just their favorite games. There seems to be an endless list of what users can post and view on the platform, at least within their guidelines, but what are the most profitable things to post? Even more apparent over the recent years, being one of YouTube's many content creators may seem like a viable means of living if an individual, or company, puts in enough effort to make it a full-time job. The study being held is to examine the most effective category of media to produce by continental region, which may guide prospective users to choose a certain category over their time creating content."
student_098,"Youtube is one of the largest platforms in the world for video distribution. It has grown to a point where creators can make entire careers off of their videos. The key factor in a Youtube channel’s success is the amount of subscribers that it has. In fact, a channel’s quality and general success is judged and measured by how many subscribers it has. Our goal in this study is to determine what factors contribute to the high or low subscriber count of a channel. This work can help future creators in what kind of content to make to best help their odds of being successful on the platform."
student_099,"Sports can be the center of many debates and controversies. Some people debate what sport is the most difficult, which takes the most skill, or which requires the strongest athletes. There are also questions within each sport: like who is the greatest player of all time or which position is hardest to play. In soccer, there are 4 main positions: Goalkeeper, defender, midfielder, and forward. Each position has different roles and responsibilities, so they can be hard to compare. However, we can use statistical evidence from the World Cup in 2010 to draw some conclusions. Many sports debates are a matter of opinion, but they become much more relevant if you use some form of statistics as support. Our overall research question is what are the similarities and differences between the various soccer positions."
student_100,"Diamonds are the most popular gemstones that are used as not only luxury gifts but also for industrial purposes. (Article about Diamond from Geology.com) The use of diamonds vary because of its hardness; on the Mohs hardness scale, Diamonds exceed its hardness compared to other natural materials such as Topaz or Corundum. Although modern-day scientists discovered ways to make artificial diamonds, the real diamonds are still more preferred. Diamonds are now significant in our lives, and their prices are increasing day by day. Our goal in this project is to have a better understanding of the worth of diamonds. This work may hopefully help consumers, the economy, and diamond companies by understanding relationships between price and other variables."
chatgpt_001,"Employee engagement is crucial for organizational success, productivity, and employee well-being. This research project focuses on examining the influence of workplace well-being initiatives on employee engagement. By analyzing employee engagement surveys, well-being program participation records, and performance metrics, we will employ significance testing to investigate whether employees who participate in workplace well-being programs exhibit higher levels of engagement and job satisfaction compared to those who do not engage in such initiatives. The findings of this study will provide insights into the effectiveness of well-being programs in fostering employee engagement and guide organizations in designing comprehensive employee well-being strategies."
chatgpt_002,"Social media platforms have revolutionized the way individuals interact and communicate, raising concerns about their impact on interpersonal relationships. This research project aims to explore the relationship between social media usage and perceived social support. By conducting surveys and analyzing data on social media activity and perceived social support levels, we will employ significance testing to investigate whether higher social media usage is associated with lower levels of perceived social support. The findings of this study will contribute to a better understanding of the social dynamics influenced by social media and provide insights into the implications for individual well-being and relationship satisfaction."
chatgpt_003,"Mental health issues have become a significant concern, with a rising prevalence globally. This research project aims to explore the impact of mindfulness-based interventions on mental well-being. By analyzing psychological assessments, mindfulness program participation records, and well-being measures, we will utilize significance testing to investigate whether individuals who undergo mindfulness-based interventions experience improved mental well-being compared to those who do not participate in such interventions. The findings of this study will provide insights into the effectiveness of mindfulness-based approaches in promoting mental well-being and guide mental health practitioners in designing intervention programs."
chatgpt_004,"The rise of online learning platforms has transformed the field of education, offering flexible and accessible learning opportunities. This research project aims to explore the effectiveness of online learning in student outcomes. By analyzing student performance data, satisfaction surveys, and course completion rates, we will utilize significance testing to investigate whether students who engage in online learning experience comparable or higher academic performance compared to those in traditional classroom settings. The findings of this study will provide insights into the effectiveness of online learning and inform educational institutions in designing effective online learning environments."
chatgpt_005,"Mental health issues have become a global concern, affecting individuals across various demographics. This research project aims to explore the impact of mindfulness-based interventions on mental well-being. By analyzing mental health assessments, intervention program participation records, and well-being measures, we will utilize significance testing to investigate whether individuals who engage in mindfulness-based interventions experience improved mental well-being and reduced symptoms of stress, anxiety, and depression compared to those who do not engage in such interventions. The findings of this study will provide insights into the effectiveness of mindfulness approaches in promoting mental well-being and guide mental health practitioners in designing evidence-based interventions."
chatgpt_006,"Artificial intelligence (AI) has emerged as a transformative technology with the potential to revolutionize various industries. This research project focuses on examining the ethical implications of AI in decision-making processes. By analyzing ethical frameworks, decision outcomes, and stakeholder perceptions, we will employ significance testing to investigate whether AI-driven decision-making processes lead to fair and unbiased outcomes compared to traditional human-driven decision-making processes. The findings of this study will provide insights into the ethical considerations of AI adoption and inform policymakers and organizations in developing responsible AI strategies."
chatgpt_007,"Online learning has become increasingly prevalent, particularly in the wake of global events that have disrupted traditional educational systems. This research project focuses on examining the effectiveness of online learning compared to traditional classroom education. By analyzing academic performance data and student satisfaction surveys, we will utilize significance testing to investigate whether there are significant differences in learning outcomes and student engagement between online and traditional classroom settings. The findings of this study will provide valuable insights into the strengths and limitations of online learning and inform educational institutions in designing effective hybrid or fully online learning experiences."
chatgpt_008,"Consumer behavior is influenced by various factors, including marketing techniques and psychological triggers. This research project focuses on exploring the impact of color on consumer purchasing decisions. By conducting experiments and analyzing data on consumer preferences and buying patterns, we will utilize significance testing to investigate whether color influences consumer perceptions, product evaluations, and purchase intentions. The findings of this study will provide insights into the role of color in marketing strategies and guide businesses in effectively utilizing color to influence consumer behavior."
chatgpt_009,"The tourism industry relies heavily on destination attractiveness and visitor experiences. This research project focuses on examining the influence of cultural heritage preservation on tourist satisfaction and destination loyalty. By analyzing tourist surveys, cultural heritage preservation efforts, and repeat visitation rates, we will employ significance testing to investigate whether destinations with well-preserved cultural heritage sites demonstrate higher levels of tourist satisfaction and loyalty compared to those with less emphasis on cultural preservation. The findings of this study will provide insights into the importance of cultural heritage conservation in tourism and inform destination management strategies."
chatgpt_010,"Aging populations present unique challenges for healthcare systems, requiring a comprehensive understanding of geriatric health and well-being. This research project focuses on exploring the relationship between physical activity and cognitive function in older adults. By analyzing activity tracker data, cognitive assessments, and health records, we will utilize significance testing to investigate whether higher levels of physical activity are associated with better cognitive function among older adults. The findings of this study will provide insights into the potential benefits of physical activity on cognitive aging and inform interventions aimed at promoting healthy aging."
chatgpt_011,"Workplace stress is a prevalent concern that can impact employee well-being and productivity. This research project aims to explore the effects of workplace mindfulness programs on stress reduction. By analyzing employee stress assessments, program participation records, and job performance metrics, we will utilize significance testing to investigate whether employees who engage in workplace mindfulness programs experience lower levels of stress and improved job performance compared to those who do not participate in such programs. The findings of this study will provide insights into the effectiveness of mindfulness interventions in managing workplace stress and inform organizations in promoting employee well-being."
chatgpt_012,"Sports performance is influenced by various factors, including physical fitness, psychological well-being, and training methods. This research project focuses on examining the relationship between mindfulness training and athletic performance. By analyzing performance data, psychological assessments, and mindfulness training records, we will employ significance testing to investigate whether athletes who undergo mindfulness training demonstrate improved performance compared to those who do not engage in such training. The findings of this study will provide insights into the potential benefits of mindfulness in sports performance and inform coaches and athletes in incorporating mindfulness techniques into their training routines."
chatgpt_013,"The chosen topic for this project revolves around the impact of social media usage on mental health among adolescents. With the widespread use of social media platforms among the younger generation, concerns have been raised regarding its potential effects on psychological well-being. This research aims to explore the relationship between social media engagement and mental health outcomes, specifically focusing on variables such as self-esteem, anxiety, and depression. By conducting significance tests on relevant data, we seek to provide empirical evidence to either support or refute the hypothesis that increased social media usage is associated with negative mental health outcomes. Understanding these dynamics can contribute to the development of strategies and interventions to promote healthier social media habits and safeguard adolescent mental well-being."
chatgpt_014,"Quality education is crucial for individual development and societal progress. This research project aims to explore the relationship between teacher-student interactions and academic achievement. By analyzing classroom observations, student performance data, and student surveys, we will utilize significance testing to investigate whether positive teacher-student interactions are associated with higher levels of academic achievement. The findings of this study will provide insights into the importance of positive teacher-student relationships in education and inform educators and policymakers in designing effective teaching strategies that foster student success."
chatgpt_015,"Aging population and the associated healthcare challenges have spurred interest in exploring innovative solutions to support healthy aging. This research project focuses on examining the effects of technology-based interventions on the well-being and independence of older adults. By analyzing health outcomes, intervention usage data, and quality of life measures, we will employ significance testing to investigate whether older adults who engage in technology-based interventions experience improved well-being and greater independence compared to those without access to such interventions. The findings of this study will provide insights into the potential of technology in promoting healthy aging and guide the development of tailored interventions for older populations."
chatgpt_016,"Psychological well-being and mental health are essential aspects of overall health and functioning. This research project aims to explore the impact of physical exercise on mental well-being. By analyzing mental health assessments, exercise patterns, and well-being measures, we will utilize significance testing to investigate whether individuals who engage in regular physical exercise experience better mental well-being compared to those with a sedentary lifestyle. The findings of this study will provide insights into the relationship between exercise and mental health and inform individuals and healthcare professionals in promoting physical activity as a means of enhancing mental well-being."
chatgpt_017,"The transportation sector is a significant contributor to greenhouse gas emissions and air pollution. This research project focuses on examining the impact of electric vehicles (EVs) on environmental sustainability. By analyzing emissions data, EV adoption rates, and energy consumption patterns, we will employ significance testing to investigate whether the widespread adoption of EVs leads to reductions in carbon emissions compared to conventional vehicles. The findings of this study will provide insights into the environmental benefits of EVs and inform policymakers and stakeholders in promoting sustainable transportation alternatives."
chatgpt_018,"The concept of emotional intelligence has gained recognition as a crucial skill for effective leadership and interpersonal relationships. This research project aims to explore the impact of emotional intelligence training on leadership effectiveness. By analyzing leadership assessments, training participation records, and subordinate feedback, we will utilize significance testing to investigate whether leaders who undergo emotional intelligence training exhibit higher levels of leadership effectiveness, employee satisfaction, and team performance compared to those who do not engage in such training. The findings of this study will provide insights into the benefits of emotional intelligence development for leaders and guide organizations in implementing leadership development programs."
chatgpt_019,"Political polarization has become increasingly prevalent, with potential implications for democratic processes and societal cohesion. This research project focuses on examining the impact of political echo chambers on public opinion formation. By analyzing social media data, survey responses, and political affiliation information, we will utilize significance testing to investigate whether exposure to echo chambers influences political attitudes and polarization. The findings of this study will provide insights into the role of social media in shaping political discourse and inform strategies to foster diverse and inclusive public dialogue."
chatgpt_020,"Climate change poses significant challenges to agricultural productivity and food security. This research project focuses on examining the impact of climate-smart agriculture practices on crop resilience. By analyzing agricultural data, climate variables, and yield records, we will employ significance testing to investigate whether the adoption of climate-smart agricultural practices leads to improved crop resilience and productivity compared to conventional farming methods. The findings of this study will provide insights into the effectiveness of climate-smart agriculture and inform farmers and policymakers in adapting agricultural practices to mitigate the effects of climate change."
chatgpt_021,"Social inequality is a complex and pervasive issue that affects various aspects of society. This research project aims to examine the relationship between income inequality and social mobility. By analyzing income distribution data, education attainment levels, and social mobility indices, we will employ significance testing to investigate whether higher levels of income inequality are associated with lower rates of social mobility. The findings of this study will provide insights into the impact of income inequality on social mobility and inform policymakers in designing interventions that promote equal opportunities and upward mobility for all individuals."
chatgpt_022,"Workplace diversity and inclusion have gained significant attention in the quest for creating equitable and inclusive organizations. This research project focuses on examining the impact of diversity in leadership on organizational performance. By analyzing organizational performance metrics, leadership diversity data, and employee feedback, we will utilize significance testing to investigate whether companies with diverse leadership teams exhibit better financial performance and higher employee satisfaction compared to those with less diverse leadership. The findings of this study will provide insights into the benefits of diverse leadership and guide organizations in fostering inclusive leadership practices."
chatgpt_023,"Sustainable energy sources are critical in addressing climate change and reducing reliance on fossil fuels. This research project focuses on the effectiveness of solar energy systems in residential settings. By analyzing energy consumption data and solar panel installation records, we will utilize significance testing to investigate whether households with solar panels exhibit lower energy consumption compared to those without solar panels. The findings of this study will contribute to a better understanding of the impact of solar energy adoption at the household level and inform homeowners, policymakers, and energy providers in promoting renewable energy solutions."
chatgpt_024,"The increasing popularity of ride-sharing services has disrupted traditional transportation systems. This research project focuses on analyzing the impact of ride-sharing platforms on urban mobility patterns. Specifically, we will investigate whether the availability of ride-sharing services has influenced the usage of public transportation and private vehicle ownership. By analyzing transportation data and ride-sharing usage statistics, we will utilize significance testing to examine the relationships between these variables. The findings of this study will contribute to understanding the evolving dynamics of urban transportation and provide insights for policymakers in optimizing transportation infrastructure and services."
chatgpt_025,"Urbanization is a global phenomenon that brings about various social, economic, and environmental challenges. This research project focuses on examining the impact of green spaces on urban residents' well-being. By analyzing survey data, health indicators, and green space accessibility measures, we will utilize significance testing to investigate whether greater access to green spaces is associated with higher levels of well-being among urban dwellers. The findings of this study will provide insights into the importance of urban green spaces for promoting residents' well-being and inform urban planning and design strategies that prioritize green infrastructure."
chatgpt_026,"The rapid growth of e-commerce has transformed the retail industry, influencing consumer behavior and market dynamics. This research project aims to explore the impact of online customer reviews on product sales. By analyzing sales data, online review sentiment analysis, and product ratings, we will utilize significance testing to investigate whether positive online customer reviews are associated with higher product sales compared to negative or no reviews. The findings of this study will provide insights into the role of online customer reviews in shaping consumer purchasing decisions and guide businesses in managing their online reputation and customer engagement."
chatgpt_027,"Introduction: In the era of digital transformation, cybersecurity is of paramount importance to safeguard sensitive information and protect individuals and organizations from cyber threats. This research project aims to explore the effectiveness of cybersecurity awareness campaigns in enhancing cybersecurity practices. By analyzing cybersecurity incident data, campaign reach and impact measurements, and employee cybersecurity knowledge assessments, we will utilize significance testing to investigate whether individuals exposed to cybersecurity awareness campaigns demonstrate better adherence to cybersecurity protocols compared to those who are not exposed to such initiatives. The findings of this study will provide insights into the impact of cybersecurity awareness campaigns and guide organizations in promoting a cyber-secure culture."
chatgpt_028,"Innovation is a key driver of economic growth and competitiveness. This research project aims to explore the factors influencing innovation within organizations. By analyzing innovation metrics, employee surveys, and organizational practices, we will utilize significance testing to investigate whether factors such as organizational culture, leadership support, and employee autonomy have a significant impact on innovation outcomes. The findings of this study will provide insights into the determinants of innovation and guide organizations in fostering a culture of innovation and creativity."
chatgpt_029,"The use of renewable energy sources is crucial for mitigating climate change and achieving sustainable energy systems. This research project aims to explore the relationship between renewable energy adoption and energy affordability. By analyzing energy consumption data, renewable energy installation rates, and energy cost indicators, we will employ significance testing to investigate whether increased renewable energy adoption leads to more affordable energy prices for consumers. The findings of this study will provide insights into the economic implications of renewable energy transition and inform policymakers in designing effective renewable energy policies."
chatgpt_030,"Corporate governance plays a critical role in ensuring transparency, accountability, and ethical practices within organizations. This research project focuses on examining the impact of corporate governance mechanisms on financial performance. By analyzing financial data, governance structure information, and performance indicators, we will utilize significance testing to investigate whether companies with strong corporate governance frameworks demonstrate better financial performance compared to those with weaker governance practices. The findings of this study will provide insights into the relationship between corporate governance and financial outcomes and inform policymakers and organizations in strengthening governance mechanisms."
chatgpt_031,"The gig economy has transformed the employment landscape, offering flexible work arrangements and income opportunities. This research project aims to explore the impact of gig work on job satisfaction and work-life balance. By analyzing gig worker surveys, work-related stress assessments, and income satisfaction measures, we will utilize significance testing to investigate whether gig workers experience higher job satisfaction and improved work-life balance compared to traditional employees. The findings of this study will provide insights into the benefits and challenges of gig work and inform policymakers in developing supportive policies for gig workers."
chatgpt_032,"Customer satisfaction is a key driver of business success, influencing customer loyalty and profitability. This research project aims to examine the relationship between service quality and customer satisfaction in the hospitality industry. By analyzing customer feedback data, service quality evaluations, and customer loyalty metrics, we will employ significance testing to investigate whether higher service quality levels are associated with higher levels of customer satisfaction and loyalty. The findings of this study will provide insights into the importance of service quality in the hospitality sector and guide businesses in delivering exceptional customer experiences."
chatgpt_033,"Mental health disorders among college students have become a growing concern, impacting academic performance and overall well-being. This research project aims to explore the relationship between sleep patterns and mental health outcomes among college students. By analyzing survey data, sleep quality assessments, and mental health measures, we will utilize significance testing to investigate whether sleep disturbances are associated with increased rates of stress, anxiety, and depression. The findings of this study will provide insights into the importance of sleep in mental health and guide universities in implementing interventions that promote healthy sleep habits among students."
chatgpt_034,"Introduction: Employee engagement is essential for organizational success, as it is linked to higher productivity, job satisfaction, and employee retention. This research project focuses on examining the relationship between leadership styles and employee engagement. By analyzing employee survey data, leadership behavior assessments, and organizational performance metrics, we will employ significance testing to investigate whether leadership styles characterized by supportive and empowering behaviors are associated with higher levels of employee engagement compared to autocratic or laissez-faire leadership styles. The findings of this study will provide insights into the impact of leadership on employee engagement and guide organizations in developing effective leadership practices to foster a motivated and engaged workforce."
chatgpt_035,"The concept of corporate social responsibility (CSR) has gained prominence as businesses strive to make a positive impact on society and the environment. This research project focuses on examining the influence of CSR initiatives on consumer purchasing behavior. By analyzing consumer surveys, CSR program implementation records, and purchase data, we will employ significance testing to investigate whether consumers are more likely to support and purchase from companies that engage in CSR activities compared to those that do not prioritize social and environmental responsibility. The findings of this study will provide insights into the consumer response to CSR and guide businesses in integrating responsible practices into their strategies."
chatgpt_036,"Access to clean water is a fundamental human right, yet many communities worldwide face challenges in accessing safe and reliable water sources. This research project aims to explore the impact of water purification technologies on water quality and public health. By analyzing water quality data, health records, and technology adoption rates, we will utilize significance testing to investigate whether the implementation of water purification technologies leads to improved water quality and reduced incidences of waterborne diseases. The findings of this study will provide insights into the effectiveness of water purification interventions and guide policymakers in promoting sustainable water management practices."
chatgpt_037,"Financial inclusion is a critical aspect of economic development and poverty alleviation. This research project aims to explore the impact of microfinance on the financial well-being of individuals in underserved communities. By analyzing financial data, microfinance program participation records, and financial well-being indicators, we will utilize significance testing to investigate whether individuals who have access to microfinance services experience improved financial stability and increased income compared to those without access to such services. The findings of this study will provide insights into the effectiveness of microfinance in promoting financial inclusion and inform policymakers in designing inclusive financial systems."
chatgpt_038,"The fashion industry is known for its rapid trends and ever-changing consumer preferences. This research project focuses on examining the influence of social media on fashion consumption behavior. By analyzing social media engagement data, consumer surveys, and purchase patterns, we will utilize significance testing to investigate whether higher levels of social media exposure and engagement are associated with increased fashion consumption. The findings of this study will provide insights into the role of social media in shaping consumer behavior in the fashion industry and guide marketers in developing effective social media strategies to enhance brand engagement and drive sales."
chatgpt_039,"Green building practices have gained prominence as a means of reducing environmental impact and promoting sustainability in the construction industry. This research project focuses on examining the impact of green building certification on building performance. By analyzing energy consumption data, environmental metrics, and building certifications, we will employ significance testing to investigate whether green-certified buildings demonstrate lower energy consumption and environmental footprint compared to non-certified buildings. The findings of this study will provide insights into the effectiveness of green building practices and inform building owners and policymakers in promoting sustainable construction practices."
chatgpt_040,"Workplace diversity and inclusion are essential for fostering innovation and achieving organizational goals. This research project aims to examine the relationship between diversity management practices and employee job satisfaction. By analyzing employee surveys and diversity program implementation data, we will employ significance testing to explore whether organizations with effective diversity management practices have higher levels of employee job satisfaction. The findings of this study will provide insights into the impact of diversity initiatives on employee well-being and guide organizations in developing inclusive work environments that promote employee satisfaction and retention."
chatgpt_041,"Access to quality healthcare is a fundamental right, and healthcare disparities continue to be a pressing concern. This research project aims to examine the impact of socioeconomic factors on healthcare outcomes. By analyzing a comprehensive dataset encompassing patient demographics, health conditions, and healthcare utilization, we will employ significance testing to explore the associations between socioeconomic variables and healthcare outcomes such as mortality rates, disease prevalence, and access to healthcare services. The findings of this study will contribute to a better understanding of the underlying factors influencing healthcare disparities and can inform policy decisions aimed at reducing inequities in healthcare delivery."
chatgpt_042,"Supply chain management is a critical aspect of business operations, impacting efficiency, cost, and customer satisfaction. This research project focuses on examining the relationship between supply chain visibility and performance. By analyzing supply chain data, performance metrics, and customer feedback, we will employ significance testing to investigate whether higher levels of supply chain visibility are associated with improved supply chain performance and customer satisfaction. The findings of this study will provide insights into the benefits of supply chain visibility and inform businesses in enhancing their supply chain management practices."
chatgpt_043,"The rise of digital streaming platforms has revolutionized the entertainment industry, transforming the way people consume media. This research project aims to examine the impact of streaming services on traditional television viewership. Specifically, we will explore whether the availability of streaming platforms has led to a decline in traditional television viewership. By analyzing viewership data and subscription statistics from both streaming services and traditional television networks, we will utilize significance testing to determine the relationship between these variables. The findings of this study will contribute to a deeper understanding of the changing media landscape and help stakeholders adapt to the evolving preferences of consumers."
chatgpt_044,"The rise of social media has transformed the way companies engage with their customers and manage their brand reputation. This research project aims to explore the impact of social media marketing on brand perception and customer engagement. By analyzing social media metrics, customer sentiment analysis, and brand reputation indicators, we will utilize significance testing to investigate whether companies that actively engage in social media marketing enjoy a positive brand perception and higher levels of customer engagement compared to those with limited social media presence. The findings of this study will provide insights into the effectiveness of social media marketing strategies and inform companies in leveraging social media platforms to enhance their brand image."
chatgpt_045,"Entrepreneurship plays a vital role in driving economic growth and innovation. This research project aims to explore the relationship between entrepreneurial mindset and business success. By analyzing survey data, financial performance metrics, and entrepreneurial behavior indicators, we will employ significance testing to investigate whether individuals with a stronger entrepreneurial mindset exhibit higher levels of business success compared to those with a weaker mindset. The findings of this study will provide insights into the factors influencing entrepreneurial success and guide aspiring entrepreneurs in developing and harnessing their entrepreneurial mindset for achieving business goals."
chatgpt_046,"The tourism industry is a significant contributor to local economies, but it can also have environmental and social impacts. This research project focuses on examining the relationship between sustainable tourism practices and destination attractiveness. By analyzing tourist satisfaction surveys, environmental impact data, and destination popularity metrics, we will utilize significance testing to explore whether destinations that prioritize sustainable tourism practices are perceived as more attractive by tourists. The findings of this study will provide insights into the link between sustainability and destination competitiveness, guiding policymakers and stakeholders in promoting sustainable tourism initiatives for long-term economic and environmental benefits."
chatgpt_047,"Innovation is crucial for the success and competitiveness of organizations in the modern business landscape. This research project focuses on exploring the relationship between organizational culture and innovation. By analyzing survey data, organizational performance metrics, and innovation indicators, we will utilize significance testing to investigate whether organizations with a strong innovation culture exhibit higher levels of innovation output compared to those with a less supportive culture. The findings of this study will provide insights into the role of organizational culture in fostering innovation and guide managers and leaders in creating an environment that encourages and nurtures innovative thinking and practices."
chatgpt_048,"The tourism industry is a significant driver of economic growth and employment opportunities. This research project focuses on examining the impact of cultural tourism on local communities. By analyzing visitor surveys, economic indicators, and community well-being measures, we will employ significance testing to investigate whether cultural tourism initiatives contribute to the economic development and social well-being of host communities. The findings of this study will provide insights into the role of cultural tourism in community empowerment and guide tourism stakeholders in designing sustainable tourism practices that benefit both tourists and local residents."
chatgpt_049,"Sustainable agriculture is crucial for ensuring food security and environmental sustainability. This research project aims to explore the impact of organic farming practices on crop yields and soil health. By analyzing agricultural data, soil quality indicators, and yield records, we will utilize significance testing to investigate whether organic farming methods result in comparable or higher crop yields and improved soil health compared to conventional farming methods. The findings of this study will provide insights into the effectiveness of organic farming practices and inform farmers and policymakers in promoting sustainable agricultural systems."
chatgpt_050,"Financial technology (FinTech) has revolutionized the way financial services are delivered and accessed. This research project focuses on examining the impact of mobile banking on financial inclusion. By analyzing banking data, mobile banking adoption rates, and financial inclusion metrics, we will employ significance testing to investigate whether individuals who utilize mobile banking services have higher levels of financial inclusion compared to those who rely on traditional banking methods. The findings of this study will provide insights into the role of mobile banking in expanding financial access and inform policymakers in promoting inclusive financial systems."
chatgpt_051,"Social entrepreneurship has emerged as a powerful force for addressing social and environmental challenges while creating sustainable business models. This research project aims to explore the impact of social entrepreneurship on community development. By analyzing community indicators, entrepreneurial initiatives, and stakeholder perceptions, we will utilize significance testing to investigate whether communities with a strong presence of social entrepreneurs experience greater social and economic development compared to those without active social entrepreneurship. The findings of this study will provide insights into the transformative potential of social entrepreneurship and inform policymakers and aspiring social entrepreneurs in fostering community-driven solutions."
chatgpt_052,"Mental health in the workplace is an important aspect of employee well-being and organizational success. This research project aims to explore the relationship between workplace stressors and employee mental health outcomes. By conducting surveys and analyzing data on work-related stressors, such as workload, job insecurity, and organizational support, we will employ significance testing to investigate whether higher levels of workplace stressors are associated with increased rates of burnout, anxiety, and depression. The findings of this study will provide insights into the impact of work environments on employee mental health and guide organizations in implementing strategies to promote a healthy and supportive workplace culture."
chatgpt_053,"Remote work has become increasingly prevalent, especially in the wake of global events that have necessitated flexible work arrangements. This research project focuses on examining the impact of remote work on employee productivity and job satisfaction. By analyzing productivity metrics, employee surveys, and work performance data, we will employ significance testing to investigate whether employees who work remotely experience comparable or higher levels of productivity and job satisfaction compared to those in traditional office settings. The findings of this study will provide insights into the benefits and challenges of remote work and guide organizations in optimizing their remote work policies and practices."
chatgpt_054,"The education system plays a vital role in shaping the future of societies. This research project aims to explore the impact of project-based learning on student engagement and academic achievement. By analyzing student performance data, engagement surveys, and project-based learning implementation records, we will utilize significance testing to investigate whether students who participate in project-based learning activities demonstrate higher levels of engagement and academic achievement compared to those in traditional learning environments. The findings of this study will provide insights into the effectiveness"
chatgpt_055,"Financial literacy is crucial for individuals to make informed financial decisions and achieve financial well-being. This research project aims to explore the relationship between financial literacy and retirement planning. By analyzing survey data, retirement savings records, and financial knowledge assessments, we will utilize significance testing to investigate whether individuals with higher levels of financial literacy exhibit better retirement planning behaviors compared to those with lower levels of financial literacy. The findings of this study will provide insights into the importance of financial literacy in retirement preparedness and inform financial education programs and policies."
chatgpt_056,"Sustainable tourism has become a priority in the travel industry, promoting responsible travel practices and minimizing environmental impact. This research project focuses on examining the influence of sustainable tourism initiatives on tourist satisfaction and destination loyalty. By analyzing tourist surveys, sustainability program implementation records, and repeat visitation rates, we will employ significance testing to investigate whether tourists who engage in sustainable tourism practices exhibit higher levels of satisfaction and are more likely to return to the destination compared to those who do not prioritize sustainability. The findings of this study will provide insights into the benefits of sustainable tourism and guide destinations in implementing effective sustainable tourism strategies."
chatgpt_057,"Online social networking platforms have transformed the way individuals connect and interact. This research project focuses on examining the impact of online social networking on interpersonal relationships. By analyzing surveys, online activity data, and relationship satisfaction measures, we will utilize significance testing to investigate whether individuals who engage heavily in online social networking experience different levels of relationship satisfaction and social connectedness compared to those who have limited online social networking activity. The findings of this study will provide insights into the effects of online social networking on interpersonal relationships and guide individuals in maintaining a healthy balance between virtual and offline social interactions."
chatgpt_058,"Environmental conservation and sustainable practices are essential for safeguarding biodiversity and ecosystem health. This research project focuses on examining the effectiveness of conservation strategies in protected areas. By analyzing biodiversity data, conservation management practices, and ecological indicators, we will employ significance testing to investigate whether protected areas with specific conservation measures exhibit higher levels of biodiversity compared to unprotected areas. The findings of this study will provide insights into the effectiveness of conservation efforts and inform policymakers and conservationists in prioritizing strategies that maximize biodiversity conservation."
chatgpt_059,"The rise of remote work has transformed the traditional work environment, bringing both advantages and challenges. This research project aims to explore the relationship between remote work flexibility and employee productivity. By analyzing productivity metrics, employee surveys, and work arrangement data, we will employ significance testing to investigate whether remote work arrangements are associated with higher levels of productivity compared to traditional office-based work. The findings of this study will provide insights into the impact of remote work on employee performance and guide organizations in effectively implementing remote work policies."
chatgpt_060,"Personal finance management is crucial for individuals' financial well-being and stability. This research project focuses on examining the effectiveness of financial literacy programs in improving financial behaviors. By analyzing personal financial data, program participation records, and financial behavior assessments, we will employ significance testing to investigate whether individuals who undergo financial literacy programs demonstrate better financial decision-making and savings habits compared to those without access to such programs. The findings of this study will provide insights into the impact of financial education and inform policymakers in promoting financial literacy initiatives."
chatgpt_061,"Online learning has experienced rapid growth and transformation, particularly in the field of education. This research project focuses on examining the effectiveness of online learning platforms on student outcomes. By analyzing academic performance data, platform usage records, and student feedback, we will utilize significance testing to investigate whether students who engage in online learning demonstrate comparable or higher levels of academic achievement and satisfaction compared to those in traditional classroom settings. The findings of this study will provide insights into the efficacy of online learning platforms and inform educators in designing effective online learning experiences."
chatgpt_062,"Workplace diversity has gained recognition as a driver of innovation, creativity, and organizational performance. This research project aims to explore the impact of diversity management practices on employee engagement and job satisfaction. By analyzing employee surveys, diversity program implementation records, and performance metrics, we will employ significance testing to investigate whether organizations with effective diversity management strategies exhibit higher levels of employee engagement and job satisfaction compared to those with less emphasis on diversity initiatives. The findings of this study will provide insights into the benefits of diversity management and guide organizations in fostering inclusive work environments."
chatgpt_063,"The concept of work-life balance has gained prominence in modern society, as individuals strive to maintain a healthy equilibrium between their professional and personal lives. This research project focuses on examining the effects of flexible work arrangements on employee satisfaction and well-being. By analyzing employee surveys, work performance data, and work-life balance indicators, we will employ significance testing to investigate whether employees who have access to flexible work arrangements experience higher levels of job satisfaction and improved well-being compared to those in traditional fixed-hour work environments. The findings of this study will provide insights into the benefits of flexible work arrangements and guide organizations in implementing effective work-life balance practices."
chatgpt_064,"Health disparities are a pressing issue in healthcare, with marginalized communities often facing unequal access to quality care. This research project focuses on examining the impact of socioeconomic factors on healthcare outcomes. By analyzing patient data, socioeconomic indicators, and health outcome metrics, we will employ significance testing to investigate whether socioeconomic status is associated with differences in healthcare utilization, treatment outcomes, and overall health status. The findings of this study will contribute to a better understanding of health inequalities and inform policymakers in designing interventions that address socioeconomic barriers to equitable healthcare."
chatgpt_065,"Climate change is a pressing global issue with far-reaching consequences. This research project focuses on analyzing the relationship between carbon emissions and renewable energy adoption. The goal is to investigate whether countries with higher renewable energy usage exhibit lower carbon emissions. By examining a comprehensive dataset of carbon emissions and renewable energy production across different nations, we will employ significance testing to explore the statistical association between these variables. The results of this study will provide valuable insights into the effectiveness of renewable energy initiatives in mitigating carbon emissions and inform policymakers in their efforts to combat climate change."
chatgpt_066,"Education plays a critical role in economic and social development. This research project focuses on examining the impact of early childhood education on academic performance. By analyzing academic achievement data, socioeconomic variables, and participation in early childhood education programs, we will utilize significance testing to investigate whether early childhood education is associated with improved academic outcomes. The findings of this study will provide insights into the importance of early education interventions and inform policymakers and educators in designing effective early childhood education programs."
chatgpt_067,"Entrepreneurship plays a vital role in economic growth and job creation. This research project focuses on examining the relationship between entrepreneurial ecosystems and startup success. By analyzing data on startup survival rates, funding sources, and ecosystem characteristics, we will utilize significance testing to explore whether robust entrepreneurial ecosystems contribute to higher startup success rates. The findings of this study will provide insights into the factors that foster a supportive environment for startups and inform policymakers and entrepreneurs in nurturing thriving entrepreneurial ecosystems."
chatgpt_068,"Talent acquisition and retention are key challenges for organizations striving for success and growth. This research project aims to explore the impact of employer branding on attracting and retaining top talent. By analyzing recruitment data, employer branding strategies, and employee turnover rates, we will utilize significance testing to investigate whether companies with strong employer brands experience higher levels of talent attraction and lower turnover rates compared to those with weaker employer branding efforts. The findings of this study will provide insights into the importance of employer branding and guide organizations in developing effective employer branding strategies."
chatgpt_069,"The rise of e-commerce has transformed the retail industry, prompting debates about the future of brick-and-mortar stores. This research project aims to explore the relationship between online shopping behavior and consumer satisfaction. By analyzing survey data, purchase histories, and customer reviews, we will utilize significance testing to investigate whether online shoppers experience similar levels of satisfaction compared to in-store shoppers. The findings of this study will provide insights into the impact of e-commerce on consumer experiences and guide retailers in optimizing their omnichannel strategies to enhance customer satisfaction and loyalty."
chatgpt_070,"Financial literacy is crucial for individuals to make informed decisions about their personal finances. This research project aims to explore the relationship between financial literacy and saving behavior. By conducting a survey and analyzing data on financial knowledge, saving habits, and economic indicators, we will employ significance testing to examine the associations between financial literacy levels and savings rates."
chatgpt_071,"The rise of remote work has brought about a paradigm shift in the way people work and collaborate. This research project aims to explore the effects of remote work on employee job satisfaction and work-life balance. By analyzing employee surveys, work performance metrics, and work-life balance indicators, we will utilize significance testing to investigate whether employees who work remotely experience higher job satisfaction and improved work-life balance compared to those working in traditional office settings. The findings of this study will provide insights into the benefits and challenges of remote work and guide organizations in developing effective remote work policies and practices to enhance employee well-being and productivity."
chatgpt_072,"Workplace diversity and inclusion have gained significant attention for their potential benefits in organizational performance and employee satisfaction. This research project focuses on examining the impact of diversity training programs on employee attitudes and behaviors. By analyzing employee surveys, diversity training effectiveness assessments, and team performance metrics, we will employ significance testing to investigate whether employees who undergo diversity training exhibit more positive attitudes towards diversity, increased cultural competence, and enhanced team performance compared to those without training. The findings of this study will provide insights into the effectiveness of diversity training initiatives and guide organizations in fostering inclusive work environments."
chatgpt_073,"The prevalence of online dating platforms has reshaped the landscape of romantic relationships. This research project focuses on exploring the role of online dating in relationship satisfaction. By analyzing survey data on relationship quality, communication patterns, and online dating usage, we will utilize significance testing to investigate whether individuals who meet their partners through online dating experience similar levels of relationship satisfaction compared to those who meet through traditional means. The findings of this study will provide insights into the impact of online dating on relationship dynamics and inform individuals seeking romantic connections in the digital age."
chatgpt_074,"Social media platforms have revolutionized the way people communicate and interact with one another. This research project focuses on examining the impact of social media usage on mental health and well-being. By analyzing survey data, psychological assessments, and social media usage patterns, we will utilize significance testing to investigate whether higher levels of social media usage are associated with increased levels of stress, anxiety, and depression. The findings of this study will provide insights into the potential effects of social media on mental health and inform individuals and mental health professionals in promoting healthier online behaviors."
chatgpt_075,"Consumer trust is vital for the success of businesses, particularly in the digital age where data privacy and security are paramount concerns. This research project focuses on examining the impact of data privacy practices on consumer trust in online businesses. By analyzing consumer surveys, privacy policy assessments, and online purchasing behavior, we will utilize significance testing to investigate whether businesses with robust data privacy practices enjoy higher levels of consumer trust and are more likely to attract and retain customers compared to those with weaker privacy measures. The findings of this study will provide insights into the importance of data privacy in building consumer trust and guide businesses in developing effective privacy strategies."
chatgpt_076,"Workplace safety is a critical aspect of employee well-being and organizational productivity. This research project aims to examine the relationship between safety climate and occupational injuries. By analyzing incident reports, safety training records, and employee surveys, we will utilize significance testing to investigate whether a positive safety climate is associated with lower rates of occupational injuries and accidents. The findings of this study will provide insights into the impact of safety culture on workplace safety outcomes and guide organizations in developing effective safety management strategies."
chatgpt_077,"Quality management is a crucial aspect of organizational excellence and customer satisfaction. This research project aims to explore the impact of total quality management (TQM) practices on organizational performance. By analyzing performance metrics, TQM implementation records, and customer feedback, we will employ significance testing to investigate whether companies that implement TQM practices demonstrate higher levels of operational efficiency, customer satisfaction, and overall performance compared to those that do not prioritize TQM. The findings of this study will provide insights into the benefits of TQM and guide organizations in implementing effective quality management strategies."
chatgpt_078,"The healthcare industry is continuously evolving with advancements in medical technology. This research project focuses on exploring the impact of telemedicine on patient access to healthcare services. By analyzing patient data, healthcare utilization metrics, and patient satisfaction surveys, we will employ significance testing to investigate whether the implementation of telemedicine services leads to improved access to healthcare and increased patient satisfaction compared to traditional in-person care. The findings of this study will provide insights into the effectiveness of telemedicine in healthcare delivery and inform healthcare providers and policymakers in optimizing telemedicine strategies."
chatgpt_079,"Consumer behavior plays a significant role in shaping market trends and business strategies. This research project aims to explore the influence of social media marketing on consumer purchasing decisions. By analyzing consumer surveys, social media engagement metrics, and purchase behavior records, we will utilize significance testing to investigate whether consumers who are exposed to social media marketing exhibit higher purchase intent and increased brand loyalty compared to those who are not exposed to such marketing efforts. The findings of this study will provide insights into the effectiveness of social media marketing strategies and guide businesses in optimizing their digital marketing campaigns."
chatgpt_080,"Climate change is one of the most pressing challenges facing our planet today. This research project focuses on examining the impact of renewable energy adoption on carbon emissions reduction. By analyzing energy consumption data, renewable energy installation records, and carbon emissions measurements, we will utilize significance testing to investigate whether regions or countries with higher levels of renewable energy adoption demonstrate significant reductions in carbon emissions compared to those with lower adoption rates. The findings of this study will provide insights into the effectiveness of renewable energy in mitigating climate change and guide policymakers in promoting sustainable energy transition."
chatgpt_081,"In recent years, the emergence of e-commerce has transformed the retail landscape, with online shopping becoming increasingly prevalent. This research project aims to investigate the impact of online customer reviews on purchase decisions. Specifically, we will explore the relationship between review ratings, review sentiment, and consumer behavior. By analyzing a diverse dataset of customer reviews and purchase data, we will employ significance testing to determine whether positive or negative reviews have a stronger influence on consumer decision-making. The findings of this study will contribute to a deeper understanding of the role of online reviews in shaping consumer perceptions and inform businesses on how to effectively leverage customer feedback to enhance sales and customer satisfaction."
chatgpt_082,"Financial markets play a crucial role in economic growth and stability. This research project aims to investigate the relationship between investor sentiment and stock market volatility. By analyzing sentiment indicators, market volatility indexes, and historical stock market data, we will employ significance testing to examine whether shifts in investor sentiment correlate with increased stock market volatility. The findings of this study will provide insights into the behavioral aspects of financial markets and their impact on market dynamics, aiding investors and financial institutions in managing risk and making informed investment decisions."
chatgpt_083,"Artificial intelligence (AI) has become increasingly integrated into various aspects of society, including healthcare. This research project aims to investigate the impact of AI applications in healthcare on patient outcomes. By analyzing patient data and AI utilization statistics, we will employ significance testing to explore whether AI-assisted diagnostics or treatments result in improved patient outcomes compared to traditional approaches. The findings of this study will provide valuable insights into the potential benefits and limitations of AI in healthcare and inform healthcare professionals in leveraging AI technologies effectively for improved patient care."
chatgpt_084,"Environmental sustainability is a global priority, and businesses play a critical role in addressing environmental challenges. This research project aims to explore the impact of corporate sustainability practices on brand reputation and consumer perception. By analyzing consumer surveys, sustainability performance data, and brand reputation indicators, we will utilize significance testing to investigate whether companies with strong sustainability practices enjoy a positive brand reputation and are perceived favorably by consumers compared to those with weaker sustainability commitments. The findings of this study will provide insights into the business case for sustainability and guide companies in aligning their strategies with sustainable principles."
chatgpt_085,"Cultural diversity in the workplace has become increasingly important in today's globalized world. This research project aims to examine the influence of diversity and inclusion initiatives on employee performance and innovation. By analyzing employee performance data, innovation metrics, and diversity program implementation records, we will utilize significance testing to investigate whether organizations that prioritize diversity and inclusion demonstrate higher employee performance and increased innovation compared to those with less emphasis on diversity. The findings of this study will provide insights into the business case for diversity and guide organizations in fostering inclusive work environments."
chatgpt_086,"Customer experience has become a competitive differentiator in the business landscape. This research project focuses on examining the impact of personalized marketing on customer satisfaction and loyalty. By analyzing customer feedback, personalized marketing campaigns, and purchase behavior, we will employ significance testing to investigate whether customers who are exposed to personalized marketing initiatives exhibit higher levels of satisfaction and loyalty compared to those who receive generic marketing messages. The findings of this study will provide insights into the effectiveness of personalized marketing strategies and guide businesses in delivering tailored experiences to their customers."
chatgpt_087,"Corporate social responsibility (CSR) has gained prominence as businesses aim to contribute positively to society and the environment. This research project aims to examine the relationship between CSR initiatives and consumer loyalty. By analyzing survey data and CSR reporting, we will employ significance testing to investigate whether consumers exhibit higher levels of loyalty and purchase intentions towards companies with strong CSR practices. The findings of this study will provide insights into the impact of CSR on consumer behavior and inform businesses in implementing effective CSR strategies to enhance customer loyalty."
chatgpt_088,"Climate change poses significant challenges to global food security and agricultural sustainability. This research project aims to investigate the impact of climate variability on crop yields. By analyzing historical climate data, crop production records, and yield statistics, we will utilize significance testing to examine whether climate variability affects crop productivity. The findings of this study will provide insights into the vulnerability of agricultural systems to climate change and inform farmers, policymakers, and researchers in developing strategies to enhance resilience and mitigate the effects of climate variability on food production."
chatgpt_089,"Introduction: Sustainable supply chain management has gained significant attention as businesses strive to minimize environmental impact and ensure ethical practices throughout the supply chain. This research project focuses on examining the impact of sustainable supply chain practices on brand reputation and customer loyalty. By analyzing customer surveys, supply chain sustainability measures, and customer loyalty indicators, we will utilize significance testing to investigate whether companies with sustainable supply chain practices enjoy a positive brand reputation and higher levels of customer loyalty compared to those with less sustainable practices. The findings of this study will provide insights into the business benefits of sustainable supply chain management and guide organizations in adopting and promoting sustainable practices throughout their supply chains."
chatgpt_090,"Introduction: Ethical leadership has gained recognition as a critical factor in organizational success and employee well-being. This research project aims to explore the impact of ethical leadership on employee trust and organizational commitment. By analyzing employee surveys, leadership behavior assessments, and organizational performance indicators, we will utilize significance testing to investigate whether employees who perceive their leaders as ethical exhibit higher levels of trust and organizational commitment"
chatgpt_091,"Introduction: Social media platforms have revolutionized communication and interaction, shaping the way individuals connect and share information. This research project aims to explore the impact of social media usage on mental well-being. By analyzing psychological assessments, social media usage patterns, and well-being measures, we will utilize significance testing to investigate whether individuals who spend more time on social media experience lower levels of mental well-being compared to those who use social media less frequently. The findings of this study will provide insights into the relationship between social media and mental health and guide individuals in adopting healthy social media habits for their well-being."
chatgpt_092,"Workplace diversity and inclusion have become important considerations for organizations aiming to foster a culture of equity and innovation. This research project focuses on examining the relationship between diversity and team performance. By analyzing team performance metrics and demographic data, we will utilize significance testing to investigate whether diverse teams outperform homogeneous teams in terms of creativity, problem-solving, and productivity. The findings of this study will provide insights into the benefits of diversity in the workplace and guide organizations in developing inclusive strategies that promote collaboration and enhance team performance."
chatgpt_093,"Teamwork is essential for achieving organizational goals and fostering a collaborative work environment. This research project aims to explore the factors influencing effective teamwork. By analyzing team performance data, team composition information, and employee surveys, we will employ significance testing to investigate whether factors such as diversity, communication strategies, and leadership support have a significant impact on team performance and cohesion. The findings of this study will provide insights into the determinants of successful teamwork and guide organizations in creating and nurturing high-performing teams."
chatgpt_094,"The phenomenon of fake news has gained significant attention in recent years, with potential implications for public opinion, democracy, and societal trust. This research project aims to explore the impact of fake news on public perception and decision-making. By analyzing survey data and media consumption patterns, we will employ significance testing to examine the relationship between exposure to fake news and changes in attitudes, beliefs, or behavior. The findings of this study will contribute to a deeper understanding of the influence of misinformation in the digital age and inform strategies to combat the spread of fake news and promote media literacy."
chatgpt_095,"Employee engagement is crucial for organizational success, productivity, and job satisfaction. This research project focuses on the relationship between leadership styles and employee engagement. By analyzing survey data from employees across different industries, we aim to explore whether certain leadership styles, such as transformational or transactional leadership, have a significant impact on employee engagement levels. Utilizing significance testing, we will investigate the statistical associations between leadership styles and employee engagement metrics. The findings of this study will provide insights into effective leadership practices that can contribute to fostering a motivated and engaged workforce."
chatgpt_096,"Digital literacy is becoming increasingly important in the digital age, as technology continues to shape various aspects of daily life. This research project focuses on exploring the impact of digital literacy on educational outcomes. By analyzing student performance data, digital literacy assessments, and technology integration measures, we will utilize significance testing to investigate whether students with higher levels of digital literacy demonstrate better academic achievements compared to those with lower levels of digital literacy. The findings of this study will provide insights into the role of digital literacy in education and inform educators in designing effective digital literacy programs."
chatgpt_097,"Artificial intelligence (AI) has revolutionized various industries, including healthcare. This research project aims to explore the impact of AI-driven diagnostic systems on medical accuracy and efficiency. By analyzing medical records, diagnostic outcomes, and physician feedback, we will utilize significance testing to investigate whether AI diagnostic systems demonstrate higher accuracy and efficiency compared to human physicians. The findings of this study will provide insights into the potential of AI in healthcare diagnostics and inform healthcare providers in integrating AI technologies for improved patient care."
chatgpt_098,"Cybersecurity is a critical concern in the digital age, with cyber threats becoming more sophisticated and prevalent. This research project focuses on examining the effectiveness of cybersecurity awareness training in mitigating cyber risks. By analyzing security incident data, training completion rates, and employee knowledge assessments, we will employ significance testing to investigate whether employees who undergo cybersecurity awareness training exhibit lower susceptibility to cyber threats compared to those without training. The findings of this study will provide insights into the impact of cybersecurity training and inform organizations in developing robust cybersecurity awareness programs."
chatgpt_099,"Water scarcity is a pressing global issue that affects various regions and communities. This research project aims to examine the impact of water conservation practices on water availability and sustainability. By analyzing water usage data, conservation program effectiveness, and hydrological indicators, we will employ significance testing to investigate whether the implementation of water conservation practices leads to increased water availability and long-term sustainability. The findings of this study will provide insights into the effectiveness of water conservation strategies and guide policymakers and stakeholders in developing sustainable water management practices."
chatgpt_100,"Climate change has become a pressing global issue, necessitating sustainable energy solutions. This research project focuses on examining the effectiveness of renewable energy adoption in reducing greenhouse gas emissions. By analyzing energy consumption data, emission records, and renewable energy investment indicators, we will employ significance testing to investigate whether regions with higher renewable energy utilization exhibit lower levels of greenhouse gas emissions compared to those heavily reliant on fossil fuels. The findings of this study will provide insights into the environmental benefits of renewable energy and inform policymakers in formulating strategies to mitigate climate change."
published_001,"All subjects that participated in our experiments are housed in accordance with the Austrian Federal Act on the Protection of Animals (Animal Protection Act—TSchG, BGBl. I Nr.118/2004). Furthermore, as the present study was strictly noninvasive and based on behavioural observations, all experiments were classified as nonanimal experiments in accordance with the Austrian Animal Experiments Act (§ 2, Federal Law Gazette No. 501/1989)."
published_002,"The major role of tropical forests in biodiversity and climate change has led the world to search for effective ways to slow deforestation. Many approaches have come in and out of fashion. Strictly protected areas, which prohibit most human activities, were popular in the early days of conservation and remain so today. As an alternative to strict protected areas, Community Forest Management (CFM) emerged in the late 1980s (Hutton et al., 2005). By virtue of involving local forest users in management, CFM is promoted as having the potential to benefit both forests and local livelihoods (Behera, 2009). This potential, however, has been questioned (Behera, 2009) and its evidence base has been found to be weak (Bowler et al., 2012). Although Payments for Ecosystem Services (PES) have become the most recent fashion in efforts to reduce deforestation, CFM remains an important part of the forest management toolkit in many developing countries (Blaikie, 2006). It is also promoted as a means by which PES schemes can be implemented. High quality studies evaluating the effectiveness of CFM are therefore important for shaping future development and investment in approaches to reduce deforestation. We aim to provide robust evidence on effectiveness of CFM at reducing deforestation. Studies investigating the effectiveness of conservation interventions often fail to adequately control for confounding factors that affect both the assignment of interventions and the outcomes of interest (Bowler et al., 2012; Ferraro and Pattanayak, 2006; Joppa and Pfaff, 2010). Recent studies investigating the effectiveness of protected areas at reducing deforestation have made progress in controlling for confounding factors by the use of statistical matching (e.g. Andam et al., 2013, 2008; Carranza et al., 2014; Ferraro et al., 2013). Matching selects comparison areas that have pre-intervention baseline values of confounding factors most similar to intervention area values, and thus makes it possible to control for these confounding factors (Joppa and Pfaff, 2011). However, we know of only one study (Somanathan et al., 2009) that has used matching to investigate the effectiveness of CFM at reducing deforestation. A significant challenge for evaluating the effectiveness of CFM is the large variation in forest management practices and designs within the approach, both among and within countries (Lund et al., 2009). In terms of practices, examples of this variation range from cases where the community has a good understanding of their rights and responsibilities to cases where CFM exists on paper only (Benjamin, 2008; Lund et al., 2009). An example of design variation is that some CFMs allow communities to benefit from commercial use of forest resources within their managed forests while others do not (Persha et al., 2011). Failure to consider this variation compromises the potential for learning about design and implementation factors that promote CFM effectiveness. The Malagasy government legislated CFM in the late 1990s (Raik, 2007) to reduce deforestation and protect the significant part of the world’s biodiversity that is endemic to Madagascar (Le Saout et al., 2013). The number of CFM units increased rapidly and continues to grow (Aubert et al., 2013). Many publications review the institutional and political aspects of Madagascar’s forest decentralization process (Pollini et al., 2014; Pollini and Lassoie, 2011; Raik and Decker, 2007; Rives et al., 2013; Urech et al., 2013), but only a few focus on empirically estimating the performance of CFM in terms of conservation outcomes (CIRAD, 2013; Sommerville et al., 2010; Toillier et al., 2011). None adequately control for factors that may confound impact estimates. Using statistical matching to control for factors that confound impact estimates, we investigate the effectiveness of Madagascar CFM at reducing deforestation between 2000 and 2010. To our knowledge, this is the first national scale study of performance of CFM at delivering conservation outcomes. First, we assess the overall effectiveness of Madagascar’s forest decentralization policy at reducing deforestation by looking at all CFM units across the country. Second, we distinguish and study effectiveness in a subsample of CFM units where we have information to suggest that CFM was implemented on the ground. Finally, we differentiate between CFM that allows and does not allow commercial use of forest resources and study effectiveness conditional on whether CFM permits or prohibits commercial use. Note that we do not consider other important potential outcomes from CFM including impacts, positive or negative, on human welfare."
published_003,"Stroke is the fifth leading cause of death among Americans and a leading cause of major disability. One of the most damaging types of ischemic stroke, an emergent large vessel occlusion (ELVO), occurs when a thrombus impedes blood flow in a major cerebral artery. ELVO patients have a high mortality rate, and survivors are often left with profound deficits in motor and cognitive function (Malhotra et al., 2017). Intravenous administration of tissue plasminogen activator (tPA), the only FDA-approved drug for ischemic stroke, is often ineffective in ELVO patients (Broderick et al., 2013; Ciccone et al., 2013; Kidwell et al., 2013), and others are unable to undergo endovascular thrombectomy (EVT). The permanent intraluminal middle cerebral arterial occlusion (MCAO) model of stroke best represents ELVO without tPA administration or EVT. This model provides the opportunity to discover new treatments to decrease mortality and disability resulting from ELVO. Treating large vessel stroke in rats requires an agent that promotes neuroprotection and reduces peripheral inflammation. Leukemia inhibitory factor (LIF), a cytokine in the IL-6 family, has shown efficacy in animal models for neurological diseases, such as multiple sclerosis and amyotrophic lateral sclerosis. Previous groups demonstrated that LIF reduces neurodegeneration through pro-survival PI3K/Akt signaling and by promoting the development of anti-inflammatory leukocytes (Azari et al., 2001; Butzkueven et al., 2002, 2006; Duluc et al., 2007; Janssens et al., 2015; Slaets et al., 2008; Suzuki et al., 2005). This laboratory has shown that systemic LIF administration is effective in reducing infarct volume and improving functional recovery after MCAO (Rowe et al., 2014). This drug regimen entails an initial dose delivered intravenously starting at 6 h after MCAO followed by additional doses at 24 and 48 h with euthanization at 72 h. This regimen was initiated to be clinically relevant and to encompass the neuroinflammatory-induced neurodegeneration that occurs at 48 h in this model (Leonardo et al., 2010). LIF protects neurons and oligodendrocytes from an ischemic environment through the upregulation of antioxidant enzymes (Davis et al., 2017; Rowe et al., 2014). LIF is also an anti-inflammatory cytokine that promotes the development of an anti-inflammatory phenotype in macrophages and T helper cells (Duluc et al., 2007; Gao et al., 2009; Janssens et al., 2015; Metcalfe et al., 2015). These combined actions lead to improved histological and functional outcomes in this severe stroke model. Neuroprotective signaling begins with LIF binding to the LIF receptor (LIFR), a 190 kDa member of the type 1 cytokine receptor family (Gadina et al., 2001). To confer downstream signaling, LIFR must be localized to the plasma membrane and near other IL-6 cytokine receptor components. Besides LIF, this receptor transduces signals of two other cellular protective proteins, cardiotropin-1 and ciliary neurotrophic factor. This receptor is unusual in that it resides in the nuclei of neuronal cells until an injury occurs (Davis et al., 2017; Gardiner et al., 2002; Zhao et al., 2014). While LIFR plays a crucial role in transducing signals for these protective factors, the molecular and cellular roles of this receptor are not entirely understood. Until now, pre-clinical studies aimed at promoting post-stroke recovery have primarily utilized primarily young male rodents. Unfortunately, results from these studies have not led to success in clinical trials. We have reported favorable results with the administration of LIF as a neuroprotective agent in young male rats (Davis et al., 2017; Konoeda et al., 2010; Rowe et al., 2014). In this study, we utilized the same drug regimen and experimental design using aged (18 mo) rats of both sexes. Our study shows that LIF treatment is less effective at promoting neuroprotection after MCAO in aged rats of both sexes, which coincides with significantly lower LIFR expression in the brain compared to young rats."
published_004,"It is common knowledge that any compound becomes toxic when its exposure exceeds a certain threshold. Nowadays it is also realized that exposure below this threshold induces an adaptive response, often referred to as hormesis. The nature of the adaptive response depends on time. For example, adaptation by activation of protective enzymes provides direct protection and is critical for survival of an acute exposure, while evolutionary adaptation provides long term protection and occurs when there is a long-lasting change in exposure, e.g. the introduction of oxygen in Earth's atmosphere [1]. An elegant example of direct adaptation is the increase of glutathione S-transferase activity by acrolein [2]. Acrolein (propenal) is the smallest α,β-unsaturated aldehyde. It is present in e.g. cigarette smoke and diesel exhaust [3]. In addition, it is formed endogenously e.g. during lipid peroxidation [3]. Due to its soft electrophilic character, acrolein readily adducts soft nucleophilic thiols present in cysteine residues of proteins [4]. The latter accounts for the toxicity of acrolein. In the protection against acrolein and other electrophiles, glutathione (GSH) plays a critical role [5]. The thiol group in GSH spontaneously reacts with these electrophiles. The GSH transferases catalyze this detoxification reaction of GSH, and therefore increase the protection by GSH [6,7]. GSH transferases are present in the cytosol, mitochondria and microsomes. Microsomal GSH transferases (MGST) belong to the MAPEG (membrane-associated proteins in eicosanoid and GSH metabolism) superfamily. They are involved in drug detoxification and play an important role in leukotriene and prostaglandin E synthesis [6,7]. Addition of acrolein to microsomes can increase GSH transferase activity 2–3 fold [2]. This is an acute adaptive response, since acrolein directly enhances its conjugation with GSH and thus its detoxification [2]. It is known that in MGST1 the thiol group of cysteine 49 is adducted by electrophiles, and this adduction increases the activity of the enzyme [6,8]. It has been speculated that in this way MGST becomes activated when it is needed, i.e. when alkylating electrophiles are to be detoxified by GSH-conjugation [2,9]. Preliminary results indicated that acrolein also can inhibit MGST activity. This was seen with relatively high concentrations of acrolein. This prompted us to further elaborate on the effect of acrolein on MGST activity, and to evaluate how this two faced effect of acrolein fits in the cellular response to this toxin, that – by increasing the level of exposure - progresses from protection into adaptation and finally toxicity."
published_005,"Detection of new events within a constantly fluctuating sensory input is a fundamental challenge to organisms in dynamic environments. Hypothesized to underlie this process is a continually-refined internal model of the real-world causes of sensations, made possible by exploiting statistical structure in the sensory input (Dayan, Hinton, Neal, & Zemel, 1995; Friston & Kiebel, 2009; Rubin, Ulanovsky, Nelken, & Tishby, 2016; Winkler, Denham, & Nelken, 2009). Evidence from multiple domains, including speech (Saffran, Aslin, & Newport, 1996), abstract sound sequences (McDermott, Schemitsch, & Simoncelli, 2013; Paavilainen et al., 2013; Saffran, Johnson, Aslin, & Newport, 1999), vision (Turk-Browne, Scholl, Chun, & Johnson, 2009) and motor control (Bestmann et al., 2008) reveals sensitivity to environmental statistics, which in turn influences top-down, expectation-driven perceptual processing. When the organism encounters sensory input that is inconsistent with the established internal model, a ‘surprise’ response is generated (Friston, 2005), promoting a rapid reaction to the associated environmental change. Understanding what aspects of stimuli are ‘surprising’, and how they are processed, is therefore central to understanding this network. The auditory system has been a fertile ground for probing sensory error responses, at multiple levels of the processing hierarchy (Aghamolaei, Zarnowiec, Grimm, & Escera, 2016; Ayala, Pérez-González, & Malmierca, 2016; Nelken, 2014; Parras et al., 2017). A common approach involves using a stream of standard sounds to establish a regularity that is occasionally interrupted by ‘deviant’ sounds (Garrido, Kilner, Stephan, & Friston, 2009, 2008; Heilbron & Chait, 2017; Khouri & Nelken, 2015; Näätänen & Alho, 1995). Deviants usually evoke an increased response relative to that measured for the standards (Garrido et al., 2009; Herrmann, Henry, Fromboluti, McAuley, & Obleser, 2015; Ulanovsky, Las, & Nelken, 2003). Since many of the investigated sequences have been very simple, often a repeated tone; neural adaptation is likely a major contributor to the observed deviant responses (Briley & Krumbholz, 2013; Grill-Spector, Henson, & Martin, 2006; Nelken, 2014). However, accumulating evidence suggests that at least part of the deviant response arises from neural processes associated with computing ‘surprise’ or detecting a mismatch between expected and actual sensory input (Daikhin & Ahissar, 2012; Khouri & Nelken, 2015; Parras et al., 2017; Taaseh, Yaron, & Nelken, 2011). The underlying network, consistently implicated in these processes, is comprised of bilateral auditory cortex (Heschl's Gyrus and superior temporal gyrus) and right inferior frontal gyrus (Barascud, Pearce, Griffiths, Friston, & Chait, 2016; Chennu et al., 2016; Garrido et al., 2009, 2008; Heilbron & Chait, 2017; Opitz, Rinne, Mecklinger, Cramon von, & Schröger, 2002). What information is used in calculating surprise? Mounting evidence suggests that the deviant response is shaped by the statistics of the sequence as it unfolds. Garrido, Sahani, and Dolan (2013) demonstrated that MEG responses to probe tones are sensitive to the statistical context (mean and variance of frequency) of randomly generated tone-pip sequences such that larger responses occurred to the same probe tone when presented in a context with low-variance than with high-variance. Rubin et al. (2016) modelled brain responses to two-tone sequences with different probabilities. They demonstrated, in line with conclusions from Garrido et al. (2013), that trial-wise neural responses in auditory cortex are well explained by the probability of occurrence of each tone frequency, calculated from the recent history of the sequence. The models that best fit neural responses were based on a relatively long stimulus history (∼10 tones); but maintained a coarse representation, reflecting a small set of summary statistics. Most previous work investigating the effect of context on deviant processing has focused on simple, random frequency patterns (Garrido et al., 2013; Herrmann et al., 2015; Khouri & Nelken, 2015). For these signals, a coarse representation, possibly underpinned by adaptation processes (Herrmann et al., 2015; Khouri & Nelken, 2015; May & Tiitinen, 2010), may indeed be sufficient to capture relevant attributes. However, it remains unclear whether the brain also keeps track of a detailed history of past sensory experience. To reveal these processes, the stimulus must contain some structural regularity. Whilst previous research (Koelsch, Gunter, Friederici, & Schröger, 2000; Koelsch, Busch, Jentschke, Rohrmeier, & 2016; Maess, Koelsch, Gunter, & Friederici, 2001; Pearce, Ruiz, Kapasi, Wiggins, & Bhattacharya, 2010; Vaz Pato, Jones, Perez, & Sprague, 2002), investigated complex sequence structure, the experiments mostly involved fixed patterns and exposure over very long durations, likely reflecting long-term structure learning. In contrast, here we focus on structure which emerges anew in each sequence. We seek to understand whether the brain represents this structure, and identify the underlying brain networks. We used fast tone-pip sequences, unique on each trial, that occasionally contained a frequency outlier presented outside of the spectral region occupied by the standards. To determine whether the deviant response merely reflects an unexpected change in frequency between the standards and outlier, or whether it is also affected by the specific order of elements in the sequence, we used as standards either regular (REG) or random (RAND) sequences of otherwise matched frequencies (see Fig. 1), such that the frequency span is identical but the precision of the available information regarding successive frequencies is either low (RAND) or high (REG). Notably, the sound sequences were very rapid (20 tones per second) such that conscious reasoning about the sequence order is unlikely to be possible. Based on the hypothesis that the human brain tracks and evaluates incoming sensory information against the specific pattern established by the sequence context, we expect outlier tones to be more readily detectable in REG than in RAND sequences. The experiments reported below investigate this assertion by measuring deviance-evoked EEG responses in naïve, distracted listeners (Experiment 1) and when listeners actively monitored the sequences for outlier tones (Experiment 2)."
published_006,"Seawater circulation through, and reaction with, the ocean crust is a fundamental Earth process that controls the magmatic accretion of new oceanic crust, modifies the composition of ocean crust and seawater, and through subduction transports surficial geochemical signatures to the mantle. At mid ocean ridges (MOR) hydrothermal circulation is most spectacularly expressed as high temperature (up to 400 °C) vent fluids, but at the ridge axis and ridge flanks, lower temperature (<150 °C) hydrothermal circulation is also important. Because vent fluids provide only limited information on the subsurface, we require improved knowledge on sub-surface fluid flow pathways, and the vigour and longevity of seafloor hydrothermal alteration to further understand magmatic accretion at MOR (e.g., Henstock et al., 1993; Kelemen et al., 1997) and better quantify the influence of seawater-basalt exchange on global chemical cycles (e.g., Vance et al., 2009). To date, much of our knowledge of MOR hydrothermal circulation is reflected by conceptual cartoons developed from active and passive seismic observations (e.g., Kent et al., 1990; Tolstoy et al., 2008), measurements of vent fluid chemistry, thermal models, and core descriptions from ocean crust “reference site” DSDP Hole 504B (Alt, 1995). Unfortunately, observations from ophiolites appear inappropriate for MOR hydrothermal systems (e.g., Alt and Teagle, 2000; Bickle and Teagle, 1992), so key questions remain unresolved principally due to a dearth of direct information from intact ocean crust. For example, many sketches of MOR hydrothermal circulation indicate focused lateral fluid movement along the top of axial magma chambers, but to date there is only limited evidence for such flow. Sheeted sill-type models of ocean crust accretion require deep hydrothermal fluid circulation to remove latent and sensible heat from the lower crust (Maclennan et al., 2004) but the conduits for the downwelling of cool seawater-derived fluid remain elusive. Borehole observations and sampling by scientific ocean drilling is essential to better understand ocean floor hydrothermal systems but the paucity of deep holes into ocean crust limits our knowledge of seafloor basalt-hydrothermal alteration. Here we present a high spatial resolution whole rock and mineral Sr isotope profile for ODP Site 1256, the first borehole to sample a complete section down to gabbros of intact upper ocean crust formed at a fast spreading rate (Teagle et al., 2006; Wilson et al., 2006). Our interpretations from this profile are tightly integrated with detailed petrography and geochemistry of the drill cores (Alt et al., 2010; Teagle et al., 2006; Wilson et al., 2003) and borehole wireline observations (Tominaga et al., 2009; Tominaga and Umino, 2010). We document sub-horizontal and sub-vertical channelling of seawater-derived hydrothermal fluids at different levels in the ocean crust, and develop a temporal model for the evolving hydrothermal system at Site 1256."
published_007,"Erosion drives the transfer of Particulate Organic Carbon (POC) to geological basins, where its long-term preservation is helped by rapid burial and/or anoxic conditions (France-Lanord and Derry, 1997; Galy et al., 2007; Hilton et al., 2015; Walsh et al., 1981). This is a key feature of the global carbon cycle, syphoning large amounts of atmospheric carbon into the lithosphere. Erosion also exposes previously buried petrogenic carbon (OCpetro) at the Earth's surface, promoting the release of CO2 via microbial or chemical degradation (Hemingway et al., 2018; Petsch et al., 2000). For the organic pathway to carbon sequestration to be effective on geological time scales, a net transfer has to occur from the atmosphere-surface system into geological storage. Although this transfer appears to be efficient in mountain belts where short, steep transport lines connect source and sink (Galy et al., 2007; Hilton et al., 2008; Kao et al., 2014), these settings are prone to rapid tectonic recycling, and carbon may reflux into the atmosphere, especially if the OCpetro is biologically-available or prone to physical decay. Graphitization by deep burial renders carbon more recalcitrant, which may limit losses during orogenic recycling (e.g. Galy et al., 2008). Survival rates of OCpetro during single cycles of erosion and deposition have been estimated at between 15% and 85%, with shorter distances between source and sink leading to better preservation (Galy et al., 2008; Hilton et al., 2008; Bouchez et al., 2010). With high survival rates, the ability for OCpetro to persist over multiple cycles of burial and exhumation is possible, but has not been demonstrated. Here, we address this knowledge gap, showing that in the rapidly exhuming mountains of Taiwan, highly graphitized OCpetro can escape degradation during repeated erosional cycles. This raises the prospect of progressive accumulation of POC in geological basins, provided that tectonic burial is sufficiently deep to drive graphitization."
published_008,"Barnacles have been of long-standing scientific interest with detailed work on taxonomy and fossil forms reaching back to Charles Darwin (e.g., Darwin, 1851, 1854). Thoracican barnacles (subclass Cirripedia) first appeared in the Early Paleozoic (Briggs et al., 2005; Pérez-Losada et al., 2008) and developed shells of phosphatic composition (Gale and Schweigert, 2016). This basal clade is represented by the living genus Ibla, which has shell plates composed of a poorly ordered hydrogen-phosphate-like material (Reid et al., 2012). Thoracicans evolved shells of calcite in the Jurassic, and form a monophyletic clade called the Thoracicalcarea Gale (2016), which are abundant in the World’s oceans today. Calcareous barnacles first appeared in the Bathonian (mid-Jurassic), and by the end of the Jurassic (Tithonian) forms referred to the family Zeugmatolepadidae were abundant and widespread, mostly as epifauna attached to ammonite shells and driftwood (Gale, 2014, in press). In the Lower Cretaceous (Aptian) stalked benthonic cirripedes of the extant families Calanticidae and Scalpellidae appeared, which persist in deeper marine habitats to the present day (Gale, 2015). One group of stalked calcareous barnacles became epiplanktonic (lepadomorphs). Calcareous cirripedes first adapted to shallow marine hard substrates in the Late Cretaceous (Gale and Sørensen, 2015) and neobalanomorphs appeared in the Paleogene. These radiated rapidly, and by the Miocene these sessile forms were globally abundant and locally rock-forming in shallow marine habitats. Today they occupy a wide range of habitats from the intertidal zone down to deep sea trenches in the modern oceans (Foster, 1987). Calcareous barnacles secrete their shells onto a chitinous cuticle in a succession of very thin layers in synchronicity with the tidal cycle (Bourget, 1987) or endogenous semi-diurnal rhythm (Bourget and Crisp, 1975). The thin layers are composed of polycrystalline calcite crystals enveloped in organic macromolecules (Gal et al., 2015) and show hierarchical patterns. These hierarchical patterns relate to seasonal changes in growth rate and decreasing band with throughout ontogeny modulating the thickness of growth bands (Bourget and Crisp, 1975). After an initial planktic life stage, barnacles settle in their final larval stage and become sessile (Shanks, 1986), recording environmental conditions at their growth site during their adult life. Particular interest has been focussed on their attachment to various surfaces due to the negative economic impact of barnacle fouling (e.g., Christie and Dalley, 1987; Swain et al., 1998). The shells of barnacles have, however, also found use in ecological and environmental studies: δ13C and δ18O values of turtle and whale barnacles have been employed to trace the movement patterns of their host animals (Killingley, 1980; Killingley and Lutcavage, 1983) and a barnacle oxygen isotope thermometer has been established (Killingley and Newman, 1982). This barnacle thermometer has, for example, been employed in determining habitats of turtle barnacles (Detjen et al., 2015), and deciphering the point of origin of museum specimens (Newman and Killingley, 1985). Additionally, carbon isotope ratios in body tissues of intertidal barnacles have been identified as a potential proxy for habitat elevation with respect to the tidal level (Craven et al., 2008). Element/Ca ratios of barnacle shells have been studied and proposed as tracers of salinity (Gordon et al., 1970) and shore level (Pilkey and Harriss, 1966; Bourget, 1974). Significant inter-specimen differences in shell composition across different localities have additionally been identified as fingerprinting tool to locate harvest points of edible goose barnacles (Albuquerque et al., 2016). Despite such encouraging studies, not much is known about the chemical composition of barnacle shell material, and compilations of barnacle shell chemistry rely on very few samples (Chave, 1954; Dodd, 1967). From the few available data it has been noted that barnacle shell calcite is dissimilar from most commonly studied biogenic calcite as its very high Sr concentrations mean it plots off a proposed correlation line of Sr versus Mg content (Carpenter and Lohmann, 1992). Whether barnacles carry any species-specific element signatures and whether there are differences in element concentrations in their shell plates has, however, not been comprehensively addressed (but see Iglikowska et al., 2018a). Here we present δ13C and δ18O values as well as Mg/Ca, Sr/Ca, Mn/Ca and Fe/Ca ratios of 42 modern barnacle species of the Lepadiformes, Scalpelliformes, Verruciformes and Balaniformes (see Pérez-Losada et al., 2008 for taxonomic scheme), constituting the first large survey of shell geochemistry in cirripedes. These analyses are complemented by XRD analyses of 27 specimens to confirm the calcitic nature of the shell material. We compare the geochemical signatures of the shells with environmental parameters and evaluate taxonomic trends in shell composition. Furthermore, we investigate intra-shell chemical variability to address overall compositional heterogeneity and plate-specific chemical fingerprints."
published_009,"In 1997, a newly emerged disease of the ovine digit was observed in the UK, the lesions of which led to severe clinical signs in affected animals (Harwood et al., 1997). The distinction between this disease and ovine foot rot was made by the differences in clinical appearance, the failure to respond to conventional foot rot therapies, and the isolation of Treponema spp. from the lesions, with and without concurrent detection of the causative agent of foot rot, Dichelobacter nodosus (Naylor et al., 1998; Davies et al., 1999; Lewis et al., 2001). The disease was named contagious ovine digital dermatitis (CODD) (Davies et al., 1999) and further work has since identified three Treponema phylogroups to be associated with clinical disease, namely Treponema medium/Treponema vincentii-like, Treponema phagedenis-like and Treponema pedis (Sullivan et al., 2015). These same three Treponema phylogroups are reported to be the cause of bovine digital dermatitis (BDD) (Evans et al., 2008), although not all BDD lesions may necessarily contain all three phylogroups (Evans et al., 2009). Recently, the clinical presentation of CODD was described in detail for the first time, and a five-point lesion grading system was proposed (Angell et al., 2015). In that study, the range of lesions was organized into a clinical progression of disease such that lesions progressed from grade 1 to 5. Briefly, the disease begins at the coronary band and extends distally, resulting in progressive, and eventually total, separation of the dorsal hoof wall. The lesions may resolve and new horn eventually replaces the horn lost in the fulminant stage of the disease, with some deformity of the regrown hoof. To date, there has been no histopathological characterization of these lesions or immunohistochemical investigation of the presence of Treponema spp. associated with the pathological changes. In cattle, immunohistochemistry (IHC) has been used to identify Treponema-like organisms in BDD lesions, and these are associated with hair follicles and sebaceous glands (Evans et al., 2009). In the present study, IHC was used to investigate the presence/absence of CODD-associated Treponema spp. It was considered that Treponema spp. would more likely be identified in earlier grades of lesion, particularly those of grade 1. The organisms may persist throughout all of the stages of the clinical disease, but might not be identifiable due to competition by secondary invaders, tissue necrosis or damage/removal by the immune system. The aims of this study were: (1) to describe the histopathological changes observed for each grade of clinical lesion and to investigate whether these grades reflected a progression of disease, and (2) to investigate whether the spirochaetes of the three phylogroups, T. medium/T. vincentii-like, T. phagedenis-like and T. pedis, were associated with clinical disease."
published_010,"High-frequency action potential firing is essential for rapid information processing in the central nervous system, and in particular in the auditory system, which must encode complex auditory information with high fidelity (Carr, 1993; Joris and Yin, 2007; King et al., 2001). Kv3.1 K+ channels mediate currents with a high activation threshold and fast activation and deactivation kinetics, allowing for rapid action potential repolarization and short inter-spike intervals (Erisir et al., 1999; Rudy et al., 1999; Rudy and McBain, 2001). Kv3.1 K+ current activation and deactivation properties explain why those currents are critical for permitting high frequency firing of neurons. In accordance with this observation, Kv3.1 K+ currents are expressed in neurones firing at high frequency such as in the spinal cord (Deuchars et al., 2001), cortex (Erisir et al., 1999), cerebellum (Joho and Hurlock, 2009) and auditory nuclei (Wang et al., 1998). The dorsal cochlear nucleus (DCN) is an auditory brainstem structure playing a pivotal role in the integration of information from multiple sensory pathways (Wu and Martel, 2016) and in acoustic cues related to vertical sound source localization (May, 2000). DCN principal fusiform cells fire reliable and precise trains of action potentials in response to depolarizations (Ding et al., 1999; Hancock and Voigt, 2002a, b; Manis, 1990; Oertel and Wu, 1989; Pilati et al., 2008). Our previous study has shown that acoustic over-exposure triggers hearing loss, and this correlated with profound changes in the firing pattern and frequency of DCN fusiform cells (Pilati et al., 2012). After acoustic over-exposure, a proportion (∼40%) of DCN fusiform cells display a distinct bursting firing pattern which has been associated with reduced Kv3.1 K+ currents, losing the ability to fire regularly and at high firing frequencies (Finlayson and Kaltenbach, 2009; Pilati et al., 2012). DCN fusiform cells also exhibit increased spontaneous firing rates (Brozoski et al., 2002; Dehmel et al., 2012; Kaltenbach et al., 2004) and increased cross-unit synchrony and bursting of spontaneous firing which correlate with behavioural measures of tinnitus (Finlayson and Kaltenbach, 2009; Kaltenbach et al., 1998; Wu and Martel, 2016). Despite evidence demonstrating firing frequency modulation and burst induction within the DCN (Finlayson and Kaltenbach, 2009; Pilati et al., 2012), the role of Kv3.1 K+ currents in DCN fusiform cell spike-timing remains unexplored. In this study we explore the effects of Kv3.1 K+ currents on the firing frequency and spike-timing of DCN fusiform cells. We used tetraethylammonium (TEA), a K+ channel blocker known to inhibit the Kv3 K+ currents at low concentrations (IC50 ∼0.3 mm) (Critz et al., 1993; Grissmer et al., 1994; Hernandez-Pineda et al., 1999; Johnston et al., 2010; Kanemasa et al., 1995) and acoustic over-exposure to trigger a down-regulation of high voltage-activated (Kv3 type) K+ currents (Pilati et al., 2012), to test the disruptive effects on spike timing. Firing precision of DCN fusiform cells was assessed using an analysis of the coefficient of variation (Pilati et al., 2012), and spike-time reliability was assessed by measuring the ability of the fusiform cell to fire consistently across repeated trials with the same current stimulus (Joris et al., 2006). Until recently, the exploration of the role of Kv3 K+ channels in neurophysiology has been hampered by the absence of pharmacological tools. However, the compound (5R)-5-ethyl-3-(6-((4-methyl-3-(methyloxy)phenyl)oxy)-3-pyridinyl)-2,4-imidazolidinedione, (AUT1) has been shown to be a selective Kv3.1/3.2 K+ channel modulator (Rosato-Siri et al., 2015) increasing the open probability of Kv3 K+ channels, and shifting the voltage-dependence of activation of Kv3.1/3.2 K+ currents to more negative potentials (Brown et al., 2016; Rosato-Siri et al., 2015; Taskin et al., 2015). Here, we used AUT1 in the presence of a low concentration of TEA, or after acoustic over-exposure, to test whether positive modulation of Kv3.1 K+ currents could rescue impaired DCN fusiform cell firing precision and spike-time reliability. Spike-timing depends upon various factors including the membrane time constant (Azouz and Gray, 2000), voltage-gated ion channels (Azouz and Gray, 2000; Fricker and Miles, 2000; Higgs and Spain, 2011; Jaeger and Bower, 1999), the coincident activation of pre-synaptic neurones (Diesmann et al., 1999; Gauck and Jaeger, 2003; Grande et al., 2004; Grothe and Sanes, 1994), and/or baseline spontaneous membrane voltage fluctuations (Dorval and White, 2005; Jaeger and Bower, 1999). We therefore tested whether modulation on spike-timing was dependent on baseline spontaneous membrane voltage fluctuations. As TEA (Erisir et al., 1999; Wang et al., 1998) and acoustic over-exposure (Pilati et al., 2012) reduced the amplitude of DCN fusiform cell action potential after-hyperpolarization, we also tested whether firing precision and spike-time reliability were dependent on the action potential after hyperpolarization. Our study shows that Kv3.1 K+ currents control the spike-timing in DCN fusiform cells, via an effect on the action potential after-hyperpolarization and on spontaneous membrane voltage fluctuations. Our study further suggests Kv3.1 K+ currents act as a potential target to restore action potential fidelity following acoustic trauma."
published_011,"Some voluntary actions are accompanied by an experience of conscious intention, of initiating and controlling our actions. The basis of this experience of conscious intention is vigorously debated. Some stimulation (Fried et al., 1991) and recording (Mukamel et al., 2010) results suggest that the experience of conscious intention is associated with activation of specific structures in the medial frontal and parietal lobes (Desmurget et al., 2009). On an alternative view, the experience of conscious intention is not so much a direct read-out of any specific brain activity, but an inference about the causes of internally-generated actions. Thus, one might infer that one's own actions have some internal cause by general principles of causal reasoning. This cause would then be retrospectively inserted into the narrative stream of consciousness. Conscious intention would then not correspond to a mental state, in the normal sense, but to a reconstructive confabulation of action authorship (Wegner, 2002). The inferential or reconstructive view has received strong support from studies showing that attribution of agency (Wegner and Wheatley, 1999) and even primary experience of action (Moore et al., 2009) are strongly influenced by the context of action, and by the occurrence of events that might plausibly be caused by intentional actions. Reconstructive inference raises a major methodological problem for studying action awareness. Most methods for investigating awareness involve a subjective report, which typically occurs after the event to which it refers. Even if there is a pure, premotor experience of intention that precedes action, most experimental reports of this experience are obtained after action. The experience of intention will then be altered by how the body actually moves. In fact, the entire experience of intention could be an invention by the mind to justify how and why the body moved. It has therefore proved difficult to access a pure experience of volition, independent of the bodily actions with which intention is associated (Fried et al., 1991). The positions of “no volition, just inferential reconstructivism” and “direct access to conscious intention” may both be excessively strong. Recent work on consciousness suggests that all perceptions, not just conscious intention, are a form of synthetic inference based on integration of multiple neural activities over time (Dehaene and Changeux, 2011). Therefore, understanding how and when the experience of volition is generated in the human brain, and which circuits are involved in generating it, remains an important question in the understanding of consciousness. The case of volition may have particular applied importance, because most systems of law assume that an experience of willing an action, and/or failing to inhibit the action, is necessary for a person to be judged socially and legally responsible. The everyday experience of action is strongly linked to the process of initiating actions (Libet et al., 1983). However, neuropsychologists have long recognised that a key element of behavioural control involves inhibiting actions that may be suggested by the environment, or by our previous experience (Della Sala et al., 1991). Accordingly, we recently suggested that the neural computations involved in voluntary action include a crucial decision regarding whether to act or not [the What, When, Whether (WWW) model of intentional action] (Brass and Haggard, 2008). This decision could act as a final point of control over behaviour (Filevich et al., 2012). The status of conscious processing in such inhibition is particularly controversial (Hughes et al., 2009; Libet, 2009). However, inhibition of action does not have any behavioural marker, by definition. This makes studying intentional inhibition of human action particularly challenging. We have previously shown that the conscious experience of intending to act can serve as a subjective marker for actions that are prepared, but subsequently inhibited (Brass and Haggard, 2007). In particular, we showed that the reduction in beta-band EEG power that precedes voluntary action was replaced, in trials where participants freely chose to inhibit their actions (Walsh, E., et al., 2010). Here we have tested an individual with congenital absence of the left arm in a voluntary action and voluntary inhibition paradigm. CL is a 37 year old (at time of testing) performance artist, born without a left hand and arm. She is a co-author of this paper. CL experiences occasional but vivid phantom sensations related to the “left hand”. The experiences include strong urges to move and control her phantom left limb. Her participation in the experiment offered a unique window into the neurophenomenological mechanisms of voluntary action and voluntary inhibition. First, we wished to investigate whether preparation of movements with the phantom hand might be accompanied by a subjective sense of volition, and by the normal neurophysiological markers of voluntary action. A further scientific reason for studying CL's performance is the opportunity for a novel comparison between voluntary action and voluntary inhibition of action. In the normal case, the comparison between action and inhibition inevitably involves body movement in the former case, but not the latter. Thus, sensory feedback from the body movement may confound any contrast between action and inhibition conditions. In principle, neurophysiological measures of preparation prior to action may avoid this confound, because physical body movement has not yet begun. However, any difference between action and inhibition conditions could still reflect differences in predicted sensory feedback, rather than an active process of inhibition confined to the inhibition condition. In the case of a phantom limb, in contrast, action and inhibition are physically identical, and are presumably predicted to be physically identical. Therefore, any difference in neurophysiological markers must reflect a central neurocognitive process of inhibition. In line with CL's subjective experiences of command over her phantom left hand, such volitional control might arise from innate core cortical mechanisms. Alternatively, CL might acquire volitional control over her phantom left hand via a process of generalisation or interhemispheric transfer from motor representations for her right hand; in which case pre-movement ERD before actions made by her phantom left hand might appear as “weak echoes”. Finally, the results could contribute to the understanding of phantom limb phenomena per se. To our knowledge, no previous study has investigated either the preparation or the inhibition of voluntary actions of a phantom limb."
published_012,"Detecting evidence of life in samples of Mars is a major scientific preoccupation. Space missions can employ two approaches to the challenge, namely in situ analysis on Mars or the return of samples for analysis on Earth. In situ approaches have been tried but have not yet provided the searched-for evidence of life (Biemann et al., 1976; Leshin et al., 2013; Ming et al., 2014), although controversy still exists over the in situ Viking data (Levin, 2014), while sample return missions are still in the planning stages (McLennan et al., 2012). Each mission to Mars provides incremental data that improves our knowledge of the martian environment. Some of this data is sought after while other data is unexpected and fortuitous. With every increase in background knowledge subsequent planning is more informed and the probabilities of successful future missions enhanced. However, owing to the great expense of martian missions and the infrequency of their occurrence, other ways of improving mission planning are desirable. Statistical approaches are one way in which mission design can be improved (Sims et al., 2002). Bayesian methods (Sivia and Skilling, 2006) in particular are useful because they can accommodate the significant unknowns associated with a mission that has yet to take place. Bayesian statistics produce degrees of belief or “Bayesian probabilities”. The Bayesian approach has been used previously to decide the amounts of sample needed to be collected during sample return missions to carbonaceous asteroids (Carter and Sephton, 2013) and to target samples and perform interpretations of inconclusive data on Mars organic matter detection missions (Sephton and Carter, 2014). Benefits of a Bayesian statistical approach include identification of key components to which mission success is most sensitive. While the values of estimated inputs into the statistics may be modified as new data is acquired, the relative importance of individual types of data is unlikely to change. With the parts of missions to which overall success is most sensitive constrained, future mission design can take account of these findings and allocate resources accordingly. Increasing mission complexity requires progressively more intricate statistical analysis, so for the purposes in this paper we will consider a relatively simple mission that will capture the fundamentals of Bayesian analysis. We will assume the following: (i) only one sample will be collected, (ii) the mission has only one sampling tool and (iii) only one type of target rock is to be sampled. The context in which these mission objectives will be operated will be varied, but these fundamental assumptions will remain. Many choices of values to include in the calculations can be updated as more information is received from Mars and the most accurate values will be perpetually open to debate, yet we hope that the method we establish provides a useful means of comparing mission designs."
published_013,"Calcium looping, CaL, is an emerging CO2 capture technology in which particles of CaO are used to capture CO2 from a flue gas, typically in a fluidized bed, giving rise to CaCO3, which is subsequently decomposed in an oxy-fired fluidized bed combustor [1–7]. The calcium looping in fluidized beds for post-combustion applications has already been demonstrated in several test facilities of up to 1–2 MWth scale [8–12]. The rapid development of the CaL technology is due to the use of reactor configurations and materials very similar to those typically employed in large-scale fluidized-bed combustion power plants.. In CaL systems, natural limestone is used as a source of CaO because of its low cost and suitable fluidisation properties. An important design parameter of the calcium looping processes is the consumption of limestone (i. e., the make-up flow) needed to sustain a certain level of activity of the CaO material for the capture of CO2 and to compensate for the CaO loss in the cyclones located downstream of the interconnected carbonator and calciner reactors. Thus, to ensure the selection of the most appropriate sorbent the mechanical properties of the material circulating between the reactors must be taken into consideration. Attrition, defined as the unwanted breakage of particles, results in the degradation of the solids and in a change in the size and number of particles. It is influenced by a large number of variables related to the design of the system the properties of the solids and the reacting environment [13]. In fluidized beds the most common attrition mechanisms are induced by mechanical motion and by temperature or pressure gradients inside the particles as they are subjected to fast changes in their environment [14, 15]. The main attrition mechanisms caused by mechanical motion are abrasion, when breakage takes place exclusively on the particle surface generating fines, and fragmentation, when the particles are broken into similar coarse fragments [13, 16]. Abrasion usually takes place due to the low velocity impacts between particles in the dense bed. There is an initial peak of fines generation which then decreases as the particles round off [17–19]. Fragmentation typically occurs due to high velocity impacts in the grid jets, cyclones and at the exit of the circulating fluidized bed reactors. The extent of particle fragmentation increases with the impact velocity, the multiplicity of impacts and it is also affected by the specific properties of the solids [20]. On the other hand, the main attrition mechanism induced by internal forces is decrepitation (or primary fragmentation), which generates both coarse and fine particles. This phenomenon is generally caused by thermal shock during rapid heating and/or by the fast release of gas from the solids [13]. There is substantial literature available on the attrition of limestone in fluidized bed combustors, where CaO is typically used as a SO2 sorbent [21–26]. However, the knowledge acquired so far on this phenomenon for the specific conditions of post-combustion CaL systems is still scarce [7, 20, 27–34]. There is general agreement that CaO-based sorbents sinter and deactivate during the CO2 capture process (i.e. their CO2 carrying capacity diminishes as the number of carbonation-calcination cycles increases) and they become progressively harder and rounder in shape [27, 28, 30–34]. The sulfation of CaO greatly reduces the generation of fines from abrasion, since the sulfate layer formed on the outer surface tends to improve the strength of the particles [35, 36]. Moreover, calcination and carbonation reactions also affect the extension and pattern of fragmentation. The particles of raw limestones and re‑carbonated sorbents are the most resistant to impact loading, whereas those of calcined sorbents are the weakest [20]. Decrepitation in limestones is mainly caused by the release of CO2 during the first calcination, which changes the morphology of particles, rather than by thermal shock [35]. Despite the existing consensus on the qualitative attrition mechanisms noted above, there is a wide variety of tests and techniques that are used to quantify attrition and to rank materials in terms of attrition performance. Standard attrition tests available for other applications do not provide sufficient relevant information for each type of material and application. The great variability in attrition of natural limestones requires for highly empirical approaches to investigate these phenomena under specific process conditions. No feasible method to study the different attrition mechanisms separately is available as yet (unlike the study of chemical reaction phenomena). Hence the use of lump attrition parameters like the “attrition index” is the norm in quantitative attrition studies. The attrition index proposed by Forsythe and Hertwig in 1949 for fluid cracking catalysts [37] is still considered to be a useful parameter for evaluating solids mechanical degradation during fluidization. In this methodology, an attrition rate is arbitrary defined considering the amount of material with a particle size of below 44 μm (325 mesh) produced per hour. This index is useful for the comparative testing of materials under similar and strictly controlled conditions [13]. Other attrition indexes reported in the literature are based on Gwyn's formulation [38]. Gwyn assumes that the amount of material elutriated corresponds to the attrited material, which can be fitted to a power-law time dependent function. However, this formulation is only valid when abrasion is the main attrition mechanism during the operation. Some authors consider that these definitions are not sufficiently accurate for describing the attrition process since they do not take into account the breaking of particles when fines are not produced [16] or the fact that not all the fines produced are elutriated [39]. Amblard et al. [39] recently defined another attrition index to overcome these limitations. This index is based on the evolution with time of the entire particle size distribution (PSD) of the solids during the test including the elutriated material (and not only the fines generated below 45 μm, as in the case of classical attrition indexes). The main limitation of this procedure is the greater experimental effort needed to obtain a detailed assessment of the particle size distributions under realistic conditions. In the present work, the methodology developed by Amblard et al. [39],has been adapted to study the resistance to attrition of four limestones with similar chemical composition in a 30 kW fluidized-bed pilot plant operated under conditions similar to those of larger calcium looping systems (i.e., with high gas velocities and high temperatures). With this procedure, the dominant attrition mechanisms for each limestone are identified and the solids can be ranked according to the extent of particle attrition, which can facilitate the screening of sorbents in calcium looping applications."
published_014,"Disturbances caused by windthrows or insect outbreaks play an immanent role in the dynamics of most forest ecosystems (Franklin et al., 2002; Turner et al., 1998). Worldwide, about 14 million hectares of forest are affected annually by abiotic agents other than fire, including 8.5 million hectares affected by insects, 3.8 million hectares affected by severe weather, and 1.2 million hectares affected by various diseases (Lierop et al., 2015). In temperate forests, insect outbreaks and severe weather conditions can affect 50 times as much forest area as fires (Dale et al., 2001). Windthrows and insect outbreaks are influenced by global change in several ways (Schlyter et al., 2006). For example, extreme climate events in combination with a higher mean temperature decrease forest vitality, which in turn increases forest susceptibility to windthrows. Windthrows, in turn, often trigger insect outbreaks, which are then facilitated by warm and dry weather conditions (Seidl et al., 2016). In recent years, disturbance events reached an unprecedented global extent (Weed et al., 2013). Future climate projections predict that disturbances will further intensify (Seidl et al., 2014; Westerling et al., 2006), with potentially major impacts on global carbon sequestration (Kurz et al., 2008). Therefore, accurate monitoring of disturbance type, size, and impact over large areas is becoming increasingly important. In addition, early detection could support timely salvage logging operations and thereby minimize economic losses (Fahse and Heurich, 2011), or could allow short-term actions aimed at reducing the ecological impact of disturbance. The European spruce bark beetle (Ips typographus) is the most important biotic disturbance agent in Europe and Siberia; other bark beetle species (e.g., Dendroctonus ponderosae, Dendroctonus frontalis) constitute major disturbance agents of coniferous forests in North America. Ignited by windthrows and droughts, bark beetle populations can reach very high densities, which allow them to kill even healthy trees (Marini et al., 2017). After hibernation, I. typographus beetles start to swarm in spring and lay their eggs underneath the inner bark of Norway spruce trees (Picea abies). Once the larvae hatch, they feed on the tree's phloem tissues, thus interrupting the flow of water and nutrients, which causes the tree to die. Infested trees go through three stages of infestation named after the appearance of the trees, namely green, red, and gray attack stages, which are reached after a few days, a few months, and about half a year, respectively. In a forest, this process can occur up to three times a year (depending on temperature) because the beetle population can produce several generations per summer season (Wermelinger, 2004b). Multiple within-year events can be separated only by frequent monitoring during spring and summer. Such monitoring can be cost effective over large areas only if remote-sensing technologies are used. Indeed, over the past decades, forest disturbance has received considerable attention by researchers that use remote sensing. Passive sensors of various spatial and spectral resolution have been employed for mapping insect-related disturbances (Frolking et al., 2009; Senf et al., 2017; Wulder et al., 2006), with methods spanning from the detection of single dead trees with very high-resolution aerial photography (Nielsen et al., 2014; Polewski et al., 2015), to taking advantage of multi-temporal algorithms based on the extensive Landsat time series archive (Cohen et al., 2010; Huang et al., 2010; Kennedy et al., 2010; Oeser et al., 2017), and to the use of hyperspectral sensors for the detection of the green attack stage (Fassnacht et al., 2014; Lausch et al., 2013). The lack of long synthetic aperture radar (SAR) time series acquired from sensors with similar or compatible specifications has hindered the development of multi-temporal algorithms using active radar sensors. SAR-derived forest disturbance has largely been based on two-date change-detection techniques (Joshi et al., 2015; Kuntz and Siegert, 1999; Mitchard et al., 2011; Rignot and Zyl, 1993; Tanase et al., 2015a). Such studies, mostly focused on fire and logging induced disturbances, show that the L-band is one of the most sensitive SAR wavelengths for detecting disturbance effects in forested landscapes and that its sensitivity does not change across environments (Mermoz and Le Toan, 2016; Tanase et al., 2015b). In contrast, wind induced disturbance has been less considered, with only few SAR based studies being available (Eriksson et al., 2012; Fransson et al., 2010; Fransson et al., 2002; Green, 1998; Schwarz et al., 2003). Furthermore, a recent literature review on the remote sensing of insect disturbances almost exclusively mentions passive optical sensors (Senf et al., 2017), except for one study that combined active and passive technologies (Ortiz et al., 2013). None of the active-based studies have taken advantage of the improved capabilities demonstrated by recent L-band space-borne platforms for delineating areas affected by insect outbreaks. Yet, the potential demonstrated by SAR sensors in monitoring forest disturbance caused by humans and fire makes their use compelling since insect outbreaks and windthrows influence forest structure similarly by changing the relative position and quantity (i.e., defoliation, fallen trees) of the vegetation material, and by changing the vegetation water content (i.e., drying up of affected trees). Such changes could influence the dominant scattering mechanisms, as well as signal attenuation through the vegetation layer. In the case of the L-band, cross-polarized radar scattering is largely the result of backscatter from large elements present in the forest canopies (i.e., branches, snags), and the foliage acts as an attenuation layer (Le Toan et al., 1992). The removal of the vegetation material or reduction in the vegetation water content is expected to decrease the cross-polarized radar scattering, with lower values being generally expected after disturbance events (i.e., reduced scattering due to decreasing scattering elements) not compensated for by reduced attenuation. For co-polarized waves, scattering from the ground plays a large role particularly when the vegetation layer is removed, which might hamper the retrieval of disturbed areas. The aim of this study was to evaluate the sensitivity of L-band data to changes in forest structure caused by wind and insect outbreaks. The specific objectives were i) to evaluate the extent to which changes in forest structure caused by wind and bark beetle outbreaks influence the L-band SAR signal, ii) to understand the influence of pre- and post-disturbance SAR acquisition timing as well as of post-disturbance management practices on SAR backscatter changes, and iii) to estimate the accuracy of disturbance mapping using L-band SAR data."
published_015,"Alkylidene carbenes, also known as alkenylidenes, are reactive intermediates in organic chemistry. Their high reactivity necessitates generation and trapping in situ, but unlike many carbenes,1 they display high levels of selectivity without the need to be associated with a metal catalyst. Indeed, although many methods for the formation of alkylidene carbenes are metal-based, their reactivity is generally rationalized without invoking a role for the metal. The most common reactions of alkylidene carbenes are shown below (Scheme 1). 1,2-Migration is the preferred reaction pathway for phenyl and hydrogen substituted alkylidene carbenes (Eq. 1). Since alkylidene carbenes precursors are often derived from aldehydes and ketones, this offers a method of carbon homologation, and is extensively utilized in organic synthesis. Alkyl-substituted alkylidene carbenes generally undergo insertion rather than migration. Intramolecularly, this leads to cyclopentenes through a highly regioselective 1,5 C–H insertion reaction (Eq. 2). This reaction occurs with high levels of chemoselectivity (for more highly substituted C–H bonds) and is stereospecific (retention of absolute configuration at the insertion site), the latter process being consistent with alkylidene carbenes reacting through the singlet state. Coupled with the ability to insert into unfunctionalized C–H bonds, thereby simplifying structural complexity in the starting material, the 1,5 C–H insertion reaction of alkylidene carbenes is particularly useful in organic synthesis. Five-membered unsaturated heterocycle formation occurs when a heteroatom is present in the connecting chain, and is also observed in formal 1,5 insertions into heteroatom bonds, most commonly O–H and O–SiR3, to make dihydrofurans (Eq. 3). This review covers synthetic aspects of both the generation and reactions of alkylidene carbenes, including related organometallic chemistry, which fits into the reactivity patterns shown above. Other reaction pathways and factors affecting selectivity are also discussed, as are applications in target synthesis. Aspects of the chemistry of alkylidene carbenes have been previously reviewed in the literature. A short review on the synthetic potential of alkylidene carbenes appeared in 1997.2 A comprehensive review by Knorr in 2004 concentrated on the evidence for alkylidene carbenes in a number of reactions.3 The use of hypervalent alkynyl iodides as precursors to alkylidene carbenes has been previously reviewed,4 as has the use of α-heteroatom substituted alkenyl lithium reagents.5 A 2010 review by Koskinen discussed developments in the conversion of carbonyl compounds to alkynes,6 and as such, the synthetic utility of this transformation will not be extensively covered."
published_016,"Protoberberine alkaloid (±)-pallimamine 1 was isolated in 1989 from Corrydalis pallida var sparsimamma collected in Nan-Shan village, Taiwan, along with other known protoberberine alkaloids α-allocryptopine 2, protopine 3 and (−)-capaurimine 4 (Fig. 1).1 A large number of diversely functionalised protoberberines2 have been discovered over a number of years, with the family typified by the bis-isoquinoline motif highlighted below in red. Many protoberberines exhibit biological activity,3 which has helped to propagate significant research efforts towards their synthesis.4 To the best of our knowledge, the pentacyclic structure of pallimamine 1 with its additional tetrahydropyran ring is a unique feature amongst protoberberine alkaloids and it has not been synthesised previously. In view of these factors, as well as a longstanding interest in the application of telescoped/tandem synthetic methodology in target synthesis,5 we were encouraged to embark upon its total synthesis. Our basic retrosynthetic analysis is shown in Fig. 2. We were confident that our recently established ‘Direct Imine Acylation’ (DIA) methodology6 would facilitate the coupling of a comparatively simple imine 5 and benzoic acid derivative 6 to construct the key δ-lactam7 ring system 7; the viability of DIA to construct C–C bonds has been demonstrated previously in our group,6a,6d–g including an example in a related natural product synthesis.6d Following reduction (7→8) it was then planned to exploit the symmetry of the resulting diol 8 to construct the tetrahydropyran ring via a C–O coupling reaction, resulting in the desymmetrisation of the diastereotopic alcohol groups. It was expected that C–O bond formation would occur selectively via the pseudo-equatorial alcohol, rather than the alternative pseudo-axial alcohol, to form a product with the trans-fused ring junction required in the natural product. While we had no evidence to support this prediction when starting this work, intuitively we felt that the trans-fused product (and the associated transition state for its formation) would be more thermodynamically stable than the cis-variant and therefore form more easily. Indeed, it is plausible that similar thermodynamic factors are at play during its biosynthesis, giving rise to the observed trans-configuration in the natural product; the fact that pallimamine was isolated in racemic form offers minor supporting evidence for this hypothesis, as this suggests that its biosynthesis may proceed via an intermediate that has undergone reversible epimerisation. This paper details our efforts to date to apply the strategy outlined in Fig. 2 to the total synthesis of pallimamine."
published_017,"Low cost renewable energy sources are needed to avert the risk of global climate changes [1]. There is no doubt that reliable and efficient Photovoltaic (PV) cells will be part of the solution. In recent years a new promising type of PV cell has emerged, with an active layer consisting of organo-metal trihalide perovskite. In particular, the methylammonium lead halide is being investigated intensely [2–4] achieving an efficiency as high as 20% [5]. The pre-perovskite film can be deposited by a Chemical Solution Deposition reaction process; the product then undergoes a phase transformation to the cubic perovskite structure by a mild heat treatment (below 140 °C). Spin coating is a suitable deposition method, providing a high quality active layer. A major weakness of the methylammonium lead halide cells is their challenged stability, coupled with the toxicity of lead. Cesium- and bismuth-based perovskites, such as Cs3Bi2I9, may serve as good candidates for replacing lead in this type of PV cells, [6] however their current performance status is far from being satisfactory [7]. One way to obtain cesium-based organo-halide perovskites involves the preparation of homogeneous thin film of cesium iodide serving as a precursor to be further doped, en route for perovskite PV cell preparation. During preparation of these films by spin coating, a unique structure of CsI was observed. While this structure shares some common features with previously reported dendritic structures grown by Diffusion Limited Aggregation (DLA), it is unique by the absence of a noticeable growth origin and by a stunning, perfectly orthogonal growth pattern. Dendritic self-similar shapes are quite common in nature. From the technological point of view, their appearance may be advantageous in certain cases, or deleterious in others [8,9]. Whether advantageous or deleterious, these shapes easily attract the interest of the scientific and technological community. Indeed, the introduction of DLA models more than thirty years ago by Witten and Sander [10] has stimulated a large interest in the study of nucleation and growth processes on surfaces. In the “classical” DLA model clusters are grown from a seed particle (the origin). Individual particles are launched at some random distance from the origin and “walk” randomly until they stick to the growing cluster, forming a fractal structure, growing outward from an inner location of the origin and having a typical fractal dimension of 1.7 [11]. Thus far, scarce attention was given to the effect of the substrate on the DLA growth above it. One possible effect is the induction of epitaxial growth as observed in the two dimensional formation of cubic-structured silicon nanosheets growing along the 〈110〉 direction [12]. Another report describes the growth of silver islands on silica, where a 2D fractal structure was formed on thermal oxide, while on a native oxide substrate non-fractal, faceted, crystallites were formed under the same conditions [13]. Here, the effect of the substrate was explained in terms of the thinner oxide layer of the native oxide, which did not block the tunneling of electrons to the silver ions, thus facilitating its fast reduction and non-fractal deposition pattern. One of the more relevant works to this report describes the growth of poly-ethylene oxide (PEO) on the surfaces of various alkyl halides [14]. There, Dendrites having right angle branches were formed along the 〈100〉 set of directions in NaCl. In contrast, the dendrites formed on KCl and KBr revealed the classical tree-like branching. A fractal dimension of 1.6 was found for PEO on NaCl, whereas the fractal dimension for PEO on KCl and KBr was 1.7 and 1.8, respectively."
published_018,"Osteoarthritis (OA) is the most commonly diagnosed chronic disease, affecting approximately 27 million adults in the United States (Hootman et al., 2016; Lawrence et al., 2008). By the year 2030 this number is expected to surpass 67 million (Hootman and Helmick, 2006). The most debilitating OA symptom affecting patients is pain (Malfait and Schnitzer, 2013; Neogi, 2013). Living with chronic pain poses negative personal and societal ramifications. Patients may experience difficulties completing daily activities, routine needs and personal care (Centers for Disease Control and Prevention, 2010). Among working age adults, 30.6% reported serious work limitations (Theis et al., 2007). In a 2010 Medical Expenditures Panel Survey, the estimated cost from absenteeism due to OA pain was $11.6 billion annually (Kotlarz et al., 2010). Further cost analyses for the United States showed $162 billion in combined direct (medical expenditures) and indirect costs (lost wages). This accounts for 2% of the annual gross domestic product (Kotlarz et al., 2010; Yelin et al., 2007). There is no cure for osteoarthritis, thus treatment goals are focused on pain management. A combination of pharmacological and non-pharmacological practices is commonly utilized. Pharmacological options include oral (systemic), intraarticular, and surgical therapies. Non-pharmacological options include: education, exercise, weight reduction, acupuncture, and joint protection (Wenham and Conaghan, 2013). An examination of these practices found insufficient joint pain relief, intolerable drug side effects and interactions, and minimal compliance with integrative therapies (Conaghan, 2012; Doherty et al., 2011; Zhang et al., 2010). In the most degenerative cases surgical treatment may be used. Several analyses concluded alleviation of pain from surgical therapies were no greater than those obtained from placebo (Kirkley et al., 2008; Mosely et al., 2002; Zhang et al., 2010). Total joint replacement has significant risk and is inappropriate for individuals with comorbidities but can alleviate pain. Therefore, there is an important unmet need for safe, effective pain treatment. Generation and maintenance of chronic arthritis pain is complex. Nociceptive articular pain originates at the site of disease or injury. The nociceptive input is a response to noxious mechanical, thermal, or chemical stimuli. Local inflammatory mediators (H+, Glutamates, Bradykinin, cytokines, and chemokines) and neuropeptides released in the periphery by efferent neurogenic signals amplify and maintain arthritis pain by lowering the threshold for nociceptor activation (Miller et al., 2014; Schaible et al., 2006). Local release of neuropeptides such as substance P and calcitonin gene-related peptide from articular sensory nerve endings causes vasodilatation, extravasation of plasma, chemotaxis of macrophages, and mast cell degranulation (Schaible et al., 2006). Given this peripheral sensitization, we hypothesize arthritis pain may be treated effectively by intraarticular neurotoxins. Botulinum neurotoxins (BoNT) are comprised of seven different serotypes that mechanistically inhibit the release of neuronal signal chemicals from sensory nerves which contribute to peripheral sensitization. All serotypes act by hindering SNARE-associated exocytosis of acetylcholine and other neuropeptides into the synaptic cleft (Dolly and Aoki, 2006; Oh and Chung, 2015). This mechanism begins with the endocytosis of BoNT, comprised of a heavy and light chain, into the presynaptic terminal via presynaptic membrane vesicle (Zanetti et al., 2015). The light chain is then translocated into the cytosol and acts as a protease, cleaving different SNARE proteins (Fischer and Montal, 2007). Without SNARE proteins, facilitated transportation of neuronal signal chemicals across the presynaptic membrane into the synaptic cleft does not occur (Pirazzini et al., 2017). Several recent studies have shown that BoNT may not stay localized in the axon terminal but also undergoes axonal transport (Favre-Guilmard et al., 2017; Marinelli et al., 2012; Restani et al., 2011). The main differences between the seven BoNT serotypes are based on affinities and which SNARE protein(s) they cleave (Aoki and Guyer, 2001; Mense, 2004; Pellizzari et al., 1999). In our earlier studies, we examined the analgesic effects of rimabotulinumtoxinB (BoNT/B) in an OA murine model. Anderson et al. used joint palpation to measure evoked pain behaviors and visual gait analysis to measure spontaneous pain behaviors. Both significantly improved with IA BoNT/B treatment (Anderson et al., 2010). Our group concluded that further analysis of Botulinum neurotoxins was necessary. OnabotulinumtoxinA (BoNT/A) differs from BoNT/B in several ways. The serotypes cleave different SNARE proteins; BoNT/A cleaves SNAP-25 and BoNT/B cleaves VAMP/synaptobrevin (Pellizzari et al., 1999; Verderio et al., 2007). There have been several preclinical studies showing that BoNT/A inhibits the release of local nociceptive neuropeptides (Aoki, 2005, 2008; Cui et al., 2004). And BoNT/A is the only serotype with FDA approval for peripheral applications to treat pain (Burstein et al., 2014). We hypothesized that IA BoNT/A could reduce pain behavior measures and produce analgesia in a collagenase induced OA murine model. The purpose of this study was to measure the analgesic response produced by BoNT/A in female mice with chronic degenerative monoarthritis pain using evoked and spontaneous pain behaviors. This animal model of OA was chosen because it is a well-established model and has been used previously to study IA BoNT/B."
published_019,"Crime ‘hotspots’ can be defined as “those areas where the local averages (e.g. concentrations of crime) are significantly different to the global averages” (Chainey & Ratcliffe, 2005 p. 164). Hotspots indicate the times and/or places that exhibit the largest relative volumes of crime, and should therefore be prioritised for intervention. However, raw counts of crime taken in isolation disregard the impacts of the population-at-risk of crime victimisation. Hence crime rates are often used to quantify hotspot relative to the population-at-risk. The residential population is the most commonly-used denominator. However, for some types of crime such as assaults (Boivin, 2013), robbery (Zhang et al., 2012) and violent crime (Andresen, 2011), this denominator is not suitable as it does not reliably quantify the size of the potential victim population. Daily flows of people will radically alter the characteristics of urban spaces and recent research has shown that these mobile populations can have a substantial impact on crime rates (Andresen & Jenion, 2010; Felson & Boivin, 2015; Stults & Hasbrouck, 2015). New data are becoming available that offer an alternative to the traditional residential crime rate. The aim of this research is to identify, from a suite of new population measures, the most reliable denominator for an inherently non-residential crime: theft from the person. This aim will be accomplished as follows: Identify data sets that might reliably represent the population at risk of theft from the person and marry them to a shared geography; Calculate correlations between the population measures and crime volumes to identify the strongest crime predictor; Use a Local Indicator of Spatial Association (LISA) statistic to identify areas with statistically significant crime rates, contrasting the clusters that emerge under the residential and ambient population denominators. The paper has been structured as follows: the following section (Background) outlines the relevant literature that this project will draw on; the Data and Methods section discusses the data and the methods used; the Results section presents the results; and the remaining two sections complete the paper with a discussion and conclusions respectively."
published_020,"It has been well established in the field of phonetics and phonology that when an utterance is produced, phonological constituents of various levels (such as syllables, words, and phrases) must be put together in a hierarchically organized way according to the prosodic structure stipulated by the grammatical system of a given language (e.g., Beckman, 1996; Shattuck-Hufnagel & Turk, 1996). A growing body of studies on the phonetics–prosody interface has further suggested that the phonetic realization of individual segments is fine-tuned systematically depending on where in a prosodic structure they occur (e.g., Cho, 2016; Fletcher, 2010). An important assumption that underlies the phonetics–prosody interface is that prosodically conditioned phonetic granularity operates systematically at the subphonemic (phonetic) level, such that phonological units are fleshed out with fine-grained phonetic content in a way that serves the linguistic functions assumed by the prosodic structure (Cho, 2011; Fletcher, 2010; Keating & Shattuck-Hufnagel, 2002), often modulating phonetic implementation of phonological contrast (e.g., de Jong, 1995, 2004; Cho & McQueen, 2005; Cho, Lee, & Kim, 2014). In the present study, we build on that premise by exploring how the phonetic implementation of phonological voicing contrast of stops in American English can be modulated by prosodic structure and how the prosodically conditioned fine-tuning of voicing contrast illuminates the phonetic nature of phonological stop voicing contrast."
published_021,"Bearings are used in virtually any piece of machinery involving relative rotation of elements to support the axial and/or longitudinal loads of a shaft while transmitting the torque to adjacent elements. This is achieved by introducing balls or cylindrical rollers between the inner and outer bearings raceways in order to transform a sliding contact into a rolling contact, which minimises the energy loss due to friction. Such geometry results in extreme contact pressures, in the order of a few GPa [1], as well as significant sub-surface shear stresses, which typically exhibit a maximum at tens to hundreds of μm below the contact surface, depending on the load, size and bearing design. The sub-surface stresses inevitably lead to fatigue damage, commonly known as spalling, unless the bearing fails earlier by one of the surface-related damage modes. Surface initiated failures in bearings are nowadays perceived as avoidable by ensuring proper lubrication [2], hence the sub-surface initiated rolling contact fatigue (RCF) remains a factor determining the ultimate life of a properly mounted and lubricated bearing [3]."
published_022,"The ability of cells to sense and align in response to the surroundings is essential for multicellular matrix formation with appropriate architecture in both the developmental stage and regeneration processes. In particular, the characteristic texture of bone tissue derived from collagen/apatite [1–3] is determined by the osteoblast directional behaviors [4,5]. We have clarified that osteoblasts produce bone matrix along cell alignment; even the crystallographic texture of apatite is regulated by the degree of cell alignment [6]. On the other hand, our recent work surprisingly unveiled a quite unique phenomenon characterized by the construction of bone matrix orthogonal to osteoblast alignment, which was induced by a nanogrooved structure [7]. This finding challenges the classical belief that the cell-produced matrix orientation follows the cellular direction in extracellular matrix (ECM) assembly [8–10]. It is expected that cells contain intrinsic molecular regulatory systems determining parallel or perpendicular bone matrix construction in response to the substrates surface. Cellular recognition of the substrate surface is mediated by focal adhesions (FAs), which are assembled multiprotein structures that include transmembrane receptor integrin, which is composed of nanoscale head and tail structure [11]. FA formation is controlled by the integrin clustering procedure mediated by multiple associated proteins [12]. The function of FAs is regulated by the dynamic maturation process in which FAs undergo morphological and compositional change driven by cellular tension; FA maturation proceeds to the formation of fibrillar FAs, which are responsible for the structural regulation of the ECM [13]. The aim of the present study is to clarify a molecular mechanism underlying our previous finding of cellular organization of bone matrix orthogonal to cell alignment [7]. Here, we focused on the morphological change of FAs as well as the cellular alignment in response to nanoscale geometry. Specifically, the osteoblasts aligned along the nanogrooves expressed supermature fibrillar FAs. Moreover, comprehensive microarray analysis enabled the identification of genetic cues triggering the uniquely oriented bone matrix formation. The identified Tspan11 significantly contributed to the perpendicular bone matrix construction by regulating the integrin clustering. The present findings of molecular regulatory systems for anisotropic bone matrix formation may lead to the development of novel biomaterials and therapeutic targets for recovery of healthy bone with appropriate matrix alignment."
published_023,"Individual differences in intelligence influence developmental trajectories across the lifespan, affecting socioeconomic, psychological, and health outcomes (Deary, 2012). Differences in intelligence have been shown to be highly stable from early adolescence to late adulthood (Deary, Pattie, & Starr, 2013), but are more variable in infancy and childhood, with some children showing substantial gains in intelligence and others considerable losses between infancy and adolescence (Bayley, 1955; Feinstein, 2003; Tucker-Drob & Briley, 2014). These variations in the development of intelligence are likely to be associated with children's family socioeconomic status (SES; e.g. Dyume, Dumaret, & Tomkiewicz, 1999; Heckman, 2006; Tucker-Drob, Rhemtulla, Harden, Turkheimer, & Fask, 2011). Children from disadvantaged family backgrounds score on average lower on intelligence tests than their high SES peers (Bradley & Corwyn, 2002; Schoon, Jones, Cheng, & Maughan, 2012; Strenze, 2007), and their performance has been suggested to worsen over time, even if they did relatively well in early assessments (Feinstein, 2003). Conversely, high SES children are thought to gain in intelligence over time, even if they initially had a lower test score (Feinstein, 2003). However, research to date on the impact of SES on developmental change in intelligence is inconclusive for two reasons. First, it has been suggested that the previously reported association between SES and children's IQ development results from regression to the mean, because children with either extremely high or low scores in early IQ tests are less likely to score as extremely in later tests, independent of their family background (Jerrim & Vignoles, 2011; Saunders, 2012). Regression to the mean occurs when children are grouped according to their scores at one measurement occasion and then the groups' development is analyzed across subsequent assessments. This statistical artifact can be avoided by applying latent growth curve (LGC) models to non-selected samples, because LGC analyzes longitudinal data at the level of individuals rather than groups. Second, most previous studies included intelligence assessments at relatively few ages and at short age intervals in early life (Feinstein, 2003; Spinath, Ronald, Harlaar, Price, & Plomin, 2003; von Stumm, 2012; see Tucker-Drob & Briley, 2014, for a review). No study to date has modeled change in intelligence in a representative sample from infancy through late adolescence, using multiple assessments of intelligence over time that allow for identifying individual differences in developmental trajectories. Overcoming these limitations, in the present study we fitted LGC models to intelligence data from the Twins Early Development Study (TEDS), whose participants were assessed 9 times on intelligence from age 2 to 16 years. We then tested the extent to which SES, as a time-invariant covariate, accounted for individual differences in slopes of change in intelligence from age 2 to 16 years, as well as individual differences in the starting points (intercepts) at the age of 2 years."
published_024,"The presence of persistent organic pollutants (POPs) in estuarine and oceanic systems is a global problem. They are ubiquitous and long-range transported, being found in many biotic and abiotic media. This property is derived from their persistence in the environment. POPs are semivolatile and hydrophobic compounds, having a great bioaccumulative capacity (Jones and de Voogt, 1999). Due to their lipophilicity and low chemical degradation rate, they tend to concentrate in lipid rich tissues of organisms and to biomagnify through food webs (Guzzella et al., 2005). Many POPs are listed as possible carcinogens and are of serious concern to human and environment health. Organochlorine compounds (OCs), such as polychlorinated biphenyls (PCBs) and organochlorine pesticides (OCPs), and polycyclic aromatic hydrocarbons (PAHs) are important groups of POPs. The list of POPs keeps, however, continuously growing. Different groups of brominated flame retardants have been commercialized to prevent the development of fire (Wilford et al., 2004), among them, polybrominated diphenyl ethers (PBDEs), hexabromocyclododecanes and tetrabromobisphenol A. The commercial formulations of PBDEs includes penta, octa and deca-BDE technical mixtures. As classical POPs, these compounds have several toxicological effects and a great bioaccumulation potential (Strid et al., 2013). Because of that, the usage of PBDEs has been regulated as regards commercialized mixtures and articles in Europe and USA (CSA, 2003; EC, 2003; ECR, 2008). Furthermore, several congeners of the penta-BDE and octa-BDE mixtures have been added to POPs list of the Stockholm Convention (UNEP, 2009). As a consequence, these flame retardants are being replaced by other compounds, generally called “novel” flame retardants (NFRs) (Betts, 2008). Some of the most important representative NFRs are decabromodiphenylethane (DBDPE) as a substitute of deca-BDE and with similar physicochemical properties, 1,2-bis(2,4,6-tribromophenoxy)ethane (BTBPE) replacing octa-BDE, 2-ethylhexyl-2,3,4,5-tetrabromobenzoate (EHTBB) used mostly in PVC, bis(2-ethylhexyl)-3,4,5,6-tetrabromo-phthalate (DEHBTP) which are used in replacement of penta-BDE, and tris(tribromoneopenthyl)phosphate (TTBPP). Another compound which is considered as hazardous to the environment and that has not yet been regulated is Dechlorane plus (DP). It has been used during decades but had received little attention until it was detected in different compartments of the environment (Gauthier and Letcher, 2009; Hoh et al., 2006; Tomy et al., 2007). DP is a chlorinated compound that can be included in the term NFRs. These NFRs are not regulated for their use or production, nor even regarding maximum tolerance levels in foodstuffs, being their presence in seafood literature scarce. Perfluorinated compounds (PFCs) comprise a diverse group of chemicals including perfluoroalkyl sulphonates (PFASs), such as perfluorooctane sulphonate (PFOS), and perfluorinated carboxylic acids (PFCAs), such as perfluorooctanoic acid (PFOA). These compounds are constituted by a hydrophobic alkyl chain of varying length (typically C4 to C16) and a hydrophilic end group (de Voogt and Saez, 2006). PFCs have been used in different commercial and industrial applications, such as: surfactants and surface protectors in paper, leather, carpets, upholstery, paints, lubricants, polishers, food packaging, and fire-fighting foams including aqueous film forming foams (Kissa, 1994). PFCs are resistant to hydrolysis, photolysis, biodegradation, and metabolism. These characteristics explain the environmental persistence and bioaccumulative potential of PFCs, of which PFOS and PFOA are the two most commonly reported in the environment (Powley et al., 2005; Tseng et al., 2006). Due to these characteristics the EU banned most uses of PFOS and related compounds in 2008 (EC, 2006b). Furthermore, in May 2009, PFOS was included in Annex B of the Stockholm Convention on POPs (UNEP, 2009). Ultraviolet (UV) filters are compounds used in sunscreens and cosmetics to prevent chemical degradation and skin damage under sunlight irradiation (Manova et al., 2013). They are considered as emerging environmental pollutants. Their worldwide usage, ubiquitous presence in water samples and endocrine disruption activity of certain species are some of the issues which have raised the concern about the long-term impact of UV filters in the environment (Giokas et al., 2007; Giraldo et al., 2017; Krause et al., 2012; Paredes et al., 2014; Ramos et al., 2015). Galicia in north-west Spain, is the second largest mussel producer in the world; with a production that has surpassed 200,000 t annually (Caballero Miguez et al., 2009). This causes that Galician administrative authorities have the duty to control and ensure the quality of shellfish that is produced in its coast. Moreover, bivalve molluscs are an important filter feeding organisms. They have been used as bioindicators in environmental monitoring programs due to several characteristics such as, resistance to stress, sessile behaviour, tolerance to salinity changes and capacity to accumulate contaminants at levels higher than found in marine water (Philips and Rainbow, 1993). They are considered as an indicator of environmental contamination in several phases, as they are exposed to seawater and sediment. Thus, it is of wide interest the monitoring of levels and trends of classical and emerging pollutants, as the above mentioned ones. For instance, since 1986 the U.S. National Oceanic and Atmospheric Administration (NOAA) National Status and Trends (NS&T) Mussel Watch Program carries out a yearly nationwide sampling to collect bivalves, Mytilus edulis, or similar species, as sentinel organism, from sites along the USA coast to assess the status and long-term trends of selected contaminants (approximately 140 analytes) in coastal marine environments (Bricker et al., 2014). More recently, to expand the utility of the Mussel Watch Program, several agencies in California partnered with NOAA to design a pilot study that targeted contaminants of emerging concern, such as PBDEs and PFCs (Dodder et al., 2014; Maruya et al., 2014a; Maruya et al., 2014b). Classical POPs distribution has been previously studied in the Galician coast (Bellas et al., 2014; Bellas et al., 2011; Carro et al., 2014; Carro et al., 2010, 2015; Fernandez et al., 2013). However, until now only a few works have been published dealing with concentration and distribution of PBDEs in Galician molluscs (Bellas et al., 2014). Regarding NFRs, PFCs and UV filters, no data is available for these geographical area, and only few published papers focused in the development of analytical methodologies, have reported some concentrations (Negreira et al., 2013; Villaverde-de-Saa et al., 2012; Villaverde-de-Saa et al., 2013). In fact, the worldwide number of studies on the distribution of such chemicals (particular if studied together) is also very scarce. Bearing that in mind, this study aimed to investigate the distribution and profile of PBDEs and NFRs, PFCs and UV filters in bivalve mollusc samples (Mytilus galloprovincialis, Cerestoderma edule and Ruditapes decussatus) collected in nine Galician Rias during the period February 2012 to February 2013. Classical POPs, PCBs, OCPs and PAHs, have also investigated not only to study their distribution but for comparison to emerging pollutants."
published_025,"The mechanical behaviour of quasi-brittle materials, such as concrete, graphite, ceramics, or rock, emerges from underlying microstructure changes. The meaning of microstructure differs for different media and could be characterised by intrinsic length scales. For concrete these could be aggregate sizes and inter-aggregate distances, while for rock systems these could be sizes of blocks formed between existing small fractures. At the engineering length scale, orders of magnitude larger than the microstructure scales, the mechanical behaviour can be described with continuum constitutive laws of increasing complexity combining damage, plasticity and time-dependent effects [1–4]. In these phenomenological approaches damage represents reduction of the material elastic constants. From microstructure length scale perspective damage in quasi-brittle media is introduced by nucleation and evolution of micro-cracks, where a micro-crack means a fracture of the order of the microstructure scale(s). Potentially the effect of the micro-cracks formed under loading could be captured by continuum damage models calibrated against engineering scale experiments. The phenomenology, however, cannot help to understand the effects of the generated micro-crack population on other important physical properties of the material. In many applications the quasi-brittle materials have additional functions as barriers to fluid transport via convection, advection, and/or diffusion. It is therefore important to take a mechanistic view on the development of damage by modelling the evolution of micro-crack population. This can inform us about changes in the transport properties. Such a mechanistic approach needs to account for the microstructure in a way corresponding to the micro-crack formation mechanism [5]. In concrete, micro-cracks typically emerge from pores in the interfacial transition zone between the cement paste and aggregates [6]. In rock systems, micro-crack generating features could be existing fractures as well as pores. Concrete will be used in this work to demonstrate the proposed methodology, because data for the micro-crack initiating features in this material is readily available. Discrete lattice representation of the material microstructure seems to offer the most appropriate modelling strategy for analysis of micro-crack populations. Discrete lattices allow for studies of distributed damage without constitutive assumptions about crack paths and coalescence that would be needed in a continuum finite element modelling. For lattice construction, the material is appropriately subdivided into cells and lattice sites are placed at the cell centres. The deformation of the represented continuum arises from interactions between the lattice sites. These involve forces resisting relative displacements and moments resisting relative rotations between sites. Two conceptually similar approaches have been proposed to link local interactions to continuum response. In the first one, the local forces are related to the stresses in the continuum cell, e.g. [7,8]. In the second one, the interactions are represented by structural beam elements, the stiffness coefficients of which are determined by equating the strain energy in the discrete and the continuum cell, e.g. [9,10]. In both cases explicit relations between local and continuum parameters can be established for regular lattices. It has been previously shown that regular 3D lattices based on simple cubic, face-centred cubic and hexagonal closely-packed atomic arrangements can be used to represent materials exhibiting cubic elasticity. However, the only isotropic materials such lattices could represent are materials with zero Poisson’s ratio [11]. A bi-regular lattice that can represent all materials of practical interest has been proposed recently [12]. It can be seen as a lattice based on body-centred cubic atomic arrangement with two types of links - between neighbours along body diagonals (nearer) and between neighbours along cell edges (further). This lattice, currently formed by beams clamped at sites, is used in the current work together with microstructure data for concrete obtained with X-ray computed tomography. Failure models based on microstructure data and the novel lattice have been previously used for modelling tensile and compressive behaviour of cement [13] and the compressive behaviour of concrete under various complex loading conditions [14]. This work makes a step into developing our understanding of the micro-crack population and its relation to macroscopic damage. Further, the structure of the micro-crack population will provide the means to study the changes in transport properties with damage in future studies."
published_026,"Savanna type ecosystems play an important role in global carbon stocks and their productivity (Ahlstrom et al., 2015; Grace et al., 2006), they are highly variable in seasonal carbon and water vapor fluxes (Eamus et al., 2013; Paço et al., 2009; Tagesson et al., 2015; Unger et al., 2012) and inter-annual (Chen et al., 2016; Costa-e-Silva et al., 2015; Dubbert et al., 2014; Ma et al., 2007; Nagler et al., 2007; Pereira et al., 2007) time scales. Savannas are complex ecosystems which consist of scattered trees and a coexisting continuous grass layer/understory (Scholes and Archer, 1997). The relative contribution of trees and the understory to overall ecosystem fluxes experience strong seasonal variations, and can vary substantially depending on the savanna type and its characteristics (e.g. Dubbert et al., 2014; Moore et al., 2016; Otieno et al., 2015; Paço et al., 2009). Within a savanna ecosystem, the spatial distribution of trees and the composition of the understory can result in spatial heterogeneity of biophysical properties and ultimately fluxes. To determine spatial flux heterogeneity on the ecosystem scale, flux footprint models offer a great possibility as they allow relating flux measurements and therefore flux variability, to surface properties. When detailed spatial information on surface characteristics are available for the footprint area, they can be utilized to explain spatial flux variability. Due to the intrinsic uncertainties associated with the eddy covariance (EC) technique, the observed variability in flux measurements is not entirely related to spatial heterogeneity. Rannik et al. (2016) reviewed different flux error estimates for EC measurements and compared their values for different ecosystems. They suggested to use the method proposed by Finkelstein and Sims (2001) or Wienhold et al. (1995) to estimate the random uncertainty of turbulent flux measurements. Both methods can be used to associate uncertainties to each individual flux averaging period. These uncertainty estimates account for the properties of the measured time series (vertical wind speed and scalar of interest), but do not integrate information about the observed variability in flux measurements observed under similar meteorological conditions and phenological stages. Two widely used methods to quantify this uncertainty are the standard deviation of the marginal distribution sampling (MDS; Reichstein et al., 2005) and the two-tower-approach (TTA; Hollinger and Richardson, 2005; Kessomkiat et al., 2013). The MDS is based on the assumption that, for a short time window and under the same meteorological conditions, fluxes should be similar. The TTA uses two co-located towers (i.e. a few hundred meters apart), which are sampling independent areas of the same homogeneous ecosystem and compares the differences of the simultaneous flux measurements. For the TTA differences in meteorological conditions are (nearly) completely eliminated and differences in phenological stage and biophysical properties should be minimized. Nevertheless, biophysical properties within the footprint area (Schmid, 2002) can change spatially and, therefore, influence flux measurements and increase the uncertainty estimate. High spatial resolution remote/proximal sensing provides means to better characterize and quantify spatial heterogeneity. For example, Balzarolo et al. (2015) and Perez-Priego et al. (2015) showed for grasslands, that variability in measured CO2-fluxes can be related to changes in vegetation indices (VIs), which were derived from hyperspectral measurements. The best agreement was found between CO2-fluxes and VIs associated to chlorophyll and water-content of the canopy, as well as sun-induced chlorophyll fluorescence. For savanna type ecosystems, where a multispecies herbaceous layer (annual grasses, forbs, and legumes) coexists with sparsely distributed trees, spatial heterogeneity of e.g. chlorophyll content of the vegetation and leaf area index (LAI) introduce a new dimension to account for. Variations of biophysical properties can occur at the herbaceous- and tree layer, or as a consequence of changes in the tree density and canopy fraction within the footprint area. From this point of view, it is not clear how representative flux measurements can be in such a complex ecosystem to represent ecosystem scale fluxes correctly. To be precise, this is not a special problem of savanna type ecosystems but to all EC-sites presenting significant variability in biophysical properties at EC footprint scale. This work focuses on the analysis of data collected with three co-located EC flux towers within the framework of a fertilization experiment (Migliavacca et al., 2017). Here, only the data acquired before the fertilization are analyzed. The main objective is to evaluate the causes of differences between the simultaneous, half hourly flux measurements collected by the three co-located EC towers and to identify the main factors, (especially random errors vs. variability in biophysical surface properties), controlling the variability of measured carbon-, water-, and energy fluxes. For this purpose we (i) conduct a thorough uncertainty analysis including three different methods and (ii) make use of a combination of EC measurements, high resolution airborne hyperspectral information, and footprint analysis to identify spatial heterogeneity of mass and energy fluxes, and to correlate spatial flux differences with VIs derived from hyperspectral data and surface properties."
published_027,"The use of diverse forms of visualization as a communication tool in the planning process of landscape and architecture projects is growing in popularity (e.g. Gill, Lange, Morgan, & Romano, 2013). In recent years, this is driven by the need to find more effective means of public participation (e.g. Lange & Hehl-Lange, 2005). Developments in the use of visualizations are aided by continuing advances in computer processing power and readily available software. The effectiveness of visualizations as a communication tool and issues which arise with their use has been subject of previous research which has focused on response equivalence and audience (Sheppard, 2001; Wergles & Muhar, 2009), realism and viewer perception (Lange, 2001) as well as on a lack of standard production methodology and assessment criteria (MacFarlane, Stagg, Turner, & Lievesley, 2005; Sheppard, 2001). Generally, methods for the assessment of existing landscapes and proposed futures using landscape visualizations (see e.g. Daniel, 2001; Lange & Legwaila, 2012; Ribe, Armstrong, & Gobster, 2002; Zube, Sell, & Taylor, 1982) can be grouped into quantitative perceptual (asking people about ‘judgments’), qualitative perceptual (asking people to describe differences between the presented stimuli), quantitative analytical (developing metrics to estimate the degree of differences e.g. in before/after images) and qualitative analytical (describing objective differences between images) approaches. Little conclusive research has been carried out to compare ex ante visualizations introduced during the planning phase of landscape and architectural projects with ex post photography of the finished site. This paper outlines a purposive critique of visualizations produced for specific landscape and architectural projects. This was achieved through qualitative analysis of the ex ante visualizations (Before) compared to ex post photographs of the completed sites (After)."
published_028,"The Spinosauridae is a diverse family of theropod dinosaurs including ten described genera (Taquet & Russell, 1998; Buffetaut & Ouaja, 2002; Kellner et al., 2011; Allain et al., 2012; Buffetaut, 2012; Malafaia et al., 2013; Evers et al., 2015) found across five continents (Ruiz-Omeñaca et al., 2005; Keller et al., 2011; Barrett et al., 2011; Buffetaut, 2012; Allain et al., 2012; Candeiro et al. 2017; Gasca et al., 2018). Spinosaurs are known from the Berriasian (Sales et al., 2017) to the Cenomanian (Buffetaut & Ouaja, 2002), and are particularly common in the middle Cretaceous (Cenomanian) aged deposits of North Africa, including Egypt (Stromer, 1915), Algeria (Taquet & Russell, 1998; Benyoucef et al., 2015), and Morocco (Buffetaut, 1989; Ibrahim et al., 2014a, 2014b). Within the Kem Kem region are two distinct morphologies, originally assigned only from cervical vertebrae (Evers et al., 2015), but also recently from portions of the skull (Hendrickx et al., 2016). These are Sigilmassasaurus brevicollis (=Spinosaurus maroccanus; Evers et al., 2015) and a second spinosaurid taxon, possibly Spinosaurus (Hendrickx et al., 2016) or a relative (Evers et al., 2015). Spinosaurs are thought to have been piscivorous on the basis of their elongated, crocodile-like skulls and their conical, spearing teeth (Sues et al., 2002; Buffetaut, 2007; Hasegawa et al., 2010; Richter et al., 2013). Furthermore, an intricate series of canals across the distal parts of the upper and lower jaws, interpreted as a system of pressure receptors, have been cited as an adaptation for hunting aquatic prey (Ibrahim et al., 2014b; Vullo et al., 2016; Arden et al., 2018), though this is disputed (Leich & Catania, 2012; Barker et al., 2017; Henderson, 2018). The skeletal morphology of spinosaurines suggest a semiaquatic lifestyle, similar to extant crocodilians (Amiot et al., 2010; Ibrahim et al., 2014b; Goedert et al., 2016; Aureliano et al., 2018). Adaptations for aquatic locomotion include retracted nares (Dal Sasso et al., 2005), a reduced pelvis and hindlimbs, and pachyostotic long bones (Ibrahim et al., 2014b; Aureliano et al., 2018). Spinosaurs are the largest known theropod dinosaurs (Dal Sasso et al., 2005; Therrien & Henderson, 2007; Gimsa et al., 2016). The largest specimen currently known is MSNM V4047 referred to Spinosaurus aegyptiacus (Dal Sasso et al., 2005), which has an estimated skull length of 175 cm and an estimated body length of 12–15 m (Therrien & Henderson, 2007). Other specimens, including the holotype of S. aegyptiacus, have estimated total body lengths of 10–14 m (Therrien & Henderson, 2007; Ibrahim et al., 2014b). Like all dinosaurs, however, giant spinosaurs would have hatched from relatively small eggs before ultimately growing to adult size. Fossils of juvenile dinosaurs are generally very rare (Lockley, 1994; Hone & Rauhut, 2010), and juvenile spinosaurs have only recently been described for the first time (Maganuco & Dal Sasso, 2018). However, little is still known about the growth and life histories of giant spinosaurs. Small spinosaurine teeth are common from Kem Kem deposits (Richter et al., 2013), however, these cannot be reliably used for size or age estimations because of the variation in spinosaur teeth within regions of the jaw (Monfroy, 2017), and the polyphyodonts nature of all theropods (Erickson, 1996; Therrien et al., 2005). Here, we describe new material of juvenile spinosaurs. These include a quadrate, cervical and dorsal vertebrae, and a premaxilla, all found in the Kem Kem beds of eastern Morocco near the towns of Begaa and Erfoud."
published_029,"The use of resources by plants is of fundamental importance to understand ecological processes in natural landscapes (Ferreira et al., 2007; Higgins et al., 2011), particularly those associated with the maintenance of species diversity (Mckane et al., 2002) and multiple patterns of vegetation structure and function (Grime, 2001). A series of recent studies have related the partitioning of essential resources, such as nutrients and water, with plant species diversity and co-existence of contrasting life-forms (Désilets and Houle, 2005; Saha et al., 2009; Staver et al., 2011; Verweij et al., 2011; Hold, 2013). Resource use and niche partitioning appear to be especially relevant in vegetation physiognomies where a large number of species occur side by side and where environmental filters (e.g. abiotic constraints) shape vegetation structure, composition and function (Higgins et al., 2011; Sales et al., 2013). The Neotropical savannas of Brazil (regionally called “Cerrado”) are characterized by contrasting environmental conditions, where strong seasonality, high evaporative demand, low soil nutrient availability and frequent fire events explain the long-term persistence of vegetation mosaics (Eiten, 1972; Gottsberger and Silberbauer-Gottsberger, 2006; Silva et al., 2013). As in many other tropical regions, natural landscapes of central Brazil comprise a wide range of vegetation physiognomies, ranging from open grasslands to savanna woodlands, readily identifiable by shifts in the proportion of woody plants, which may occur densely or sparsely distributed within a continuous herbaceous matrix composed mainly by eudicots herbs and grasses (Oliveira-Filho and Ratter, 2002). The variability in physiognomic types often appears along shallow topographic gradients, where a mosaic of plant communities co-exists as a consequence of the wide range of interactions between water flow, soil properties, nutrients and fire frequency that change even with small differences in topography (Oliveira-Filho et al., 1989; Rossatto et al., 2012). In these gradients, vegetation structure and species composition vary greatly across the landscape, and structural changes tend to follow diversity patterns of woody and herbaceous species distribution (Oliveira-Filho and Ratter, 2002). In the upper portions of the gradient, characterized by deep oxysols, dense woody savannas are the most common vegetation type (Silberbauer-Gottsberger and Eiten, 1987). The density of woody species decreases as soils become shallower towards lower elevations, where more open shrubby savanna formations with a few trees and richer eudicots herbaceous communities appear, and eventually give place to wet grasslands at the lowest elevations near riparian zones and seasonally flooded soils (Eiten, 1972; Silberbauer-Gottsberger and Eiten, 1987; Furley, 1999). Variations in density of woody species in Neotropical savannas have long been thought to represent the result of competition for resources with herbaceous species (Eiten, 1972; Medina and Silva, 1990). Alternatively, changes in soil properties and groundwater depth (Oliveira-Filho et al., 1989; Villalobos-Vega, 2010; Rossatto et al., 2012) combined with fire frequency (Oliveira-Filho and Ratter, 2002) and nutrient limitation (Silva et al., 2013), could explain the persistence of contrasting vegetation forms. Water availability is considered to be one of the most important factors constraining woody cover in savannas (Sankaran et al., 2005), but this has not been tested separately from other environmental filters (i.e. nutrient limitation and fire frequency), limiting predictions of vegetation shifts based on water regime alone. It is well known that the water niche partitioning plays an important role in the co-existence of diverse plant species in a wide variety of vegetation types (Walter, 1971; Nippert and Knapp, 2007; Eggemeyer et al., 2008; Ward et al., 2013). This potential mechanism of co-existence has been extensively explored in natural systems dominated by tree species (Schenk and Jackson, 2002; Verweij et al., 2011) or to explain tree–grass co-dominance in dry savannas (Hold, 2013; Ward et al., 2013), however whether similar mechanism could explain the co-existence of trees and diverse communities of eudicot herbs as typical of mesic and humid Neotropical savannas are yet to be determined (Rossatto et al., 2013). Here we test whether woody and herbaceous eudicot plants compete for water resources across a topographic gradient consisting of distinct vegetation physiognomies typical of Neotropical savannas. The estimation of the depth of plant water uptake was based on comparisons of oxygen isotope ratios of plant stem water and soil pore water collected at different depths (Dawson et al., 2002). Nutritional and fire pressures are similar across the gradient and water depth is considered the main driver of vegetation change at the study site. We expect to find that the preferential depth of water uptake will differ between woody and herbaceous communities only in upper slope vegetation, where groundwater level is deeper (Rossatto et al., 2012; Rossatto et al., 2013) and woody species, which typically have better developed root systems (Oliveira et al., 2005), will be able to exploit deeper layers of the soil profile. As a result, we expect to demonstrate that competition for water is stronger in lower elevation plant communities, where groundwater depth decreases (Rossatto et al., 2012) and the exploitable soil layers reduced in comparison with upper slope vegetation."
published_030,"Lack of water resources and high water salinity levels are among the most important growth-restricting factors for plants species in some arid and semi-arid regions of the world (Levitt, 1980). Knowledge of relative salinity and drought tolerance among turfgrass species/cultivars is important for selecting turfgrasses that persist during drought and salinity stress. Turfgrasses are the most important groundcover-like plants in the world. Common bermudagrass (Cynodon dactylon [L.] Pers.) is widely distributed throughout the world between 45°N and 45°S (Harlan and de Wet, 1969). Cynodon spp. is frequently used in the transition zone where it can provide an excellent surface for golf course fairways and athletic fields (Munshaw et al., 2006). Salinity tolerance in different bermudagrass species ranges from 6 to 10 dS m− 1; and are classified as semi-tolerant to tolerant species to drought stress (Kamal Uddin et al., 2012). Drought or salinity tolerance, especially in grasses, depends on plant morpho-physiological features (Bahrani et al., 2010). Although the general effects of drought and salinity stresses on plant growth and development have been studied, their influence at the physiological and biochemical levels is not well understood (Jaleel et al., 2008). However, Du et al. (2012) studied metabolic responses of hybrid bermudagrass to short-term and long-term drought stresses. Salinity stress may result in the destruction of osmotic balance and ion toxicity (Turkan and Demiral, 2009). Studies have shown that the ability of plants to tolerate saline conditions, may alleviate the adverse effects of stresses such as salinity and drought (Kamal Uddin et al., 2012). The plants are able to ameliorate the stresses by synthesizing metabolites such as protein to prevent enzyme degradation through reduction of turgor changes within the cells (Kamal Uddin et al., 2012; Harivandi, 1988). Therefore, limited changes occur in growth rate compared to sensitive plants (Harivandi, 1988). Salinity and drought stresses were reported to act via overproduction of reactive oxygen species (ROS) (Gomez et al., 2004) which leads to oxidative stress (Mane et al., 2011). Accumulation of ROS may cause destruction of cell structures and molecules such as proteins and nucleic acids (Mittler, 2006). Hu et al. (2012) determined that ROSs are harmful to membranes, thus to scavenge them, plants have a well-developed complex antioxidant defense system including enzymatic and non-enzymatic antioxidant processes. The antioxidant enzymes that are found in plants include SOD, CAT, POD and APX (Apel and Hirt, 2004). Hu et al. (2012) further suggested that the activity of these antioxidants increased in some Lolium perenne L. cultivars after 4 days of treatment with 250 mM NaCl. DaCosta and Huang (2007) observed the same using drought stress, but stated that severe drought stress resulted in the reduction of antioxidant enzymes in Agrostis spp. Recent studies of salinity or drought effects on turfgrasses have focused on growth response mechanisms (Marcum, 1999). This research was designed to investigate the effects of combined drought and water salinity stresses on common bermudagrass and to investigate their biochemical and physiological responses under these conditions."
published_031,"This study investigated cyclooxygenase-1 (COX-1) inhibitory activity of 17 West African plants as part of a larger research collaboration examining historical medicinal plants in Ghana (Soelberg et al., 2015). In the time of the Atlantic slave trade, ethnomedicinal uses were recorded in 1697, 1803 and 1817 among the Fante, Ga and Ashante people respectively (Petiver, 1697; Bowdich, 1819; Schumacher, 1827). Plant species with historical uses, which could indicate inhibition of the COX-enzymes were included in the present study. Many of the plants were used for ‘old leg injury’, which is the wound left after a guinea worm infection. The COX enzymes convert arachidonic acid to prostaglandins. Prostaglandins are involved in the complex process of inflammation and are also responsible for the sensation of pain. Inhibition of the prostaglandin synthesis will therefore result in pain relief and reduce the inflammation in inflamed tissue. There are at least two iso-enzymes of COX, COX-1 and COX-2, where COX-1 is constitutively expressed and COX-2 is induced during inflammation. Inhibitors of both forms have side effects, COX-1 inhibitors especially on the gastro-intestinal tract, and COX-2 on the heart, with fatal cases, which has lead to restriction on use and withdrawal from the market of some COX-2 inhibitors. A number of studies have shown that plants possess inhibitory activity against the COX enzymes (Jäger et al., 1996; McGaw et al., 1997; Noreen et al., 1998; Jeppesen et al., 2012). Such plants may therefore be an option in treatment of pain and inflammatory ailments."
published_032,"Overproduction of free radicals and reactive oxygen species (ROS) has been confirmed in a human body due to the perturbation of various metabolic reactions (Hancock et al., 2001). Free radicals and ROS are generated through normal reactions within the body during respiration in aerobic organisms which can exert diverse functions like signaling roles and provide defense against infections (Hancock et al., 2001). However, many degenerative human diseases including cancer, cardio- and cerebrovascular diseases have been recognized being a possible consequence of free radical damage to lipids, proteins and nucleic acids (Choi and Lee, 2009). Natural antioxidants protect the living system from oxidative stress and other chronic diseases, therefore they can play an important role in health care system (Lopez et al., 2007). The food industry has long been concerned with issues such as rancidity and oxidative spoilage of foodstuffs (Shahidi and Wanasundara, 1992). The auto-oxidation of lipids during storage and processing resulting in the formation of various free radicals, is the major reaction responsible for the deterioration in food quality affecting the color, flavor, texture and nutritive value of the foods (Choi and Lee, 2009). Hence, antioxidants are often added to foods to prevent the radical chain reactions of oxidation by inhibiting the initiation and propagation steps leading to the termination of the reaction and a delay in the oxidation process (Thorat et al., 2013). Synthetic antioxidants such as butylated hydroxytoluene (BHT), butylated hydroxyanisole (BHA) and tert-butylhydroxyquinone (TBHQ) effectively inhibit the formation of free radicals and lipid oxidation. However, these frequently used synthetic antioxidants are restricted by legislative rules due to their being toxic and carcinogenic by nature (Shahidi and Wanasundara, 1992). Therefore, there has been a considerable interest in food practices and a growing trend in consumer preferences for using natural antioxidants over synthetic ones in order to eliminate synthetic antioxidants in food applications (Thorat et al., 2013), giving more emphasis to explore natural sources of antioxidants. This has led to develop a huge working interest on natural antioxidants by both food scientists and health professionals. Nowadays, there has been a convergence of interest among researchers to find out the role of natural antioxidants in the diet and their impact on human health (Formanek et al., 2001). Metasequoia glyptostroboides Miki ex Hu is a deciduous coniferous tree of the redwood family, Cupressaceae. This species of the genus Metasequoia has been propagated and distributed in many parts of Eastern Asia and North America, as well as in Europe. Previously we reported various biological properties of various essential oils derived from M. glyptostroboides such as antibacterial (Bajpai et al., 2007a), antioxidant/antibacterial (Bajpai et al., 2009a), antidermatophytic (Bajpai et al., 2009b) and antifungal (Bajpai et al., 2007b) activities. In addition, the antibacterial activities of terpenoid compounds isolated from M. glyptostroboides have also been reported against foodborne pathogenic bacteria (Bajpai et al., 2010; Bajpai and Kang, 2011a, 2011b). The biological efficacy of M. glyptostroboides has been reported previously in vitro and in vivo both, however, no research has been reported on the antioxidant and free radical scavenging efficacy of taxoquinone from M. glyptostroboides. Hence, in our continuous efforts to investigate the efficacy of biologically active secondary metabolites, in this study, we assayed the antioxidant, and free radical scavenging efficacy of taxoquinone, an abietane type diterpenoid isolated from M. glyptostroboides."
published_033,"Amongst horticultural and floral crops, the orchids are inarguably one of the most charismatic, having captivated the attention of conservationists, growers, and enthusiasts worldwide. They are one of the most pampered plants and occupy a top position amongst all the flowering plants valued for cut flower and potted plants, fetching a very high price in the international market. The world consumption of orchids was reported to be valued at more than $500 million in 2000 (Hew and Yong, 2004; Wang, 2004). Apart from their ornamental value, orchids are also known for their medicinal importance, especially in the traditional systems of medicine. It is believed that the Chinese were the first to cultivate and describe the orchids, and they were almost certainly the first to describe orchids for medicinal uses (Bulpitt, 2005). As in the world's other traditional medicinal systems, African pharmacopeia orchids also form an important part of which Ansellia africana figures prominently, primarily because of its broad spectrum of medicinal properties, especially affecting the central nervous system (CNS) (Hossain, 2011; Chinsamy et al., 2014; Bhattacharyya and Van Staden, 2016). A thorough survey of literature revealed the usage of stem infusions of A. africana as antidotes to bad dreams (Hutchings, 1996). The smoke from burning roots is also used for the same purpose (Hutchings, 1996; Chinsamy et al., 2014). Recent studies have shown that A. africana has potent acetylcholinesterase inhibitory activity and can be used as an important source of various biomolecules for the treatment of Alzheimer's disease which might be the reason behind the usage of the leaves and stems for the treatment of madness by the Mpika tribes of Zambia (Gelfand, 1985). Apart from these, this orchid has also been used for various ethnobotanical purposes and has significant horticultural usage in the cut flower industry. Thus having such a wide array of usage, its natural populations are under tremendous anthropogenic pressure. Due to indiscriminate collection from the wild and heavy deforestation, the natural populations of A. africana are severely threatened and presently they are categorized as “vulnerable” in the IUCN Red Data Book (http://www.iucnredlist.org/details/44392142/0). In South Africa, the status of A. africana is “declining” according to the recent Red List of South African Plants published by South African National Biodiversity Institute (SANBI; http://redlist.sanbi.org/). In order to conserve orchids, plant tissue culture techniques have been successfully applied for their clonal propagation and conservation (Tandon and Kumaria, 1998). However, for large-scale propagation, efficiency of propagation methods along with genetic stability of the regenerated plants is of paramount importance (Haisel et al., 2001). Reports have shown that the regenerated plants might not always be clonal copies of their mother plant when passed through micropropagation pathways (Devi et al., 2014). The presence of cryptic genetic defects occurring due to somaclonal variations can deregulate the broader utility of the in vitro propagation system (Salvi et al., 2001). The occurrence of clonal variability is due to various causes of which explant source and types of plant growth regulators (PGRs) used plays a pivotal role (Devi et al., 2014). Keeping a perspective view of the various advantages, molecular markers are considered much more efficient tools in detection of clonal variability, primarily as they are not influenced by any environmental factors and also because of their high reproducibility. Traditionally, various conventional molecular markers like random amplified polymorphic DNA (RAPD), inter simple sequence repeats (ISSR), and simple sequence repeats (SSR) are used extensively in the assessment of clonal fidelity (Bhattacharyya et al., 2014, 2015; Devi et al., 2015). However, as the conventional molecular markers target a specific region of the genome, they have various limitations which have been largely resolved by the evolution of gene-targeted molecular markers such as start codon targeted polymorphism (SCoT) (Collard and Mackill, 2009; Bhattacharyya et al., 2016a, 2016b). SCoT is an extremely reliable and consistent molecular marker in which the primers have been designed in accordance with the short conserved region surrounding the ATG translation start (or initiation) codon (or translational start site, TSS). More specifically, it is a type of targeted molecular marker technique with the ATG context as one part of a functional gene; markers generated from SCoT may be mostly correlated to functional genes and their corresponding traits (Collard and Mackill, 2009; Bhattacharyya et al., 2013; Singh et al., 2014). In the recent past, SCoT marker has been widely used in the assertion of clonal fidelity in various medicinal plants including orchids (Bhattacharyya et al., 2016a, 2016b). The present study was therefore aimed at developing a genetically stable, sustainable regeneration protocol for A. africana and to assess how explant source and plant growth regulators (PGRs) influence the clonal fidelity of the micropropagated plants."
published_034,"Opuntia ficus indica (L.) is commonly planted in arid and semi-arid lands of North Africa area. Its economic and ecological importance rises from the fact that it can be used as a forage crop for sheep (Anaya-Pérez, 2001; Nefzaoui and Ben Salem, 2001; Reynolds and Arias, 2001) or as a medicinal plant (Griffith, 2004). Similarly, it is considered as a human food where the sweet fruits or the young cladodes (called nopalitos) are consumed as a vegetable crop in Mexico (Griffith, 2004). The species O. ficus indica is native to Mexico where it was domesticated by the ancient Mexicans (Kiesling, 1998; Griffith, 2004). After the discovery of America, O. ficus indica was introduced into Spain by sailors, because of its anti-scurvy properties. Afterwards, it was introduced to other parts of the world; particularly to the Mediterranean region (Kiesling, 1998). A special interest has been attached to cacti because of their particular traits, such as spiny succulent cladodes that store water and perform photosynthesis by the CAM pathway metabolism (Nobel, 2001). O. ficus indica reproduces sexually and propagates vegetatively (Reyes-Agüero et al., 2005), where outcrossing (allogamous) is common among cacti (Pimienta and del-Castillo, 2002). It may lead to a certain degree of genetic diversity to this species. Generally, the cactus is present in different climatic ecosystems throughout the world. The greatest diversity of the cactus family is recorded in Mexico; followed by Brazil, Bolivia and Peru (Ortega et al., 2010). This diversity was positively associated with some environmental factors such as temperature and precipitation (Mourelle and Ezcurra, 1997). According to Mourelle and Ezcurra (1996), the cactus diversity of articulated species was positively related to the amount of rainfall in summer and the annual mean temperature. The domestication process of Opuntia was begun by producing spineless cladodes with large sweet fruits (Colunga Garcia Marine et al., 1986). The partial or total absence of spine is the main diagnostic character of O. ficus indica (Reyes-Agüero et al., 2005), where it has been mistakenly related to Opuntia ameclya, Opuntia megacantha and Opuntia streptacantha (Kiesling, 1998; Labra et al., 2003). O. ficus indica was considered as a synonym of O. megacantha since the presence or absence of spines is insufficient for separating them (Benson and Walkington, 1965). Based on the combination of the differential vegetative and reproductive characters, Reyes-Agüero et al. (2005) considered that O. ficus indica constitutes a taxonomic entity that differs from O. megacantha and O. streptacantha. Moreover, Kiesling (1998) suggested that the spiny and spineless specimens are only forms of O. ficus indica. The inheritance mode of the spineless character has not been identified. However, reversal of spineless back to spiny forms has been reported (Mondragón-Jacobo and Pérez-Gonzalez, 2001). Kiesling (1998) noted that new spiny and spineless specimens were developed in the new locations from seedlings that exhibited relic traits of the old-world parents. Furthermore, the recovery of spiny genotypes from spineless parents suggests that the parental genotypes each contained alleles for both spinelessness and spininess; it should be possible to obtain spineless individuals if the parents yield fertile progeny and if one of the parents were spineless (Felker et al., 2006). These clues suggest the existence of recessive genes associated with spininess (Mondragón-Jacobo and Pérez-Gonzalez, 2001). Subsequently and based on their studies, Felker et al. (2006) reported that the spineless is simply inherited. However, the inheritance of some features that can be related to the spininess was not identified. Previous works on phenotypic diversity of O. ficus indica were carried out with the aims to describe the variation of some spine features within the same environment. Peña-Valdivia et al. (2008) have worked in the Regional Arid ZonesUnit (URUZA) in Mexico, where accessions from different localities and variable level of domestication are located. The agriculture practices and environmental conditions at URUZA were similar; thus, they may be promoting morphological homogeneity among Opuntia accessions. As a result, only a few morphological quantitative characteristics are significantly different among spineless and spiny accessions. Therefore, the most relevant morphological characteristics among URUZA accessions belong to areoles, like spine presence or absence (Peña-Valdivia et al., 2008). However, the Algerian accessions cultivated in different localities appear to reveal more variation between or within spiny and spineless forms. This variation may involve many traits; particularly the spine features. It seems that they could show a gradual variation in nature between and within these two forms (spiny and spineless). The main interest of this work was to evaluate the morphological diversity among O. ficus indica accessions in the Algerian steppe and assess the probable existence of a gradual variation of some morphological features within the species and the bioclimatic effect on these features."
published_035,"In recent years, sustainable food supply has become a prevalent topic for consumers, industry and the scientific community (Adams and Demmig-Adams, 2013; Aiking, 2011; Cakmak, 2002; Leiserowitz et al., 2006; Pimentel and Pimentel, 2003). With the world population projected to increase by 2.3 billion inhabitants by 2050, the need to address uncertain food supplies is required at this moment to prevent malnutrition in coming years (Bruinsma, 2009). Adams and Demmig-Adams (2013) approach the topic by considering it not a question of plant- versus meat-based diets causing the most undesirable environmental and health issues, but more the methods chosen for animal rearing or crop production which makes a greater contribution to sustainability. However, no consideration is given for the post-processing during food manufacture. Processing of food sources requires examination to yield the most value from land available for agriculture, which will be limited by urbanisation and water required for their production in the near future. To maximise the availability of nourishment for human consumption, the extraction of health-promoting components also requires optimisation. Used for a broad range of products, the soybean crop is versatile, and produces good quality protein for human consumption (Masuda and Goldsmith, 2009). Aqueous extraction of components from soybeans is currently undertaken for the production of some soy-based products, such as soymilk and tofu. The most frequent aqueous extraction protocol involves the grinding of dried soybeans with hot aqueous solution prior to the removal of insoluble fibrous material, termed okara (Giri and Mangaraj, 2012). The protein extraction yield of the described process is less than desirable, with a large fraction, typically 25–40% (dry basis) of soy proteins being separated into the fibrous by-product, okara (Campbell et al., 2011; Kasai and Ikehara, 2005; O’Toole, 1999; Rosenthal et al., 1998). Most commonly usage of the by-product is not widely employed for human consumption due to the uneconomical resale value of soy fibre products. More sustainable processing for food production could result in nutritious material being used for human sustenance (Jankowiak et al., 2014), rather than exiting the process within a waste stream, not limited just to soy milk and tofu preparation. The microstructure of the starting material for extraction, the soybean, has been well studied (Bair and Snyder, 1980; Horisberger et al., 1986; Lili et al., 2013; Rosenthal et al., 1998; Tombs, 1967). The soybean is mainly comprised of cotyledon cells; oval in shape, 15–20 μm in diameter and 70–80 μm long, and consisting of lipid and protein bodies, in the size ranges of 0.2–0.5 μm and 8–20 μm respectively (Rosenthal et al., 1998). The protein bodies account for 60–70% of the total soybean protein content, consisting of storage proteins glycinin and β-conglycinin, confirmed using an immunogold method (Horisberger et al., 1986). Techniques used to deduce the microstructure of soybeans, including electron microscopy and fluorescence microscopy, possess limiting factors, ranging from laborious sample preparation prior to visualisation and/or poor resolution, which prevent their routine use. Previously studies to investigate ways to increase the extraction yields of health-promoting components from soybeans have been conducted (Giri and Mangaraj, 2012; Rosenthal et al., 1998; Vishwanathan et al., 2011). Yet there is a lack of understanding of the microstructure of processing materials which is crucial for achieving greater extraction yields. With this information, processing which targets the exposed restraints can be employed to improve their extraction yields. In this paper we show the location of oil and protein in soy extract, soy slurry and okara, alongside the effects of heat treatment using confocal laser scanning microscopy (CLSM)."
published_036,"Mechanically stirred vessels are widely used in a broad range of chemical process industries, and their purpose varies from mixing different materials or generating solids suspensions to enhancing heat and mass transfer. The geometric configuration of both impeller and tank, together with the fluid physical properties, in particular viscosity, dramatically affect the fluid dynamics of the system. For Newtonian fluids general guidelines for the most appropriate geometric configuration in different applications are available in the literature (Paul et al., 2004; Zlokarnik, 2003; Nienow et al., 1997). Conversely, even if non-Newtonian fluids are employed in many industrial processes, less information is available about them. These fluids are complex to mix, and often sophisticated impeller designs are required (Kresta et al., 2015; Chhabra and Richardson, 2011). This is the case in the oral health industry, where highly viscous non-Newtonian fluids are used in the production of non-aqueous specialized oral care products, such as non-aqueous toothpastes. Computational fluid dynamics (CFD) is a powerful tool that yields relevant process information and permits assessing the performance of stirred vessels. The software relies on basic balance equations of fluid mechanics, such as mass, linear momentum and energy balance equations, solved on discretized fluid domains defined by the user. A number of researchers have used CFD to study the behaviour of different types of fluids in mechanically stirred tanks with different types of impellers (Chapple et al., 2002; Shekhar and Jayanti, 2002; Adams and Barigou, 2007; Bulnes-Abundis and Alvarez, 2013; Busciglio et al., 2015; Sun and Sakai, 2016; Ramsay et al., 2016). All the previous studies highlighted the importance of validating the CFD models against experimental data. There are three main methods for validating CFD models: (i) power consumption of impellers, (ii) velocity profiles and (iii) change of properties over time. These are briefly reviewed below for non-Newtonian fluid mixing. Experimentally, there are a number of different ways to measure the power required for mixing (Ascanio et al., 2004; Paul et al., 2004). In the recent literature on non-Newtonian fluid mixing, investigators have used torque meters extensively to measure power consumption mainly for validating CFD models, as in the following studies. Ameur (2015) studied the efficiency of four impeller configurations (Maxblend, gate, anchor, and double helical ribbon) for mixing yield stress fluids; he used experimental power consumption data available in the literature (Patel et al., 2012) for the anchor and Maxblend impellers to validate the model. Zhang et al. (2014) measured the power during mixing of corn-stover and water (shear thinning fluid) at three different scales (5, 50, and 500 l), and used the findings to validate a CFD model. Pakzad et al. (2013a) studied computationally and experimentally the hydrodynamics and the mixing performance of a coaxial impeller that combined Scaba and anchor geometries for the mixing of yield stress fluids. They developed a correlation for the specific master power curve (power number versus generalised Reynolds number) that applies to the particular system investigated, which they used to validate a CFD model. Apart from power consumption, velocity fields in the stirred vessel can also be used to validate CFD models. Sossa-Echeverria and Taghipour (2015) obtained velocity profiles of yield stress and shear thinning fluids stirred with three different side-entered axial flow impellers using particle image velocimetry (PIV) and evaluated the cavern formation around the impeller. Comparable results were found with the CFD model. Khapre and Munshi (2015) studied computationally the entropy generated by a Rushton turbine in a baffled tank when mixing shear thinning fluids. The computational velocity profiles were compared against experimental ones available in the literature (Wu and Patterson, 1989; Dyster et al., 1993; Venneker et al., 2010). The previous experimental studies used the Laser-Doppler Anemometry technique (LDA). More information about entropy generation in stirred tanks can be found in Naterer and Adeyinka (2009). Pakzad et al. (2013b) developed a novel and efficient coaxial impeller to agitate yield stress fluids and used CFD to aid the design of the impeller and assess the mixing performance in terms of cavern formation and destruction. The results were validated experimentally with Electrical Resistance Tomography (ERT). A number of studies have looked at the variation of properties in the mixing system over time as a validating tool for CFD models. Kazemzadeh et al. (2016) studied the mixing time and efficiency of a coaxial mixer (Scaba-anchor system) using yield stress fluids. They developed a CFD model of the mixing system, and they validated experimentally the model using the ERT technique: they injected a tracer in the mixer and they tracked the concentration of it over time at different positions of the vessel. Then, they analysed the effect of the yield stress coefficients together with the speed ratio of the two impellers on the mixing time and power consumption. Hurtado et al. (2015) developed a CFD model of a continuous stirred tank reactor (CSTR), used in wastewater treatment plants, agitated by recirculation of material. To validate the model experimentally, they introduced particles in the system and measured their concentration in the outlet line over time. Patel et al. (2015) developed a CFD model of a baffled stirred tank fitted with a Rushton turbine to quantify the mixing/stagnant volume fraction as a function of the rheological properties of yield stress materials in continuous flow. To evaluate this, a tracer was injected into the inlet and the conductivity of the mixture was measured over time in the outlet. The good agreement between experiments and modelling in the works above demonstrated the applicability of this validation methodology (that is, change of properties over time). Carbopols are high molecular weight synthetic polymers of acrylic acid that are widely used in the pharmaceuticals and consumer healthcare industries as thickening agents (Barry and Meyer, 1979). A large number of Carbopol formulations is available. Carbopol gels are usually prepared as dispersions of Carbopol molecules in water, albeit other water-miscible solvents are also used (Bonacucina et al., 2004). The rheological properties of Carbopol gels dramatically depend on the combination of Carbopol molecules and solvent, pH, and temperature. Viscoelasticity is associated to Carbopol gels (Barry and Meyer, 1979; Bonacucina et al., 2004; Coussot et al., 2009). When sufficiently high shear stresses are present, the viscous component dominates over the elastic component, and a number of studies have been presented to derive constitutive expressions to relate the viscosity of Carbopol gels to the shear rate; investigators have modelled Carbopol gels as very highly viscous shear thinning fluids or as Bingham pseudoplastic materials (Islam et al., 2004; Kim et al., 2003; Amanullah et al., 1997; Barry and Meyer, 1979). In this article, we first aim to characterise the rheology of a non-Newtonian mixture and derive a constitutive equation to model it. The mixture consists of glycerol and a polymer made of polyethylene glycol and Carbopol, which is relevant to the manufacturing of non-aqueous toothpastes. While a lot of information is available in the literature for aqueous Carbomer systems, the rheological behaviour of the combination of glycerol/Carbopol gel has not been studied before. The second objective is to use this rheological information to simulate numerically the fluid dynamic behaviour of the mixture in a stirred vessel and validate the model predictions against accurate experimental results of power consumption obtained in a stirred vessel equipped with a frictionless air bearing and a load cell. This study is part of on-going research that aims to develop a CFD model of a pilot plant mixer used to manufacture non-aqueous toothpastes. The article is structured as follows. In Section 2, the rheology study of the non-Newtonian mixture and the power consumption measurements in the stirred vessel are presented. The setup of the CFD model is then discussed in Section 3. The computational results on power consumption are finally presented and compared with the experimental findings in Section 4."
published_037,"Cervical cancer is caused by persistent infection with one of 13 sexually transmitted high-risk human papillomavirus (HPV) genotypes [1]. Globally, approximately 70% of cervical cancers are attributable to HPV genotypes 16 and 18 [2]. The World Health Organization recommends two-dose prophylactic HPV vaccination for young girls aged 9–14 years, with a 6-month interval between doses and completion prior to initiation of sexual activity [3]. Several studies—including the Costa Rica Vaccine Trial [4], a multicenter cohort study in India [5], and the industry-sponsored PATRICIA trial [6]—have indicated similar vaccine protection among those receiving one or two doses of HPV vaccine. Compared with a two-dose HPV vaccination schedule, one-dose HPV vaccination could potentially reduce programs costs, ease administration, enable the delivery of multi-cohort vaccination, and increase HPV vaccine program adoption in populations with limited access to healthcare and a high burden of cervical cancer. Yet many uncertainties remain, including the efficacy, duration of vaccine protection and value of a single HPV vaccine dose. Several vaccine trials are underway to evaluate the properties of a one-dose vaccine schedule, but it will be several years before those data become available. As we await additional empirical data, mathematical models that integrate available evidence on sexual behavior and cervical cancer natural history can project epidemiological, health, and economic outcomes over the considerable time horizon between intervention and prevention of cancer. Such analyses can help inform stakeholder decision-making in light of data gaps and uncertainties. In particular, countries eligible for funding from Gavi, the Vaccine Alliance face severe resource constraints, prompting decision makers to consider the value – or cost-effectiveness – of reduced HPV vaccine dosing schedules prior to policy adoption or augmentation. Our objective was to evaluate the long-term health and economic impacts of routine one-dose HPV vaccination, compared to (1) no vaccination and (2) two-dose HPV vaccination in the context of a low-income Gavi-eligible country."
published_038,"The UK continental shelf experiences large tidal ranges, generating periodic and locally large near-bed currents, as well as winter storm events, which generate strong near-bed currents and also wind waves. These ‘benthic storms’ are a major source of disturbance for benthic communities. The impact of these disturbances will depend on (i) the sediment type present, (ii) bottom stress and (iii) the ability of benthic organisms to cope with displacement or a rapid accretion of sediment (Cooper et al., 2007; Warwick and Uncles, 1980; Maurer et al., 1981a,b; Schratzberger et al., 2000; Dernie et al., 2003). Organisms can be threatened by movement of sediment leading to smothering, as well as by the direct impact of hydrodynamic stress in displacing anchored animals and plants. The former effect is examined in a companion paper (Aldridge et al., 2015) while this paper focuses on the direct effect of nearbed wave and current velocities. Many studies have focused on recovery of sites after anthropogenic disturbance, either following dredging for aggregate material, or the disposal of maintenance dredging material, e.g. Bolam and Rees (2003) and Bolam et al. (2004). Natural disturbances also cause resuspension and restructuring of soft sediments at the seabed (Hall, 1994; Levin, 1995). If the disturbance is weak, then some fauna can ‘dig themselves out’ of a burial, generating bioturbation but little change to the overall community (Cooper et al., 2007). After a major disturbance the benthic community recovers mainly by re-colonisation, then succession (Levin, 1995). Cooper et al. (2007) identify faunal types better suited to life in high-energy environments which display characteristics including rapid reproduction, short life span and high mobility and dispersal. The natural level of bottom disturbance determines which species will inhabit the seabed (Hemer, 2006). Herkul (2010) assesses the impacts of physical bed disturbance on sediment properties and benthic communities in the Baltic Sea. Wave exposure significantly affects the biomass and abundance of benthic animals, with recolonisation found to be higher in sheltered sites. Dernie et al. (2003) investigates the response of marine benthic communities within a variety of sediment types to physical disturbance, raising the issue that faunal recovery rates will depend on local hydrodynamics, which will be very strongly affected by changing weather conditions. This work is motivated by the potential impacts of natural disturbances on benthic habitats and communities. We aim to identify the relative impact of tides and storm events at the sea bed of the UK continental shelf by mapping the exposure over a 10-year period, and calculating a representative measure of bed disturbance. The forces generated by waves and tidal currents will be considered separately, before conclusions are drawn about their potential impact at the bed. While the disturbance generated by tides is regular and predictable, wave generated currents can be produced at the bed irregularly in the form of sudden storm events. These short violent episodes can affect areas of the sea-bed which are not commonly disturbed by the regular tidal currents. Wave and tidal near-bed currents depend on water depth in different ways, and wave induced currents (especially those generated by long period waves) regularly penetrate down to the sea bed in coastal areas (Draper, 1967). Before moving to the core issue of bed disturbance, it is important to understand the driving processes of wind-waves and tidal and surge currents. Fortunately the UK continental shelf has been the subject of many studies of tides, waves and coastal change using models and observations. The tides and hydrodynamics of the UK continental shelf has been extensively studied, e.g. Flather (1976), Griffiths (1996), and Jones (2002). Most relevant to our work is the study of Holt and James (2001b) who simulated the barotropic tides and the residual currents of the UK continental shelf for a year, at a resolution of 12 km. They conclude that their model domain is suitable for a long term study of transport around the UK coast. Early work on storm surge began with Heaps (1977) and modelling methods are reviewed in Bode and Hardy (1997). Storm surge forecasting models are presently run operationally with a predictive range of 36 h (Williams and Horsburgh, 2010). The state and variability of the wave climate has also been well studied, e.g. Draper (1980, 1991) and Woolf et al. (2002), and wave models are also routinely run operationally (Janssen, 2008). Most recently, Brown et al. (2010) performed a wave/tide/surge model hindcast for the Irish Sea. We extend their work by performing a shelf-wide model hindcast, and by making predictions about extreme waves and the impact on bottom stresses. In this study wave and tidal bed-shear stresses are calculated from a 10-year model hindcast of tides, surge and waves on the northwest European shelf. Modelling and observation methods are presented in Sections 2.1 and 22 respectively. Shelf-wide validation of wave and tidal conditions is presented in Section 3.1. In Section 3.2, the modelled bottom velocities and pressures are validated against in situ observations. In these datasets wave and current data were observed simultaneously, giving a unique opportunity to investigate combined wave and bed disturbances. By using the full 10-year hindcast, estimates of the frequency of bottom disturbance by waves and currents are presented in Section 4. In Section 5 a measure of force on an idealised object, representing a benthic organism, is introduced. This can be used to compare the relative disturbance at the bed across the whole continental shelf. This combined bottom force associated with waves, surges and tides is then mapped, to give a spatial picture of the seabed climate and implications for sediment transport around the coastal seas of Britain. The results are discussed in Section 6 and summarised in Section 7."
published_039,"For out of olde feldes, as men seyth, Cometh al this newe corn from yer to yere, And out of olde bokes, in good feyth, Cometh al this newe science that men lere. Out of old fields comes all the new grain Year after year, an old saying goes And out of old books, new knowledge we gain For people to learn, as everyone knows —Parlement of Foules, Geoffrey Chaucer, 1382 Most experts agree that the hippocampus contributes to some of the most sophisticated aspects of human cognition, including episodic and autobiographical memory, scenario construction, constructive episodic simulation, future-thinking, prospection, perspective-taking and situational modeling. In contrast to these highly derived cognitive capacities, the evolution of the hippocampus can be traced to the earliest vertebrates. This combination of ancient ancestry and higher brain functions seems incongruous to some neuroscientists. Yet, as Chaucer recognized, it is sometimes the old that brings forth the new. In this review we discuss the hippocampus and its homologues in a phylogenetic perspective. As we explain more fully in The Evolution of Memory Systems: Ancestors, Anatomy, and Adaptations [1], specialized representations emerged in the brains of particular ancestral species as they adapted to a new way of life, especially during major evolutionary transitions. Specific areas encode, process and store these representations, and the hippocampus homologue is one structure that does so. In early vertebrates it developed map-like representations that provided selective advantages for navigation. As new brain areas developed during subsequent evolution, the innovative representations in these areas influenced the hippocampus homologue and modified its function. The human hippocampus performs many roles, but we focus here on its contribution to the perception and memory of visual scenes, with some comments on sequence memory. After a few points on evolution, we trace the modern form of this function to anthropoid adaptations in the Oligocene ∼34 million years ago."
published_040,"Ubiquitination is a protein modification process where the C-terminus of ubiquitin is enzymatically attached to a lysine side chain (-NH3+ group) of the target protein. This process is catalysed by the E1-E2-E3 enzymatic cascade [1]. The main function of this modification is to promote the proteasomal degradation of the target protein. Several other ubiquitin-like domains (UbLs) have been identified and grouped, and together with ubiquitin they form the ubiquitin superfamily. The ubiquitin-like fold, also called the β-grasp fold, is one of the most represented three-dimensional structures found in the protein universe [2]. Despite its high structural conservation across various protein families, the sequence similarity is low [3]. Members of the ubiquitin superfamily can be divided into two groups based on whether they are post-translationally conjugated or not. Ubiquitin-like modifiers (ULMs) comprise different protein domains, which become attached to their target protein by enzymes similar to the enzymatic cascade operating on ubiquitin [4]. This group includes SUMO, NEDD8, ISG15, APG8, FAT10, URM1, Ufm1 and Hub1 [5]. Contrary to ubiquitin, these domains are not involved in protein degradation but exert other functions, often related to signalling and trafficking pathways. ULMs are translated either as precursors or single domains, and they share a conserved R-G-G sequence in their C-terminus, required for their maturation and conjugation. In addition to being found as independently expressed domains, UbLs can also be part of larger proteins. Proteins containing UbLs are called ubiquitin-like domain proteins (UDPs) [6]. UbLs belonging to this group are neither processed nor conjugated but instead are integral part of the “host protein” since its translation. Because of their sequence similarity with ubiquitin, integral UbLs are often able to interact with a plethora of proteins, including the proteasome, with which UbLs interact through the key residues, known as those forming the “hydrophobic patch”, equivalent to residues L8, I44 and V70 of ubiquitin. This interaction, however, does not result in the degradation of the host protein [7]. In fact, in some cases, like the deubiquitinase USP14, the interaction of the UbL domain with the proteasome is critical for efficient catalysis [8]. Both the E3-ligase parkin and the UV-excision repair protein Rad23 are also recruited onto the proteasome upon interaction between their UbLs and the Rpn10 proteasomal subunit [9] [10] [11] and mutations or deletions of the UbL are associated with the loss of the host protein function causing pathological conditions, such as the autosomal recessive juvenile Parkinsonism [12]. Interestingly, S. cerevisiae Rad23 (UNIPROT_P32628) functional rescue was observed for the mutant lacking the UbL when the missing domain was replaced with ubiquitin [9]. This evidence indicates that the conservation of the hydrophobic patch is sufficient to guarantee the function of the UbL while variations of the β-grasp fold are better tolerated. It is reasonable to think that, in this case, ubiquitin fold operates just as a scaffold to display the hydrophobic patch driving protein-protein interaction. Alternative functions for UbLs have been established as well. Finley et al. [13] described the chaperone-like function observed for UbL of ribosomal proteins, and their importance in promoting the ribosome assembly. In parkin, the UbL maintains the host protein in an ‘idle’ state until both the phosphorylation of UbL and the interaction of parkin with the phosphorylated ubiquitin have occurred [12] [14] [15]. Integral UbLs featuring the conserved hydrophobic surface use it to exert the regulative function by interacting with either the host protein or the foreign protein(s), or both. Thus, integral UbLs lacking the patch are unable to establish similar interactions with the ubiquitin-interacting motif (UIM) [16], ruling out the possibility to be recognized by most ubiquitin-binding proteins, although not necessarily by their host protein. In this work, we describe the NMR solution structure of the T.th-ubl5 domain of Tetrahymena thermophyla BUBL1 (BIL-ubiquitin-like) locus. First described by Dassa et al. [17], the T.th-BUBL1 locus (Fig. 1) consists of five UbLs, two Bacterial-Intein Like (BIL) elements and an ADP-ribosyl transferase (ART) domain, similar to bacterial toxins. Although BUBL1 is translated as a single protein, BIL-operated cleavage of the polypeptide chain has been hypothesized as a post-translational modification, likely responsible for the maturation and possible conjugation of the flanking UbLs [17] [2]. Such cleavage is the result of the side reaction of BIL splicing activity. In fact, while inteins accomplish splicing by efficiently linking the two flanking regions, BIL domains often induce cleavage of either or both the C– and the N-flanks [18] [19]. The two BILs in the BUBL1 locus act upstream of T.th-ubl5 and as a result T.th-ubl5 C-terminus remains permanently connected to the rest of the protein. Therefore, T.th-ubl5 can be considered as an integral part of a protein containing an ART domain. To date, the BUBL loci from T. thermophila and Paramecium tetraurelia represent the only co-occurrences of an UbL and an ART domain in the same protein. Interestingly, T.th-ubl5 lacks the hydrophobic patch responsible for the interactions with the UIM motif, precluding its recognition by most of ubiquitin partners. By structurally and sequentially comparing T.th-ubl5 with other integral UbLs, a novel biological function for the domain, with respect to the host protein, is proposed and demonstrated."
published_041,"Angelman syndrome is a rare genetic neurodevelopmental disorder characterized by severe intellectual disabilities, impaired speech, developmental delays, microcephaly, seizures, anxiety, motor dysfunctions, ataxic gait, social communication deficits, and a happy demeanor with excessive laughter (Angelman, 1965; Williams et al., 2010; Bird, 2014; Wheeler et al., 2017; den Bakker et al., 2018; www.angelman.org). The primary genetic cause of Angelman syndrome resides in the deletion of a sequence of imprinted genes at chromosomal locus 15q11-q13. Maternal transmission of the deletion, particularly the reduced expression of the ubiquitin ligase UBE3A gene within this locus, results in Angelman syndrome, whereas paternal transmission results in another distinct neurodevelopmental disorder, Prader-Willi syndrome (Knoll et al., 1989; Nicholls, 1993; Buiting et al., 2016). No medical treatments currently exist for the underlying causes of Angelman syndrome (Wheeler et al., 2017). Mouse models incorporating the loss of maternal Ube3a have been generated and well-characterized for several behavioral features relevant to the symptoms of Angelman syndrome, including motor and cognitive deficits (Jiang et al., 1999, 2010; Heck et al., 2008; Mabb et al., 2011; Baudry et al., 2012; Jana, 2012; Kaphzan et al., 2012; Huang et al., 2013; Santini et al., 2015; Leach and Crawley, 2018; Sonzogni et al., 2018). Ube3a mutant mouse models provide a preclinical research tool for the discovery of effective therapeutics for Angelman syndrome (van Woerden et al., 2007; Huang et al., 2011; Egawa et al., 2012; Margolis et al., 2015; Beaudet and Meng, 2016; Bi et al, 2016; Tan and Bird, 2016; Ciarlone et al., 2017; Stoppel and Anderson, 2017; Guzzetti et al., 2018; Lee et al, 2018; Rotaru et al., 2018). Here we focus on a relatively unexplored aspect of Ube3a mice. Although diagnosed in childhood, Angelman syndrome is a lifetime disorder. Adults with Angelman syndrome continue to display severe symptoms (Smith, 2001; Larson et al., 2015; Prasad et al., 2018). In contrast, most behavioral characterizations of Ube3a mutant mouse phenotypes have employed young mice, in the 8–14 week old range, paralleling the early stages of this neurodevelopmental disorder. At these younger ages, deficits have been consistently reported on motor assays including rotarod (Miura et al., 2002; Heck et al., 2008; Jiang et al., 2010; Daily et al., 2011; Egawa et al., 2012; Huang et al., 2013; Ciarlone et al., 2017; Leach and Crawley, 2018; Sonzogni et al., 2018) and open field exploratory locomotion (Allensworth et al., 2011; Huang et al., 2013; Ciarlone et al., 2017; Sonzogni et al., 2018). Higher anxiety-related behaviors (Jiang et al., 2010; Ciarlone et al, 2017 and others), impaired water maze spatial learning (Miura et al., 2002; Jiang et al., 2010; Huang et al., 2013; Leach and Crawley, 2018) with slower swim speeds in some genetic backgrounds (Huang et al., 2013; Leach and Crawley, 2018), and impaired fear conditioned learning and memory (Miura et al., 2002; Jiang et al., 2010; Huang et al., 2013) have been reported for Ube3a mice at younger ages. To our knowledge, the oldest ages of Ube3a mice tested behaviorally have been 17–23 weeks for water maze and at 31 weeks for rotarod (Huang et al., 2013). The present studies evaluated behavioral phenotypes of older Ube3a mice as compared to their wildtype littermates (WT) on two anxiety-related tests and two social tests, conducted primarily at age 12 months. In addition, open field locomotion was quantified to detect decreases or increases in activity at the older age, which could confound the interpretations of anxiety-related and social assay results."
published_042,"The subjective sensation of discomfort generated from a glare source is not yet fully understood, and its robust prediction is still characterised by uncertainties, particularly in the presence of daylight [1]. Various studies have investigated whether there may be variables, other than those conventionally included in glare formulae, which might influence the occurrence and magnitude of discomfort glare. Among these, an influence of view interest on glare response was detected in laboratory tests and from a real window [2–4]. Research conducted by Kuhn et al. [5] showed that glare may be more frequently reported by older observers, while Pulpitlova and Detkova [6] found a higher tolerance to glare in Japanese than in European subjects. Akashi et al. [7], Cai and Chung [8], and Rowlands [9] also suggested that glare sensitivity may not be consistent across cultures. Moreover, a potential link between perceived thermal sensation and visual discomfort has recently been hypothesised [10]. A previous series of laboratory experiments conducted by the authors detected a tendency towards greater tolerance to luminance increases in artificial lighting as the day progresses [11]. A follow-up study explored the relationships between visual task difficulty, temporal variables, and glare response at different times of day, revealing that an increased time gap between test sessions resulted in lower glare sensitivity to a constant source luminance along the day [12]. Coherent with the literature [13], when luminance levels for each vote of glare sensation provided by test subjects were regressed, a large scatter was observed. This suggested that there could be other factors varying with time of day, not experimentally controlled, which could influence glare response. Among these variables, statistically and practically significant evidence was found of greater tolerance to source luminance for earlier chronotypes and for subjects not having ingested caffeine. Further trends were detected, postulating an influence of fatigue, sky condition, and prior daylight exposure on glare sensation [14]. On the basis of these earlier laboratory results, and of a comprehensive review of the literature presented by the authors in previous work [11,12,14], this study sought to explore the influence of time of day on glare response in the presence of daylight from a window, and analyse the effects of several temporal variables on the subjective evaluation of glare sensation as the day progresses."
published_043,"Agriculture production worldwide is often limited by water deficits and the case is very acute in semi-arid tropics of Asia and Africa where populations are large, dense, and depend on subsistence agriculture. Developing “drought” tolerant cultivars has then become a critical agenda to breeding programs in many crops species. Because the root system is the plant organ in charge of capturing water and nutrients, besides anchoring the plant in the ground, it is naturally seen as the most critical organ to improve crop adaptation to water stress. Here we review the research carried out on roots for drought adaptation and mostly on root depth and density (Kashiwagi et al., 2006; Silim et al., 1993; Gowda et al., 2011). Then we review the limitation to these “traditional” approaches to root architecture, discuss the relevance and limit of pursuing water extraction at depth, and address the limits to the current experimental approaches to measure root systems. Especially, we highlight the need to progress toward 3-D in situ representation of the root system (Burton et al., 2012; Mooney et al., 2012) to reach a true representation of the roots in their environment, and of their potential to capture water. In a Section 2, we present an alternative way to approach the role of root for water stress adaptation, moving away from actual root measurements and, instead, assessing water extraction by roots as a way to harness the functionality of root systems. This recent approach consists of a lysimetric system, i.e. a set of long and large PVC tubes in which plants are grown individually and have plant spacing and soil volume available for soil exploration close to what is practiced under field conditions (Vadez et al., 2008; 2013a). In that section we present results on the genetic variation for water extraction under different types of water stress in different legumes and cereal crops. We also discuss the usually low/inexistent relationships between total water extraction and grain yield, in comparison to the positive relationships between the grain yield and the harvest index (HI) or the transpiration efficiency (TE), i.e. the other components of the Passioura equation (Y = WU × TE × HI, Passioura, 1983). By contrast, recent evidence across several species point out to the importance for crops to secure water availability at the critical stages of reproduction and grain filling (e.g. chickpea, Zaman-Allah et al., 2011b); pearl millet (Vadez et al., 2013a); wheat (Kirkegaard et al., 2007). Therefore this section highlights the importance of (often) small but critical water availability for reproduction and grain filling, and briefly presents the traits that make this possible. Here we mostly refer to a recent paper where these traits, mostly related to the shoot, are exhaustively reviewed (Vadez et al., 2013c). The end of this section is then a transition in which we discuss the linkage between some of these traits related to the plant water budget and the plant hydraulic characteristics. There is indeed evidence that some of the traits related to the plant water budget are ruled by hydraulic mechanisms, e.g. the control of leaf expansion (Reymond et al., 2003; Simonneau et al., 2009), or the transpiration response to high vapor pressure deficit (VPD) (Sinclair et al., 2008), and some of these are determined by differences in the root hydraulics (Parent et al., 2010). Of course, water conservative mechanisms should not be seen as “drought tolerance” mechanisms, but rather as mechanisms that alter the plant water budget and that need to be tailored to specific drought scenarios. The last section then deals with possible root characteristics that can influence root hydraulics (Maurel et al., 2010) and eventually can alter the different traits related to the water budget. We first briefly review the existing differences in root absorption kinetics within and across species (Dardanelli et al., 1997; Collino et al., 2000; Dardanelli et al., 2004), and then the architecture of the root cylinder and how water penetrates the root (Steudle and Peterson, 1998; Steudle, 2000a). Then we review the “root development” options to alter root hydraulics, in particular the xylem vessel sizes (Richards and Passioura, 1989), but also other root characteristics like the root cortical aerenchyma or root cell size and file number, which have been approached from the angle of root carbon cost (Lynch and Brown, 2012; Burton et al., 2013; Lynch, 2013) but that could have a role on the root hydraulics. Finally we review the role of aquaporins in influencing hydraulic conductance of plant tissues, focusing here on their role in root tissues (e.g. Ehlert et al., 2009; Thompson et al., 2007)."
published_044,"Vitamin K, a fat-soluble vitamin, functions as a cofactor for gamma-carboxylase enzymes within the liver (Ozdemir et al., 2012). This role enables posttranslational conversion of inactive hepatic precursors II, VII, IX, and X into active clotting factors through gamma-carboxylation of glutamic acid residues (Shearer, 1995; Smith et al., 2015). Hepatocyte secretion into the blood signifies the end stage of carboxylation, enabling regulation of the clotting cascade (Smith, et al., 2015). Due to its fat-soluble properties, vitamin K is largely stored in the liver of adults and children. Only a small amount (approximately 1 µg/kg/day) is required in the diet. Sources of vitamin K include green leafy vegetables, vegetable oils, and cereal grains (Shearer, 1995). However, within neonates, a relatively small amount of vitamin K is found in the liver compared to other fat-soluble vitamins (Kayata et al., 1989). In addition, poor diffusion of vitamin K across the placenta and only very low concentrations of it within breast milk put newborns at risk for vitamin K deficiency bleeding (VKDB), a rare and potentially dangerous bleeding disorder in infants in the first hours to months of life (Anonymous, 1993; Kries et al., 1987; Sutor et al., 1999). As defined by the Paediatric Subcommittee of the International Society on Thrombosis and Haemostasis, VKDB can present itself in three forms: early, classic, and late (Schulte et al., 2014). Early VKDB can present as an intracranial haemorrhage during the first 24 h of birth. This particular form is most common in newborns whose mothers have been prescribed antiepileptics, anticoagulants, or other medicines that interact and prevent the normal functioning of vitamin K (Shearer, 2009). In contrast to early VKDB, the classic form of this condition is largely regarded as idiopathic with the gastrointestinal tract, umbilical region, and skin, which are defined as common bleeding sites. These classic symptoms present themselves between the second and seventh day after birth (Shearer, 2009). Late VKDB has been shown to mainly affect breastfed infants; however, there have been reports of links with hepatobiliary dysfunction alongside other mild abnormalities of the liver (Matsuda et al., 1989; Newman and Shearer, 1998; von Kries et al., 1985). Bleeding due to this form of deficiency is present after the eighth day and up to six months postbirth (Sutor et al., 1999). Vitamin K prophylaxis can prevent VKDB (Schulte et al., 2014). The World Health Organization recommends that newborns receive a 1 mg intramuscular (IM) injection of vitamin K at birth. Evidence from multiple surveillance studies shows that the introduction of vitamin K prophylaxis reduces the incidence of VKDB (Schulte et al., 2014). Current recommendations support universal prophylaxis due to the lack of predictors for vitamin K deficiency (Shearer, 2009). Despite these recommendations, coverage of vitamin K prophylactic treatment in low-resource settings is limited (Newman and Shearer, 1998). Oral formulations are available, but the optimal regimen and associated efficacy of oral doses are not well understood. Both oral and IM routes present disadvantages, which may affect their efficacy. Oral vitamin K is administered as three 1 mg doses at intervals of up to 5 weeks (von Kries et al., 1995). Due to the short duration of action of each dose, poor absorption in unknown liver abnormalities, and possible compliance issues, infants may not be protected against late VKDB by oral administration of vitamin K (Sutor et al., 1999). IM administration, on the other hand, involves a single 1 mg dose of vitamin K. This is due to its ability to produce mean plasma levels of vitamin K 20,000 times greater than normal levels, 4 h postinjection (McNinch, 2010; Shearer, 1995). Although effective, a recent survey in a single southeastern state in the United States highlighted the concerns and mindset of expectant parents intending to refuse IM vitamin K prophylaxis (Hamrick et al., 2016). Eighty-three per cent of parents in this study reported awareness of the risks associated with vitamin K refusal; however, only 11% decided to accept IM prophylaxis after discussions with study workers (Hamrick et al., 2016). Reasons for refusal included concerns over injection pain, localised bruising, and toxic ingredients (Hamrick et al., 2016). A novel delivery mechanism using drug-loaded dissolving microneedles (MNs) has been designed to provide a potential alternative to hypodermic needles (Kaushik et al., 2001). MN arrays consist of micron-scale projections arranged in perpendicular orientation to a baseplate. MNs can encapsulate drug within the baseplate or throughout the entire array (González-Vázquez et al., 2017; McCrudden et al., 2014). Dissolving MN arrays have been extensively used for the delivery of a wide range of compounds, such as small molecules, proteins and nanoparticles (Donnelly et al., 2012; González-Vázquez et al., 2017; Kennedy et al., 2017; Larrañeta et al., 2016a, 2016b; Vora et al., 2017). Upon application to the skin, interstitial fluid causes the MN polymer to dissolve, resulting in the release of the drug. This platform has the ability to create microconduits through the stratum corneum, enabling the delivery of drugs into the deeper regions of the skin where they can be absorbed directly into the systemic circulation (Donnelly et al., 2012). There are significant advantages in facilitating transdermal drug delivery using MNs, one of which is self-application (Bariya et al., 2012; Ripolin et al., 2017). As a result of a micron-scale design, penetration does not reach the dermal layers, meaning application is pain-free (Giudice and Campbell, 2006; Xie et al., 2005). Research has proven that patients show no needle fear, discomfort is reduced, and healing is faster at the injection site (Haq et al., 2009; Kaushik et al., 2001; Xie et al., 2005). Furthermore, as this design of MN is dissolvable, it is limited to one application; thus, it prevents needlestick injury and eliminates sharps disposal requirements. To date, dissolving MN arrays have been used to deliver hydrophilic drug molecules such as theophylline, caffeine, and lidocaine (Garland et al., 2012). On the other hand, hydrophobic, liquid drugs have yet to be tested within an MN-based delivery platform. The objective of this study was to incorporate vitamin K into a dissolving MN array for neonatal prophylaxis of VKDB. This has the potential benefit of improving acceptance within developed countries and greatly reducing VKDB cases in less economically developed countries (LEDCs)."
published_045,"Biomass provides the fuel for forest and urban fires. Whether in a forest in the form of plants, or in buildings in the form of engineered wood, like Cross Laminated Timber, biomass is made out of three main components: cellulose, hemicellulose, and lignin [1]. During a fire, biomass undergoes pyrolysis, a thermochemical process in which it decomposes to char, condensable liquids (tar), and gases [1]. Cellulose is often taken as a surrogate for the pyrolysis of biomass. It is the main component of biomass and dominates the chemistry of pyrolysis in reactors and fires [2,3]. In a timber building the chemistry of pyrolysis controls roughly the first 50 minutes of the fire behaviour (charring) of timber [4]; Suuberg and Milosavljevict even recommended to use cellulose as a standardised material to study the behaviour of wood in fire [5]. To study cellulose is therefore a natural first step in studying the chemistry of pyrolysis of wood. Despite 60 years of research, the chemistry of pyrolysis for cellulose is still under scientific debate [3], with reaction schemes ranging from one step (one reaction, three parameters) [6] to mechanistic (>300 reactions,>600 parameters) [7]. There is no consensus on the optimal kinetic model, with different authors favouring different complexities of kinetic models [7–10]. The chemistry of pyrolysis is traditionally studied on the microscale using thermogravimetric analysis (TGA). In TGA, a small sample (usually a few mg) is inserted in a furnace and subjected to a temperature-time program, either isothermal or non-isothermal. In both cases, the temperature is measured close to the sample [11], and the residual mass and mass loss rate of the sample are recorded with respect to time. From this history of residual mass and mass loss rate the kinetic parameters are inferred, for example by fitting a kinetic model to the data [12]. In non-isothermal experiments the temperature of the furnace changes with time, usually at a linear rate, while in isothermal experiments the temperature is quickly ramped to a desired temperature and then held at that value for the duration of the experiment. The mass loss and mass loss rate is important in fires in timber buildings to determine the remaining load-bearing cross-sectional area of structural elements. Measurements of in-depth temperatures in thick wood at the macroscale reveal transient temperature profiles [13,14]. Temperature plateaus occur either below 200 °C or above 500 °C. Below 200 °C there is practically no decomposition taking place, only drying [15] (negligible mass loss), and above 500 °C cellulose decomposes instantaneously (within 1 s) [7]. Non-isothermal experiments are, therefore, more important for fire conditions as they resemble an increasing temperature with time. Isothermal experiments also provide information about the underlying chemistry of pyrolysis, but are less important for fire, and more important for other technologies, like torrefaction or the production of biofuels. Mostly fragmented studies on the decomposition chemistry of cellulose are available in the literature, where any particular scheme was calibrated to either isothermal or non-isothermal experiments. We use the word calibrated here to indicate that a validation study—prediction of data outside of those used for calibration—is usually missing. Antal [6,16,17] found that for non-isothermal experiments at the microscale with ash-free samples of cellulose the reaction of cellulose can be represented by a single step. This scheme, however, cannot explain the variation of the char yield with heating rate [18]. For this variation at least two competing reactions are required [2]. Broido and co-workers first proposed the mechanism of two competing reaction pathways: one to char and one to tar, with an intermediate [19] and then without an intermediate [20]. Bradbury et al. [21] added an activation reaction to Broido's second scheme, without an intermediate, while Agrawal [22] modified Broido's first scheme, with an intermediate, by separating the reaction of char and gas formation into two reactions. None of the above have been validated for both isothermal and non-isothermal conditions with the same set of kinetic parameters. Only Capart et al. [9], and Corbetta et al. [23] were able to predict isothermal and non-isothermal experiments of cellulose with the same set of kinetic parameters using more than 9 parameters to calibrate the model. The question remains if it is appropriate to predict both isothermal and non-isothermal experiments using reaction schemes with lower complexity than Capart et al. [9] and Corbetta et al. [23]. Here we investigate this question by inverse modelling isothermal and non-isothermal experiments using multi-objective optimisation."
published_046,"The tuberculosis (TB) is among the top list of global infectious disease and Mycobacterium tuberculosis (Mtb) is the pathogenic bacterium of TB [1]. In 2017, an estimated 10 million new TB cases arose worldwide, and 1.6 million died [1]. TB ranks the first among all the killers in AIDS/HIV victims and approximately 35% of HIV deaths were due to TB in 2017 [1]. A multi-drug regimen according to the Mtb susceptibility is the routine therapy for TB patients with a prolonged course more than 1 month. The accompanying side effects of multiple organs, especially for liver and gastrointestinal tract, seriously interrupt the treatment, and the discontinuous course and poor patient compliance are the key factors for Mtb resistance and relapse. The multidrug-resistant TB (MDR-TB) is another severe challenge and a sustained threat, with more than half a million new MDR-TB cases were confirmed in 2017 and had to receive non-first-line therapy with less effective [1,2]. Hence, more potent chemical entities and regimens are urgently needed to conduct a shorter course of treatment for both sensitive and MDR-TB to promote clinical efficacy and lessen side-effect burden [3]. Bedaquiline (approved by U.S. Food and Drug Administration) and Delamanid (approved by European Medicines Agency) are only two new compounds approved to counter the TB. However, several safety concerns are persistently alarming like drug interactions on CYP3A4, drug related hepatic disorders, and especially cardiovascular risks and deaths, and the clinical application is partly restricted in combination [4]. Developing a brand-new compound for TB is a costly and endless course, generally containing preclinical study, phase I, II and III. In view of the multidrug treatment has been widely accepted and recommended by World Health Organization (WHO), it may be a valuable approach to regroup the marketed and under-developed drugs to exploit the latent synergistic effect and maximize anti-TB efficacy. Strictly speaking, the current multidrug regimen is proposed on the basis of empiric evidence without detailed studies of optimal combinations or dose proportions. But to identify the most efficacious 3-/4-drug combinations under optimal drug-dosage ratios, huge and tedious tasks are required because of a big candidate pool of anti-TB drugs (marketed and under developed) and the number of combinations can be exponentially large. Recently, Ho’s et al. have developed an algorithm, called Feedback System Control (SFC) or Parabolic Response Surface (PRS), to search the optimal drug-dose combination from a large candidate drug pool [5–12]. Based on the second order algebraic equations and experimental date, the special algorithm can plot out the drug-dose-efficacy landscape and rapidly home in on optimal solution without burdensome process [11,12]. And this outstanding platform has been used to identify optimal drug regimens in several biological systems, including antiretroviral therapy for HSV and cancer chemotherapy [5–10]. For TB therapy, a new combination consisting of pyrazinamide (PZA), ethambutol (EMB), protionamide (PTO) and clofazimine (CFZ) was selected and proposed by this method and the new one shows exceeding bactericidal or bacteriostatic ability in vitro and is worth for furthermore development [11,12]. PZA, one of the first-line drugs, is fundamental for clinical standard regimens with the most excellent sterilizing ability to latent Mtb, and the only killer in acid environment [13–15]. Since its introduction to the treatment, the therapeutic course has been shortened to six months [15]. EMB is another first-line drug administrated during initial period. Other than PZA, the target of EMB is the biosynthesis of arabinogalactan, damaging the integrity of Mtb cytoderm [16]. PTO belongs to thioamide homologous series and serves as the core option of second-line anti-TB compounds for MDR-TB cases with the resistance to front-line therapeutics like isoniazid or rifampin [1]. CFZ is also one of the typical non-first-line drugs for MDR-TB and the good safety with no serve toxicity is attested by the long therapeutic history of leprosy, although the reversible skin discoloration happens frequently [1]. Regardless the mechanisms and physiological targets have not been clarified, several in vitro and vivo trails have demonstrated that CFZ has good efficacy against MDR-TB in different models, making CFZ an intriguing candidate for the management of drug-resistant patients [17]. Some researches have uncovered the synergism activity among CFZ and other anti-TB drugs, especially co-administrated with EMB and PZA [19,20]. The treatment guidelines for drug-resistant TB of WHO in 2016 highlighted the capacity of CFZ and the CFZ-contained regimen (including PZA, EMB, PTO, CFZ, kanamycin acid and isoniazid) for MDR-TB patients for the shorter treatment course, low cost and accessibility [18]. One most effective regimen containing these four compounds for MDR-TB patients with a minimum period of nine-month treatment has been validated for naive MDR-TB patients by a clinical trial in Bangladesh from 1997 to 2007 [21]. And this treatment has since been adopted by many low-income countries, including Cameroon [22] and Niger [23]. All these clinical studies have demonstrated impressive treatment outcomes, with success rates ranging from 84% to 89% [24]. Furthermore, these drugs are ready-made on the formulary and easy access to in the hardest-hit areas. Compared with the routine therapy for tuberculosis patients, this new regimen proposed by Ho’s group showed the excellent performance in sterilizing effect in vitro and vivo [11,12]. Hence, the combination of PZA, EMB, PTO and CFZ maybe a new chance for the whole world to counter the tuberculosis. However, according to our surveys, no report uncovered the PK properties of these four drugs in combined therapy. To date, numerous analytical methods have been used for anti-TB drugs determination alone or in combination. Yet, the established methods suffer in some cases from the limited quantitative range, tedious sample preparation, huge sample volume, which make them less suitable for routine application [25–28]. However, in consideration of the huge dispersion in physicochemical property of the compounds in this new regimen, it is also a big challenge to obtain suitable chromatographic behaviors on a single chromatographic column with tiny bio-sample volume during one LC or LC–MS/MS loop. This is the first time to propose PZA-EMB-PTO-CFZ combination therapy and none of the methods allows their simultaneous analysis in biological matrix within a single loop. So, it is therefore imperative to develop a robust and rapid method for this vacancy. In this assay, a high-throughput and robust LC–MS/MS method coupled with 96-well protein precipitation plates for simultaneous determination of the PZA-EMB-PTO-CFZ combination was established and fully verified. This method has been successfully applied to the study of the pharmacokinetics of the new anti-TB regimen in Beagle dogs."
published_047,"Norway has a universal welfare model for caring of older people. Dementia care is organized by the municipalities and consists of nursing homes, sheltered housing, home nursing, and day centres. To make Norway a more “dementia friendly society” (Norwegian Ministry of Health and Care Services, 2016), the availability of daily activities for people diagnosed with dementia who are living at home should be ensured. This follows Kitwood's (1997) claim that continued and prolonged participation in social activities will improve the memory and quality of life for people diagnosed with dementia. In this article, the social activity we focus on is the breakfast meal at a Norwegian day centre for people diagnosed with dementia. Our central question is how the active framing of a regular activity like the breakfast meal in an institutional setting, together with professional guidance or support, may affect or promote users' enactment and display of social agency and personhood. When we refer to persons at the day centre who are diagnosed with dementia, we either refer to them as such, as persons living with dementia or as users. Individuals diagnosed with dementia at the day centre were referred to as “users” by the caregivers. “User” is a moral and political term that refers to a relatively vague but formal social role that belongs to Norwegian welfare services. The term indicates social agency and citizenship. In our institutional context, the user role was introduced to disregard or moderate individual attendants' potentially stigmatizing neurobiological character."
published_048,"Throughout the visual system, the activity of individual neurons signals specific features of object motion (Barlow and Levick, 1965; Dräger, 1975; Hubel, 1960; Hubel and Wiesel, 1959, 1968; Niell, 2015; Van Essen and Gallant, 1994). However, to detect the motion trajectory and speed of an object in the external world, visual stimuli must be placed within a context that includes information regarding the motion status of the observer. This integrative process is fundamental, since it underlies judgments of spatiotemporal coincidence (Hoy et al., 2016), including the distance (Barlow et al., 1967), direction (Hubel and Wiesel, 1959, 1968), and speed (Roth et al., 2012; Van Essen and Gallant, 1994) of potential animate activity (Grossman and Blake, 2002) that may indicate the presence of a threat, food source, or conspecific. Growing evidence suggests that attention (Niell and Stryker, 2010; Roelfsema et al., 1998; Zhang et al., 2014), learning (Makino and Komiyama, 2015; Poort et al., 2015; Yan et al., 2014), and context (Fiser et al., 2016; Roth et al., 2016) modulate activity in the primary visual cortex (V1). For example, in head-fixed animals, running evokes modulation of V1 activity (Erisken et al., 2014; Fu et al., 2014; Niell and Stryker, 2010; Polack et al., 2013; Saleem et al., 2013), and in virtual-reality environments where visual flow can be uncoupled from running speed, V1 activity can also report sensory-motor mismatch when the actual and expected visual flow are not coherent (Keller et al., 2012). More broadly, internally generated signals are believed to play a fundamental role in spatial sensory processing and navigation (Morris et al., 1982; Tsoar et al., 2011). The neuronal underpinnings of these representations are widely distributed (Moser et al., 2008) and, at least in part, utilize internal vestibular signals (Cho and Sharp, 2001; Smith et al., 2005; Stackman et al., 2002; Taube et al., 1990; Valerio and Taube, 2016). While the involvement of vestibular signaling in limbic regions known to contribute to spatial processing is well described, the extent to which this internal signal is involved in primary sensory cortical representation is not known. Stimulation of the vestibular organ or nerve does, however, evoke responses in V1 (Grüsser and Grüsser-Cornehls, 1972; Rancz et al., 2015; Vanni-Mercier and Magnin, 1982) and in the retrosplenial cortex (RSP) (Rancz et al., 2015), a multisensory area known to project directly to V1 (Leinweber et al., 2017; Makino and Komiyama, 2015; van Groen and Wyss, 1992; Vélez-Fort et al., 2014; Vogt and Miller, 1983). From a functional perspective, the RSP appears well positioned to convey head-motion information to V1, since it receives projections from subcortical vestibular-related nuclei (Amin et al., 2010; Oh et al., 2014; Shibata, 1993; Van Groen and Wyss, 1995, 2003; Vogt et al., 1981) and signals running speed, head direction, and angular velocity (Cho and Sharp, 2001). Although the functional relevance of the RSP projection into V1 remains unknown, its dominance in the presynaptic network (Leinweber et al., 2017; Vélez-Fort et al., 2014) and role in visual-based spatial navigation (Alexander and Nitz, 2015; Chen et al., 1994; Jacob et al., 2017; Mao et al., 2017; Vann et al., 2009) make it an attractive candidate for providing internal motion signals."
published_049,"Current diagnostic criteria in psychiatry are based around symptom patterns and course of illness, however, no symptom is uniquely associated with an individual condition, and symptoms vary between people with the same diagnosis. Psychosis, mood instability, and cognitive impairments for example are observed across multiple diagnoses. There is also considerable overlap in genetic contributions, as well as commonalities in implicated brain networks, for example the prefrontal cortex and medial temporal lobes (Phillips et al., 2003; Shaw and Rabin, 2009; Dickstein et al., 2013; Hong Lee et al., 2013). There is an increasing uncertainty therefore over the degree to which current diagnostic criteria define biologically-valid distinct entities, or whether common mechanisms contribute to multiple conditions or cross-disorder phenotypes. To address such issues, previous imaging studies have employed a dimensional approach, by examining the neurobiology of specific symptoms crossing diagnostic boundaries. These have included individuals with, or at increased risk of, schizophrenia with and without mood symptoms (Whalley et al., 2008; Simon et al., 2010; Tomasino et al., 2011; Barbour et al., 2012), and patients with mood disorder with and without psychotic features (Sommer et al., 2007; Khadka et al., 2013). Although literature is limited, evidence suggests alterations in medial temporal lobe and limbic structures in association with mood-related symptoms across disorders (Tomasino et al., 2011), and alterations in lateral prefrontal functioning in association with psychosis, also trans-diagnostically (Anticevic et al., 2013). Genetic imaging studies have also examined the impact of shared genetic risk on underlying neurobiology. Previous studies have investigated the effects on neurobiology of individual SNPs identified as potential risk markers for illness within and across diagnostic groups (Mechelli et al., 2008; Chakirova et al., 2011; Papagni et al., 2011; Prata et al., 2011; Whalley et al., 2012a,b). Current evidence however suggests that for psychiatric disorders a substantial proportion of the heritability is explained by a polygenic component. We previously used the polygenic approach to demonstrate increased activation of mood-related limbic regions in association with increased polygenic loading for bipolar disorder (Whalley et al., 2012a,b, 2013). One recent study has used genetic strategies to explore shared genetic architecture across the 5 major psychiatric disorders using Psychiatric Genomics Consortium data (Smoller et al., 2013). The authors identified shared genetic effects between Attention Deficit Hyperactivity Disorder (ADHD), Autism (Aut), Bipolar disorder (BD), Major Depressive Disorder (MDD) and Schizophrenia (SCZ), in 33,332 cases and 27,888 controls (Smoller et al., 2013), firstly by examining effects of shared GWAS hits for BD and SCZ, and then by generating cross-disorder polygenic risk scores (PGRSs) to examine a broader set of common variants. This cross-disorder PGRS is likely to account for an even greater proportion of overall risk than for single disorder PGRS and allows examination of processes involved in enhanced risk across diagnostic groups (Smoller et al., 2013). In the current study we examine the neural effects of this broader set of common variants on brain activation in regions previously associated with the 5 major psychiatric disorders, namely the prefrontal cortex and medial temporal lobe structures (Phillips et al., 2003; Shaw and Rabin, 2009; Dickstein et al., 2013). We also sought to test whether there was an additive or interactive effect of family history on the effect of PGRS on neural activation by examining groups with and without a family history of mood disorder. The paradigm, a language-based executive function task, was chosen as it had previously been shown to differentiate psychiatric patients, and those at increased familial risk, from healthy controls in these regions (McIntosh et al., 2008a,b; Whalley et al., 2011). Moreover, it probes frontal neuropsychological deficits in executive function, verbal initiation and verbal fluency seen across a range of psychiatric disorders (Clark et al., 2000; Arts et al., 2008; Booth and Happe, 2010). We were also interested in examining whether there was any evidence for disease-specific brain activation associations by deconstructing the components of the cross-disorder PGRS into diagnosis-specific sub-scores (Smoller et al., 2013). Based on neuroimaging evidence described, we hypothesised that there would be abnormal frontal activation in association with increased loading for schizophrenia, and increased activation of medial temporal regions in association with mood disorder."
published_050,"The World Health Organization has indicated that 1.2 million people die in accidents each year (WHO, 2015). Driverless cars have been deemed an important technology in reducing a portion of those deaths due to human error (Kyriakidis et al., 2015). A driverless car, otherwise termed a self-driving car or an autonomous car, broadly refers to a robotic vehicle that works without a human operator (Benenson et al., 2008; Paden et al., 2016). More specifically, it can be defined as ‘those in which at least some aspects of a safety-critical control function (e.g. steering, throttle or braking) occur without direct driver input’ (NHTSA, 2013, p. 7). There are various levels of automation of driverless cars and various classification systems exist (the widely adopted SAE standard, the National Highway Traffic Safety Administration (NHTSA) standard and the German Federal Highway Research Institute (BASt) standard). These systems generally encompass five levels of automation from no automation to various levels of partial automation to fully automated (Kyriakidis et al., 2015). Since the Internet and smart phone revolutions, driverless cars have now been deemed as one of the key disruptors in the next technology revolution along with drones and the internet of things and have been recognized as a key area for future research (NHTSA, 2013). Google’s self-driving car has become a hot topic in the media and governments around the world have begun to develop strategies to address the challenges that may result from self-driving vehicles (Schoettle and Sivak, 2014). While driverless cars promise to provide many benefits, a key barrier to its adoption is the public trust in driverless cars (Bansal et al., 2016; Kyriakidis et al., 2015). As automobiles are becoming unsustainable, there has been many consequences such as the emission of carbon, high traffic and accidents (Paden et al., 2016). To control these, driverless cars have been proposed as a suitable alterative. Although this may potentially provide safety and efficiency benefits, there are major concerns around the public’s willingness to adopt the technology. These concerns relate to security, trust, privacy, reliability and liability (Fagnant and Kockelman, 2015). Additionally, there are certain situations in which users may be more willing to adopt driverless cars, compared to others. Further research in understanding the scenarios when users are most willing to adopt driverless cars will assist in early implementation programs among adopting target groups and settings. Consequently, this study will attempt to answer the following research question: ‘What are the key factors influencing trust in driverless cars’? It will investigate perceptions of benefits, concerns, trust and importantly, situations when users are more willing to adopt driverless cars. It is widely accepted that driverless cars will not become mainstream on the majority of roads globally in the immediate future (Benenson et al., 2008; Godoy et al., 2015). The most likely adoption settings may be in closed environments such as university campuses, airports, golf courses, holiday parks and retirement villages (Miralles-Guasch and Domene, 2010). However, the majority of existing studies have collected data in broad brushed random approaches internationally or nationally rather than focus on closed environments. For instance, a study by Kyriakidis et al. (2015) obtained 5,000 broad responses from 109 countries with only 40 countries having at least 25 responses. Similarly, a study by Schoettle and Sivak (2014) collected 1533 responses from the US, UK and Australia. However, as the mass consumer market would not be the first ones to adopt the technology, research is needed that is more nuanced in terms of the groups and situations when people will most likely adopt the technology. For instance, prior studies did not indicate who may be willing to use driverless vehicles for public transportation. In fact, prior research makes little mention of public transport, although government transport departments and those providing transport services in certain closed precincts will be interested in attitudes towards driverless cars (Lam et al., 2016). Given the need to focus on early adoption settings of closed environments as a public transportation service, the scope of this study will focus on obtaining responses from a closed environment of a university setting, where driverless cars will soon be launched as a free service to transport passengers around the campus. This has important implications for the implementation of driverless cars in closed settings. The study is significant for many reasons. First, it provides an understanding of key factors influencing the adoption of driverless cars in closed settings such as university campuses. Second, it may also have implications for the implementation of driverless solutions in other closed settings such as airports, golf courses, holiday parks and retirement villages. The Royal Automobile Association (RAA) in Australia offers mobility options for the elderly. For the RAA, cars and their drivers may increasingly be less of their business model with an aging population as the members with valid driver’s licenses decrease (Gifford, 2017). Therefore, understanding key factors in closed environments will be helpful. Third, the research will be helpful to car manufacturers as they will be interested in offering the next generation of convenience features for early adopting target markets which will be known to us through the survey."
published_051,"Melioidosis is the name given to any infection caused by the saprophytic environmental bacterium Burkholderia pseudomallei, which is widespread in the soil and surface water in southeast Asia and northern Australia. The disease is being recognised with increasing frequency in known endemic areas [1,2] and new foci are regularly being identified [3,4]. The organism is intrinsically resistant to many antimicrobial agents, including those often used for the empirical treatment of sepsis in the tropics [5], and may be even more resistant when growing in biofilms [6–8] and in the anaerobic acidic conditions that might be found in vivo [9]. There is considerable evidence supporting current treatment recommendations, mainly derived from a series of large randomised clinical trials conducted in northeast Thailand since 1986, although there are also many unanswered questions. This review will summarise that evidence and the current recommendations and will consider some of the outstanding issues."
published_052,"Titanium alloys are used extensively in aerospace applications where fatigue is one of the most significant issues to contend with and manage. In particular, the nature of modern flying now includes some significant holding at high load for extended periods, for instance between take-off and cruise [1], in order to maximise fuel efficiency and reduce noise pollution. Further load-holds occur during thrust reversal to land on short run ways. These regimes are cyclic (once or twice per flight) and have a time sensitive load hold, leading to concern regarding rate sensitive deformation modes involved in dwell fatigue. Cold dwell fatigue has been a critical issue in aero-engine industry for the last few decades [1,2]. Cold dwell susceptible alloys are those which suffer from a significant reduction in fatigue life due to the inclusion of a short load-hold (∼120s) at low to moderate temperatures (up to ∼200 °C). This can reduce the number of cycles to failure by a decade or more and this is called the “dwell debit”. In dual-phase Ti alloys (Ti–6Al–2Sn–4Zr-xMo), Mo content significantly affects dwell sensitivity, where Ti6242 is dwell sensitive and Ti6246 is not [3]. A chemical, structural or morphological effect has been thought of as a cause of dwell fatigue due to differences in β volume fraction and local chemistry between these alloys. In practice, dwell fatigue failure is mitigated by use of dwell insensitive alloys and careful maintenance schedules, but management of this phenomena costs the aerospace industry significantly (∼£100 m/year). It is therefore important to understand microstructural contributions towards the dwell process to enable more cost effective component management strategies and ultimately engineering new materials that are dwell insensitive. Recently it has been demonstrated that dwell fatigue failure is dominated by local microstructure in many dual-phase Ti alloys, including the presence of a rogue grain combination [4]; where, during the load-hold, stress is shed (i.e. redistributed) from a ‘soft’ grain to a neighbour ‘hard’ grain in the Stroh model [2,5]. Therefore local regions of very high stress form across the interface [6,7]. Hard and soft grains here represent those grains that are poorly oriented for an applied deformation mode and appropriately oriented for easy slip (i.e. a type basal or prism slip) respectively. For titanium this is often described considering the angle between the c axis of the unit cell and principal loading axis, where applying a normal stress parallel to the c axis is a hard orientation and deforming perpendicular to the c axis is a soft orientation. In dwell fatigue, time dependent stress amplification during this load shedding process at local microstructural regions near the interface is thought to play a predominant role in failure through facet formation [8]. Prior research indicates that the function of basal plane orientation is important in dwell fatigue for dual-phase Ti alloys [9–13]; yet fundamental deformation mechanisms within each hard and soft grain (i.e. contributions of individual slip systems) are still unknown. One hypothesis on time sensitive dwell fatigue is that strain rate sensitivity (SRS) is a crucial factor governing the load shedding phenomenon. As titanium alloys are typically more elastically and plastically anisotropic at the grain scale [14–16], there are likely to be differences in the SRS of different grain orientations due to variable rate sensitivities of the different slip systems [17]. These slip systems are a dislocations on prismatic planes; a dislocations on basal planes; and c+a slip systems on pyramidal slip planes. Each slip system has a different critical resolved shear stresses, where their ratio is likely to be ∼3:4:9 [18,19]. Even though the c+a deformation mode has a significantly higher CRSS value it is still likely to activate, as slip with a type dislocations only does not provide 5 independent deformation modes which are required to accommodate an arbitrary shape change. Nanoindentation has been previously used to study the mechanical response of single grains in titanium alloys. Quasistatic nanoindentation, combined with TEM of Ti–7Al, reveals that indentation into different grain orientations activates different slip systems [20]. In particular indentation into the basal plane activated both c+a type dislocations and a type dislocations. The a type dislocations were of opposite sign to the a component of the c+a dislocations to enable local deformation and support the local curvature around the indent. This supports a prior demonstration [21] that indentation into grains of Ti–6Al–4V revealed clearly that these mechanisms also happen in dual-phase microstructures and in particular that c+a type dislocations are geometrically required if twinning does not activate. This change in plastic deformation mechanism, when indenting into grains of different crystallographic orientation, results in significant anisotropic hardness measurements. Additionally indentation, combined with EBSD, has been used to systematically study grains of different orientations and confirmed that the anisotropic mechanical performance is consistently observed across a range of Ti alloys [14,16,22,23]. Furthermore, experimental data has been fitted with crystal plasticity models has enabled extraction of the critical resolved shear stress for different dislocation types [14,16]. The nanoindentation approach is insightful and relatively inexpensive to undertake, but precise mechanistic analysis is fraught with complications, as the stress state around the indention is complex and there are likely to be significant lengthscale contributions due to the indentation size effect for small indentation depths. Often these effects are considered for relatively isotropic materials, yet in HCP alloys ignoring the effect of orientation is likely to be problematic. This does not diminish from the potential use of nanoindentation to elucidate mechanisms and highlight opportunities for further study. Moving beyond hardness (i.e. strength) and considering strain rate sensitivity is now possible with indentation, as new experimental approaches have been developed. Typically nanoindentation is used for exploring a deformation resistance (i.e. hardness and modulus of elasticity) on a local scale [24], and use of the continuous stiffness measurement (CSM) approach [25] enables continuous measurement of hardness with depth during indentation, through measurement of the phase change of an applied oscillation of the tip displacement and the measured response of the material. Recent work has highlighted that this CSM approach can be very valuable in performing variable strain rate testing and therefore accessing the strain rate sensitivity at a local scale, such as exploring the SRS of nanocrystalline and ultrafine-grained metals [26–30]. The SRS at the nanoscale [36,37] and macroscale [38–41] of Ti alloys have been determined using either uniaxial tensile/compression or nanoindentation-based test with different methods (e.g. constant strain rate and strain rate jump test). Observed m values were within a range of 0.007–0.04 for CP-Ti [36–39,41], 0.0185 for Ti–6Al and 0.01 for Ti6242 [40], where the m value of metallic materials is in general lower than 0.1 at room temperature [42]. Direct comparison across alloys from these studies is not possible as microstructural features, such as grain size, and testing method were not similar. Within this work, we use the CSM technique due to its fairly-straightforward experimental setup and data analysis. We note that there are some significant limitations that must be considered with this approach [28,43], such as: (i) indentation data are sensitive to microstructure; (ii) thermal drift may play a prominent role at lower strain rates (i.e. longer testing time); and (iii) significant experimental error can be occurred in soft metallic materials (e.g. single crystal) with a large E/H ratio. Recently several authors have reported [28,44–46] using a modified CSM method to study SRS effects on hardness by utilising step changes in load rate in a single indentation. This has the advantage that a single indentation can be utilized to study the effect of rate sensitivity on hardness and this can also overcome issues with environmental instabilities leading to thermal drift artefacts in very slow strain rate indentations. However, while the modified CSM method has been shown to work well on monocrystalline metals, there are issues with interpretation of the data on coarser grained materials. This is especially important when there are significant indentation size effects [47] that lead to an increased hardness as a function of indentation depth, superimposed on-top of any change in hardness due to strain rate changes. Secondly the hardness change measured is by definition taken at points where differing volumes of plastic deformation have occurred. In nanocrystalline materials this is not of concern as the ‘length scale effects’ are linked to the nano-grain sizes, rather than the indentation volume. However in other microstructures this can lead to comparing hardness changes which are controlled by both sampling differing microstructures, e.g. due to heterogeneity or length-scale effects under the indenter impression, as well as strain rate changes. While nanoindentation approaches can clearly measure different rate sensitivities during deformation, care must be taken in interpretation, as comparison with more conventional engineering measures of strain rate sensitivities is difficult because the local strain field around the indenter during each test is rather complex [14,48]. Here, we describe rate sensitivity (i.e. equivalent to SRS obtained from uniaxial tensile or compression test) as the material response to the variable indentation rate. We use nanoindentation to provide a relatively simple and inexpensive method of testing multiple individual microstructural units in complex alloys at an appropriate lengthscale. In this work, we investigate local rate sensitivity in dual-phase Ti alloys with a nanoindentation CSM method using multiple indentations at differing strain rates. This aims to provide a mechanistic insight on how the rate sensitivity varies with respect to crystal orientations (i.e. hard and soft grain). We focus on this as dwell fatigue has a time-sensitive load-hold and different orientations are important for the rogue grain combination within the Stroh model."
published_053,"There is a strong and growing literature on resilience, addressing the challenge how we can make a system able to sustain or restore its functionality and performance following a change in the condition of the system (disruption, threat, opportunity) (e.g. [8,12,18,21,22,24,28,31,35,36,41]). In the following such changes are referred to as events. By a system it is understood a collection of elements or components that are forming a unified whole for example organised for a common purpose. The resilience management (engineering) can be conducted without considering risk. For example having redundancy in the system may be an effective resilience management strategy, and it does not require assessing specific events and associated risk, to be implemented. This is the great attraction of resilience management. We do not need to know what type of events that can occur and express their likelihoods as needed in traditional risk assessments. In situations with large uncertainties, this is important as risk assessments then are not able to produce reliable probability estimates. It is of special relevance for complex systems, where it is acknowledged that surprises will occur. Resilience analysis and management are especially suited for confronting unknown and uncertain categories of events, and both quantitative and semi-quantitative approaches for resilience assessment have been proposed (e.g., [11,15–17,30]). Traditional risk assessment is not a part of the methodology used in these studies. The links between resilience and risk have been discussed by several authors, see e.g. [18], [2], [33] and [32]. The present paper seeks to bring the discussion forward by making a careful study on how risk considerations can support resilience analysis and management. The main aim of the paper is to argue for the thesis that risk assessments can provide useful input to the resilience analysis and management. These assessments are not traditional quantitative risk assessments searching for accurate probability estimates, but broad qualitative assessments of events, recovery (return to the normal condition or state) and uncertainties. The objectives of these assessments are to obtain insights by Making a judgement of the type of events that can occur, what we know and do not know (highlighting key assumptions and justified beliefs). Making a distinction between known types of events, unknown types of events, and surprising events. Assessing the probability for these types of events whenever found meaningful (using subjective probabilities or subjective interval probabilities). Assessing the strength of knowledge supporting these judgements. How can the knowledge be strengthened? Conducting assessments to reveal unknown and surprising events. Assessing the degree of resilience is difficult. As an example think about the human body. It is commonly considered resilient “in its ability to persevere through infections or trauma. Even through severe disease, critical life functions are sustained and the body recovers, often adapting by developing immunity to further attacks of the same type” [31]. An example of a critical life function is breathing. However, the human body can also be considered vulnerable ‒ history has shown that, if medical advances like penicillin had not been made, the consequences of some bacterial infections would have been devastating. This may suggest a characterisation of the human body as quite resilient but not highly resilient, but how should we distinguish between quite resilient and highly resilient? Or, in more general terms, how should we measure the degree of resilience? A number of methods and metrics have been suggested for this purpose; see for example [20], [12] and [23]. Some of these methods and metrics are simple, others very complicated as these papers show. As an illustration of a simple metric, consider the definition by Hashimoto [19], who defines the resilience of a system as the conditional probability of a satisfactory (i.e. non-failure) state in time period t + 1 given an unsatisfactory state in time t. Many authors have argued that resilience cannot simply be measured in a single unit metric, for example [18]. According to Haimes, the question “What is the resilience of system x” cannot be answered as it would require knowing whether system x would recover following any attack (event) y (also unknown types of events). What can be done, however, is to study how the system functions – what are the outputs (the consequences) of the system ‒ for any specific inputs (events). The point made by Haimes is important. We cannot see resilience independently of the events. Say that a system can be subject to two types of events, A1 and A2. The system is resilient in relation to event A1 but not to A2. Now suppose A2 will occur with a probability of 0.000001% and A1 with a probability 0.999999%. Is the system resilient? Yes, with high probability. If we allow for unknown and surprising types of events, we cannot conclude in the same way as there is no basis for making the probability judgements. Haimes’ statement on the difficulty of measuring resilience is thus reasonable. We are faced with a basic problem, which events to include in the resilience judgements? With respect to some events, the system considered may have shown itself to be resilient in the sense that it was able to recover and sustain its performance. However, for other events the system may not have shown itself to be resilient, and when facing the future it may turn out that the system will also experience recovering problems when faced with other events, of known or unknown type. As the above redundancy example indicates, we can still perform effective resilience management without the need for quantifying or expressing probabilities. We know that resilience can be improved in many ways – key instruments are strengthening of structures and processes within the system to defend it against events (like the immune system of the body), diversification, flexible response options, and the improvement of conditions for emergency management and system adaptation [35]. We do not need to identify all possible events and assess their likelihoods to understand that a specific resilience arrangement or measure will be useful in many situations. However, there is a need for ways of describing and comparing the resilience for alternative arrangements and measures. Resource limitations mean that we have to prioritise – where should we improve the resilience? There could be a huge number of areas in which the resilience can be improved, but which should be selected and given weight? Many resilience metrics exist as mentioned above, but what events will in fact occur? For the above example with the two events A1 and A2, we can think of a specific arrangement that could significantly improve the resilience with respect to event A2, but its effect on risk (interpreted in a wide sense) could be marginal. The arrangement could still be justified, but some type of considerations of risk seem useful, also in the case that we have difficulties in assessing likelihoods and being accurate on what type of events that will occur, as we always need to make prioritisations. The question is rather how can we make these considerations of risk informative? The present paper provides some ideas and guidance on how to think in relation to this challenge. The present paper argues that resilience analysis can benefit from recent risk analysis developments which highlight the knowledge and strength of knowledge judgments supporting the probability judgments. Subjective probabilities (and subjective interval probabilities) can always be assigned and used, even if the uncertainties are large – however, they should be supplemented with strength of knowledge judgments. The idea is that any probability judgment is conditional on some knowledge and this knowledge could be more or less strong, and even erroneous, and the risk assessment needs to take this into account (refer to Appendix for details). In the same way the paper argues that the resilience analysis needs to incorporate such judgements in their analyses. Risk assessment and management supplement resilience analysis and management by addressing the potential occurrences of events. Through such analysis new insights may be gained, for example, unknown and potentially surprising types of events could be identified, and new “cause-effect” relationships can be revealed. Concrete and more effective measures can then be developed to meet these events. By studying why certain infections occur, more effective measures can be developed than if the focus is limited to how to make the body withstand infections in general. When using the term resilience analysis it is referred to all type of activities with the purpose of understanding and characterising the resilience (including understanding how the system performs when subject to specific events), whereas resilience management captures all activities conducted to change (normally to improve) the resilience, for example the implementation of a measure to strengthen the resilience. When studying resilience we may distinguish between being resilient and performing resiliently, in the same way as distinguishing between being safe and performing safely. In the present paper we talk about the system being resilient but the discussion also applies to the system performing resiliently, for example when considering the operation of a system. The paper is organised as follows. In the coming Section 2 the meaning of key risk and resilience related concepts are clarified. We use the recent conceptual ideas and definitions from the Society for Risk Analysis [37] as the basis for this analysis. Then two examples to illustrate the analysis are presented (Section 3), covering a safety and a security application, respectively. Section 4 follows up the previous sections with some further discussion on the link between resilience and risk, and the final Section 5 provides some conclusions. The Appendix presents some fundamental ideas about probability, which are useful for the discussion in the coming sections."
published_054,"Due to continuing technological advancement, an increasing number of geochemically scarce metallic raw materials1 are entering into our daily lives. With a reversal of this trend not foreseeable (Zepf et al., 2014), there are growing concerns for the security of raw material supply. For many raw materials, the supply situation is considered critical due to: (i) their production being concentrated in a few countries (Simoni et al., 2015), (ii) limited options for appropriate substitutes (Graedel et al., 2013), and (iii) very low recycling rates for these materials (UNEP, 2012). To improve the long-term sustainability2 of critical material supply (Giurco et al., 2014), there is a view that raw material management needs to be rethought (Ongondo et al., 2015). Specifically, raw material management needs to consider the mining of materials from both the geosphere and the anthroposphere. To ensure comparability and consistency, both mining management approaches should be developed and evaluated in parallel. In the cycle of a material, a parallel development and evaluation requires the establishment of linkages between mining the geosphere, anthroposphere and the subsequent processing. In this sense, for both the mining of geosphere and anthroposphere, knowledge of the material (e.g. physical and chemical properties, element concentration, and abundance) and knowledge about potential economic viability is required (Brunner, 2008). Raw material supply has previously been evaluated based on the ‘availability’ of materials (UNEP, 2013b; USDOE, 1996). Evaluation of material availability can be based, for example, on the geological knowledge (UNFC, 2010). Availability can also be evaluated through material criticality assessment, which assesses raw material supply based on two functions: their ‘availability’ and ‘importance of uses’ (Graedel et al., 2012). Studies of material availability show a large degree of variability in how availability is defined. It has been suggested that material availability evaluation is too narrow in its scope and that evaluation of raw material supply should be expanded to consider the ‘accessibility’ of materials (USDOE, 1996). Cook and Harris (1998), for example, recommend that such an evaluation should consider environmental, legal, social, and political aspects in addition to an evaluation of project feasibility. This would be particularly important for materials that are currently unavailable but approachable. Materials in this category include for instance the large amounts of illegally-exported raw materials from End-of-Life (EoL) products, such as obsolete Waste Electrical and Electronic Equipment (WEEE) from the European Union (Huisman et al., 2015). Rankin (2011) adds that it is important to understand, how access to raw materials will change in the long term. Gruber et al. (2010) considered raw material ‘accessibility’ in relation to policies about raw materials at the European level and they concluded that indicators and specific targets for raw material conservation remained absent. Accessibility has further been applied to evaluate product recycling, specifically in identifying the relevant product parts for dismantling (Hagelüken, 2014) and in geological mining, where ‘accessibility’ has been used to describe the physical path to a deposit (Weber, 2015). At a systems level, individual aspects of evaluating raw material accessibility have been implicitly included in the fields of economic geology. For instance, accessibility has been integrated in resource classification frameworks (Cook and Harris, 1998) and ecological and social sustainability studies (MacDonald, 2015). There is need to advance the management of raw materials at different levels. Firstly, there is a lack of consistency in how the terms ‘availability’ and ‘accessibility’ are used in studies of raw material supply and what these terms actually mean. Clarification of fundamental terms used in the evaluation of raw material supply is required before a commonly agreed, rational raw material mining strategy can be developed (Cossu and Williams, 2015; Winterstetter et al., 2015). Secondly, although different efforts have been undertaken to link quantitative evaluation methods across different disciplines, there is a lack of a broadly applicable assessment method for a potential supply of sustainable raw materials (Haines et al., 2014). Thirdly, there is a deficiency in a strategy that evaluates the different operational steps along the collection/mining, processing for continual sourcing of raw material (Roelich et al., 2014). Fourthly, there is need for consistent quantitative evaluations for elements with few available data such as rare earths (Gleich et al., 2013; Weber, 2013). This is particularly important for implementing new waste management regulations, such as the currently revised Swiss ‘ordinance for the return, take-back, and disposal of electrical and electronic equipment’ (ORDEE). The future ORDEE will require for the first time the recovery of scarce metallic elements from technological equipment wherever possible (FOEN, Federal Office for the Environment Switzerland, 2013). In this paper we aim to establish a consistent framework for evaluating raw material supply from both anthropogenic and geological sources at an early project development stage. The objectives were to: systematically investigate the use of fundamental terms in the evaluation of raw material supply; develop a novel, consistent framework for evaluating the supply of raw materials; and demonstrate the utility of the developed framework by evaluating the raw material supply in four rare earth element (REE) case studies."
published_055,"A machine tool is a mechanically powered device used during subtractive manufacturing to cut material. The design and configuration of a machine tool is chosen for a particular role and is different depending, amongst other things, on the volume and complexity range of the work-pieces to be produced. A common factor throughout all configurations of machine tools is that they provide the mechanism to support and manoeuvre the functional position, and sometimes the orientation, between the cutting tool and work-piece. The physical manner by which the machine moves is determined by the machine’s kinematic chain (Moriwaki, 2008). The kinematic chain will typically constitute a combination of linear and rotary axes. Fig. 1(a) shows an example of a five-axis gantry machine tool that has three linear and two rotary axes which are used to move the tool around the work-piece. Typically, this configuration of machine will be used to machine heavy, multi-sided, large volume work-pieces. Fig. 1(b) shows an alternative design of a three-axis C-frame machine tool. This particular machine tool configuration consists of three linear and no rotary axes and is typically used to machine smaller work-pieces than the five-axis machine. In a perfect world, a machine tool would be able to move to predicable points and orientations in three-dimensional space, resulting in a machined artefact that is geometrically identical to the designed part. However, due to tolerances in the production of machine tools and wear during operation, this is very difficult to achieve mechanically. Pseudo-static errors are the geometric positioning errors resulting from the movement of the machine tool’s axes that exist when the machine tool is nominally stationary. Machine tool error mapping is the process of quantifying these errors (Schwenke et al., 2008) so that predictions as well as improvements of part accuracy can be made via numerical analysis and compensation. The significance of the error mapping process is dependent on application; manufacturers machining high value parts to tight tolerances, usually in the order of less than a few tens of micrometres, should have their machines regularly error mapped otherwise they are at risk of producing non-confirming parts. Manufactures with broader tolerances may calibrate less frequently. There are many error components that collectively result in deviation of the machine tool from the nominal. For analytical and correction purposes, it is important to measure each error component. For example, as seen in Fig. 2(a) a machine tool with three linear axes will have twenty-one geometric errors. This is because each linear axis will have six-degrees-of-freedom and a squareness error with the nominally perpendicular axes (Mekid, 2009; Schwenke et al., 2008). Therefore, a three-axis machine tool will have a total of twenty-one geometric errors. Additionally, as seen in Fig. 2(b) a rotary axis will have six motion, two location errors, and two squareness errors (Bohez et al., 2007; Khim and Keong, 2010; Srivastava et al., 1995). Therefore, a five-axis machine tool will have a total of forty-one geometric errors. The measurement of each error will involve the use of a test method and a measurement device. The selection of equipment will usually be done in unison with the test method, influenced by the engineer’s preference. However, there are many cases where many different instruments can be used for performing the same test method, where each require a different duration to install and perform the test. For example, both a laser interferometer and a granite straight edge can be used to measure the straightness of a linear axis. The laser interferometer might take longer to set-up, but if the machine has an axis with a long travel, the granite straight edge might need to be repositioned multiple times to measure the entire axis, therefore, taking more time to perform. For most manufacturers, removing a machine tool from manufacturing has large financial implications. Downtime can be in excess of £120 per hour (Shagluf et al., 2013). Therefore, the accumulative cost for a manufacturer with many machines can be large. For example, consider a manufacturer with 10 machine tools, each of which undergoes a 12 h error mapping exercise per year. The estimated downtime cost would be £14,400. However, this is a conservative figure for many high value manufacturing companies. The estimated uncertainty of measurement is a parameter associated with the result of a measurement that characterises the dispersion of the values that could reasonable be attributed to the measurand (BIPM, 2008). The uncertainty of measurement is calculated for each individual measurement and the accumulative estimated uncertainty of measurement has a direct effect on tolerance conformance of parts manufactured using the machine. Therefore, from the manufacturer’s perspective, it is desirable to reduce the estimated uncertainty of measurement. The estimated uncertainty of measurement is affected by change in environmental temperature. If the same calibration plan was carried out at different times throughout a working-day while the temperature is continuously changing, the accumulative estimated uncertainty would also change. Depending upon the manufacturer’s motivation for performing the error map, they may want to optimise the error map plan to either minimise financial cost, or maximise the quality of the error mapping exercise. The following different optimisation criteria considered in this paper are: (1) the reduction of machine tool downtime, (2) the reduction of the estimated uncertainty of measurements, and (3) balancing the two parameters with the possibility of customising their individual weighting. The change in environmental temperature throughout a measurement, as well as between interrelated measurements, will have a significant impact on estimated uncertainty of measurement ISO230-9 (2005). The decision making process involved for construction optimal error maps plans is exhaustive. However, computational intelligence in the form of domain-independent Artificial Intelligence (AI) can be used to provide optimal solutions when given a model of the problem (Ghallab et al., 2004). In this paper, a description of individual factors that affect a machine’s downtime and estimated uncertainty of measurement during error mapping are defined. This leads to a discussion of a previously developed model (Parkinson et al., 2012a, 2012b) that can be used by state-of-the-art domain-independent AI planning tools to find optimised solutions (Russell et al., 1995). The development of this model to produce measurement plans that are optimised to reduce machine downtime and the estimated uncertainty of measurement due to the plan order is presented and discussed. This multi-objective optimisation is motivated by the desire to reduce both machine tool downtime and the uncertainty of measurement, which to some extent are competing as a temporally optimised plan will generally have a high estimated uncertainty of measurement. Following the development of a multi-objective model, twelve different case-study examples are presented and described to show the ability to generate plans that are optimised for (1) downtime, (2) uncertainty of measurement, and (3) the arithmetic mean of them both. The generated calibration plans are then examined and discussed to evaluate their fitness-for-purpose. It is then identified that computational resources are restricting the planner’s ability to find optimal solutions in ten minute time allocation. This leads to a further investigation into the produced measurement plan when solving the planning problem on both personal and High Performance Computing architectures."
published_056,"Students' incentives for engaging in learning activities and the way they perceive their competence are important motivational precursors of achievement outcomes, including school performance (e.g., grades; Marsh, Trautwein, Lüdtke, Köller, & Baumert, 2005) and educational choices (e.g., choosing non-compulsory courses; Simpkins, Davis-Kean, & Eccles, 2006). These effects also seem to apply to performance in specific tasks (e.g., task interest and self-efficacy in a problem-solving task; Niemivirta & Tapola, 2007) and to achievement in different subject areas (e.g., value and self-concept in reading; Schoor, 2016), among younger students (Eccles & Wigfield, 1993) as well as older students (Guo, Parker, Marsh, & Morin, 2015). Longitudinal studies have been conducted on the development of domain-specific motivation (i.e., students' relatively stable motivational beliefs in relation to a subject domain such as mathematics; Jacobs, Lanza, Osgood, Eccles, & Wigfield, 2002) and its relations to achievement (Arens et al., 2017; Seaton, Marsh, Parker, Craven, & Yeung, 2015), but regarding task-specific motivation, research seems to have focused on single tasks or situations (Ainley, 2006). Relatively few studies have looked at the consistency of task motivation across tasks or stability over time, or its predictions on domain-specific motivation and achievement within a longer time span (Fryer, Ainley, & Thompson, 2016; Rotgans & Schmidt, 2011). The available work has mainly focused on out-of-school settings (e.g., extra-curricular courses) and older students (e.g., Knogler, Harackiewicz, Gegenfurtner, & Lewalter, 2015). As it is often argued that domain-specific motivation (e.g., intrinsic value and self-concept in mathematics) accumulates through repeated experiences in tasks and situations that reflect certain subject areas and related activities (Bong & Skaalvik, 2003; Hidi & Renninger, 2006), it would seem reasonable to investigate whether students' task-specific motivation generalizes across different tasks, and whether they predict similar experiences and domain-specific motivation over time. Accordingly, the aim of the present study was to examine i) the consistency of students' interest, success expectancy, and performance across different tasks (i.e., cross-task consistency) and over time (i.e., longitudinal stability), ii) their longitudinal reciprocal relationships, and, iii) their predictions on intrinsic value, self-concept, and achievement in mathematics. The effects of gender were also taken into account."
published_057,"Urine is a high concentrated wastewater stream, which contributes with around 75% of the total amount of nitrogen (N) and 50% of the total amount of phosphorus (P) present in domestic wastewater [1]. At the same time, urine accounts for only 1% of the total volume of wastewater. Undiluted fresh urine, collected by separation toilets and water-free urinals, has a chemical oxygen demand (COD) concentration of 10 g L−1 and a total nitrogen concentration of 8.1 g L−1 [2]. In fresh urine most of the N is mainly found as urea [3]. During storage, urea is hydrolyzed due to microbial urease activity into ammonia (NH3)/ammonium (NH4+) and carbonate (CO32−) [4]. Urine stored during 6 days, contains a COD concentration of 9.3 g L−1 and approximately 1.9 g L−1 of ammonium nitrogen (NH4+-N) [5]. Thus, urine can be considered as a valuable wastewater stream for energy production and N recovery in bioelectrochemical systems (BESs) [6]. BES can be operated in a microbial fuel cell (MFC) mode if electrical energy is produced or in a microbial electrolysis cell (MEC) mode if energy is supplied for hydrogen (H2) production [7]. MECs have several advantages over MFCs: (1) oxygen supply to the cathode is not required in an MEC and (2) H2 gas has a higher economic value than electricity [8,9]. BES for N recovery are commonly composed of two compartments, an anode and a cathode chambers separated by a cation exchange membrane. At the anode, electrochemically active bacteria (EAB) are used to oxidize organic substrate producing protons and electrons [10,11]. Electrons flow to the cathode chamber through an external circuit generating electricity. Electricity will drive the migration of protons and cations such as NH4+ from the anode to the cathode through the membrane, where it is converted to NH3 and easily stripped and recovered [8]. In general, MECs can reach higher current densities and therefore higher N removal rates than MFCs [12]. Kuntke and co-workers demonstrated N-recovery and H2 production in an MEC as a viable treatment option for diluted urine [8]. However, in their work only 34% of the NH4+-N was removed suggesting that further improvements are necessary to increase N-recovery. MECs fed with fermentable substrates are, in general, outperformed (in terms of the Coulombic efficiency (CE)) by MECs using non-fermentable substrates [9]. Several studies have shown the efficient conversion of volatile fatty acids (VFAs) such as acetate and butyrate [13] and propionate [14] directly into electricity. Chae and co-workers investigated the effect of four different substrates (acetate, butyrate, propionate and glucose) on MFC performance [15]. Glucose-fed-MFC showed the lowest CE due to glucose consumption by diverse non EAB via several metabolic pathways including fermentation and methanogenesis [15]. Among several hydrolysable composites such as lipids, proteins and carbohydrates, urine contains fermentable compounds including glucose and lactate [16,17]. The presence of fermentable compounds in urine will promote the growth of non-EAB. Non-EAB will be the responsible of diversion of electrons due to the substrate consumption via other metabolic pathways such as methanogenesis, nitrate and/or sulphate reduction, thus reducing the Coulombic efficiency of the system. In a two-stage bioconversion process (TSBP), the complex biodegradable compounds that are not directly converted by the EAB to electricity are firstly converted in a preliminary anaerobic fermentation step to simple compounds namely VFAs, which can be directly converted by the EAB to electricity in BES [18]. Cerrillo and co-workers investigated the integration of the anaerobic digestion process with a BES in order to improve the COD removal and N recovery from pig slurry [19]. In this study, pig slurry was firstly submitted to a process of anaerobic digestion in a lab-scale continuous stirred tank reactor. The effluent of the anaerobic digestion was then used as feed for a subsequent MEC. MEC operated with digested pig slurry showed higher COD removal and higher CE compared with MEC fed with raw pig slurry [19]. Several works have been highlighting the potential of using BES as a downstream step to recover energy from anaerobic digestion effluent [20–22]. Since urine contains fermentable compounds that can be used by diverse non-EAB via several metabolic pathways, a TSBP may be a suitable option to improve anodic performance of a MEC with urine. In the present work, the MEC operation in combination with a previous anaerobic fermentation step of urine was investigated to improve the COD removal and the current production. This current will drive the migration of NH4+ from the anode to cathode. Ammonia can be recovered from the cathode [23]. At first, a preliminary anaerobic biodegradability assay was performed to assess the anaerobic biodegradability of urine. Afterwards, anaerobic urine fermentation over 15 days was performed to convert complex organic compounds present in urine into simpler compounds. The fermented urine was then used as feed for a MEC. Finally, the organic compounds found in urine before and after fermentation as well as the metabolic products associated to urine degradation were analyzed by proton nuclear magnetic resonance (1H NMR)."
published_058,"Our environment and quality of life are affected by the devastating consequences of global population growth. It will be increasingly difficult to provide food and energy for such a dense population [1]. Recent economic developments in many countries all around the world have heightened the need for alternative energy resources, based mainly on renewable sources due to the depletion of fossil fuel, increasing energy demand, greenhouse gasses emission and global warming. All the mentioned perspectives have strengthened the interest in alternatives that are renewable, sustainable, and economically viable [2,3]. Biomass has recently received increasing attention as an attractive energy resource for the production of renewable biofuels and other value-added chemicals due to being a significant renewable resource and the only viable feedstock for carbon based fuels and chemicals [3–5]. The concept of a “biorefinery” describes all the processes and technologies that are involved in optimal use of biomass resources. The incoming raw material is completely converted to a range of products such as fuels and value-added chemicals [6–8]. In this context, research on second-generation biofuel is focused on the more abundant and often relatively cheap plant-derived lignocellulosic biomass. The bulk of the lignocellulosic biomass consists of three basic polymeric components: cellulose, hemicellulose and lignin. Cellulose and hemicellulose are complex polysaccharides [9]. Lignin is an amorphous, highly branched phenolic polymer and the third most abundant biopolymer in nature, which accounts for 10–30 wt% of the feedstock and carries the highest specific energy content of all the three fractions. Lignin can be obtained as a cheap byproduct either from the pulp and paper industry or from bio-ethanol production. At present, it is mostly burned as low value fuel for process energy purposes and only approximately 2% is used commercially [1,5,9–12]. Sustainable use of biomass must include using all fractions of the raw material to make products with high value [8,13]. Various types of biofuels and value-added chemicals have been produced from cellulose and the technical feasibility of this process has been well demonstrated [14]. However, major challenges must be addressed to develop the valorization of lignin to provide chemicals and fuels [11,15]. Lignin as a feedstock has significant potential for sustainable production of fuels and bulk chemicals and is regarded as the major aromatic resource of the bio-based economy [7,16]. However, the amount of lignin and the structural and monomeric composition of lignin within the lignocellulosic biomass varies from tree to tree, species to species, and even in samples from different parts of the same tree [1,9,12]. Different pathways have been explored for the conversion of lignin and lignin-rich residues to fuels and bulk chemicals. Thermochemical processes such as pyrolysis or catalytic pyrolysis, liquefaction, gasification, solvolysis and hydrogenolysis are among the most interesting concepts investigated in this respect [3,5,17–21]. In a review by Mohan et al. (2006), products from fast pyrolysis of biomass in the absence of oxygen were shown to be potentially useful both as an energy source and a feedstock for chemical production [22]. Yan et al. (2008) explored cleavage of ether bonds present in the lignin structure during reductive depolymerisation of lignin with hydrogen [23]. In this study, birch wood sawdust was converted to four main monomers, namely guaiacylpropane, syringylpropane, guaiacylpropanol, and syringylpropanol using an organic solvent and a series of active carbon supported catalysts, such as Ru/C, Pd/C, Rh/C, and Pt/C, at 523 K and 4 MPa H2 pressure for 0.5–2 h. The maximum monomer yields were 46% relative to the initial lignin mass for Pt/C catalyzed conversion in dioxane. Production of value-added chemicals from lignin requires the simultaneous depolymerisation of the lignin structures with subsequent hydro-deoxygenation of the lignin monomers and alkylation of aromatic rings, and is thus very complex [11,15,24]. Several studies have shown that the use of formic acid as a hydrogen donor enhances the depolymerisation of lignin. In 2008, Kleinert and Barth reported an innovative conversion method for valorization of lignin termed lignin to liquid (LtL). The LtL-process can be considered as a thermochemical solvolysis process in a liquid or near-critical reaction medium at high temperature and high pressure. Lignin to liquid conversion involves the use of a hydrogen donor solvent instead of molecular hydrogen. A well-known hydrogen donor is formic acid. At the given conditions, the formic acid acts not only as an in situ hydrogen source, but also as assists in the depolymerisation of the lignin molecule, and thus results in higher oil yields than comparable reactions with H2 as reductant. A recent study reported by Oregui-Bengoechea et al. (2017) has revealed that the decomposition of formic acid and the chemical reaction between lignin and formic acid are competing reactions, and based on this study, a formylation–elimination–hydrogenolysis mechanism for the formic acid aided lignin conversion has been proposed. Commonly used solvents are water and ethanol [8,11,26]. The reaction product consists of a well-separated mixture of an organic top layer and an aqueous bottom layer with a total liquid yield of up to 90 wt%. The liquid organic phase is an oil with a high H/C and a low O/C ratio relative to the lignin input. GC-MS analysis show that the liquid organic fraction mainly consists of phenols and aromatic compounds, and this makes it a potential source of components for blending in motor fuels, or for utilization in fine chemical production. The isolated phenolic fractions were reported to be within 25–35 wt% of the lignin input and are quite high compared to the reported values from pyrolysis of similar feedstocks at the same reaction severities [8,9,12,13,24,25]. Furthermore, Huang et al. (2014) reported that hydrogenolysis of lignin using formic acid as an in situ hydrogen donor in the presence of a water-ethanol solvent mixture has contributed to more than 90 wt% depolymerized lignin yield from Kraft lignin even without any catalysts [26]. In a study by Kloekhorst et al. (2015), the catalytic solvolysis of Alcell® lignin using formic acid as hydrogen donor and Ru/C as the catalyst in an alcoholic reaction solvent has been shown to enhance the hydrodeoxygenation and thus reduce the oxygen content of the bio-oil significantly. The best result reported in this study was conversion of approx. 71 wt% of lignin input to bio-oil with low O/C ratio (0.09). High yields of valuable chemical compounds such as alkylphenolics and aromatics were determined using GCxGC-FID [27]. Hita et al. (2018) recently reported that 16–29 wt% of lignin can be converted directly into valuable platform chemicals through solvent-free hydrotreatment using a Fe-based limonite catalyst at 450 °C for 4 h 67–81% of the lignin oil components were detectable by 2D GCxGC-FID confirming that the majority of the lignin oil is composed by volatile components with low molecular weight [28]. However, optimizing process conditions yielding high amount of the desired products is challenging and time-consuming, especially since it is probable that there are interactions between different experimental factors. In a study reported by Kleinert et al. (2009), optimizing experiments have shown that high-pressure conditions give high product yield [24]. However, some important reaction parameters such as (i) shorter reaction time, (ii) lower reaction temperature and (iii) reduction of low-value side products, i.e., gas and solid residues need to be improved to make LtL-oils competitive with fuels and chemicals obtained from petroleum [21]. The development of the LtL-process and the chemical composition and bulk properties of LtL-oils produced in small laboratory scale have been described in previous papers [8,11,12,21,24,25]. In the next step of development towards industrial scale production, the effect of increasing the scale must be investigated, and the conversion needs to be optimized at larger scale. The larger product volumes also makes testing of separation and upgrading processes, e.g., distillation, possible. In this work, all experiments done in laboratory scale have been multiplied by a factor of 200 and performed in a 5 L reactor where pressure readings and stirrer torque were continuously recorded and controlled, to investigate whether the effect of the reaction variables are similar for the different reactors. In addition, the 5 L reactor is equipped with a stirrer, which could improve the mass transfer in the system and give a faster reaction. The main purpose of the work reported here is: (i) to compare amount of oil and char produced in stirred 5 L scale with 0.025 L laboratory scale, (ii) to find reaction conditions that give high oil yields of good quality in large scale, (iii) to provide an evaluation of product distribution and product composition in terms of reaction scale and various reaction parameters, such as lignin type, reaction temperature, reaction time and reaction solvent."
published_059,"As the energy efficiency of a ship, i.e. the amount of fuel consumed per unit of transport supplied, is a function of both the technical specification and the way in which a ship is maintained and operated, one can distinguish between “technical efficiency” which refers to some baseline conditions, and “operational efficiency” which takes into account the practicalities of the voyage, variability in environmental conditions and commercial realities of operations. One example of the former is the Energy Efficiency Design Index (EEDI) (Buhaug et al., 2009) while a measure of operational efficiency can be obtained by taking measurements of fuel consumption and work over a period of time (IMO, 2005). Energy efficiency is expected to be an important feature for a firm operating a ship, as it influences its overall costs and revenues. There are a number of different markets in shipping where energy efficiency might be reflected in prices: the new build market, the second hand market and the charter markets, both voyage and time charter. In the voyage market, charterers hire ships on a given route and pay a fixed amount, which includes fuel consumption, while in the time charter market the daily price for hiring a ship excludes the fuel costs which are additionally borne by charterers. This article focuses on the time charter market as it represents a classical example of the principal–agent problem, also known as split incentive and tenant–landlord problem (Blumstein et al., 1980; Brown, 2001 and Graus and Worrel, 2008; Vernon and Meier, 2012 in the transport sector), although the verification of the agency problem on the level of investments is not tackled as the data do not enable us to compare the optimal and the observed level of investments. In the time charter vessel owners (agents) decide the level of technological energy efficiency, while charterers (principals) bear the costs associated with agent’s chosen level of energy efficiency, i.e. the fuel bill in the case of the shipping market. This paper quantifies the extent to which the fuel savings related to energy efficiency ships are captured by ship owners through higher charter rates using a linear regression of about 2000 fixtures in the Panamax dry bulk market observed between 2007 and 2012. This is an important endeavour, as this issue directly impacts the revenues of ship owners and therefore their incentive to invest in energy efficiency. Panamax refers to ships with deadweight (payload capacity) of approximately 60,000–80,000 tonnes which are designed to have maximum capacity whilst being able to transit via the Panama Canal, although according to the taxonomy of the database used in this study Panamax ships range between 60,000 and 100,000 tonnes. We selected the Panamax dry bulk sector to carry out our analysis because of its reputation for being competitive (Stopford, 2009). This sector was attributed a total of about 50 MtCO2 in 2007, i.e. about 5% of total sea transport emissions, a considerable quantity which is expected to increase due to higher demand for shipping services (IMO, 2009). The paper is structured as follows. Section 2 discusses econometric analyses of the time charter market, rewards of energy efficient investments in the shipping sector and the way energy efficiency can be defined. Section 3 describes the data we use in this article while Section 4 discusses the estimation and the result presented in this study. Section 5 draws the conclusions and the policy implications of our work before presenting recommendations for further work which could help take the analysis in this paper forward."
published_060,"Many signaling proteins have multiple isoforms that are often co-expressed in the same cell. Isoforms can arise from different genes or through alternative splicing of the same gene. The fact that green algae, yeast, Caenorhabditis elegans, and vertebrates all express protein isoforms reinforces the view that this tight evolutionary conservation underlies an important, isoform-specific biological function. Critical questions are therefore how some isoforms can be activated, but not others, when they are co-expressed and how different isoforms evoke distinct effects. A partial answer has been provided by the finding that isoforms of protein kinases are corralled to different sub-cellular locations (Mochly-Rosen and Gordon, 1998), controlling only those substrates constrained within the immediate vicinity. A more fundamental problem arises when protein isoforms share the same spatial domain and are activated by the same intracellular messenger. How can one isoform now be activated selectively and how might it gain access to targets that other isoforms are excluded from? This issue is nicely encapsulated by the NFAT family of transcription factors, which are essential for vertebrate development, differentiation, and function. Four members of the NFAT family (NFAT1–4) are activated by cytoplasmic Ca2+ and are encoded by highly homologous genes (Hogan et al., 2003). In resting cells, NFAT proteins are extensively phosphorylated and reside in the cytoplasm. Upon stimulation with agonists that increase intracellular Ca2+, these transcription factors are dephosphorylated by Ca2+-calmodulin activated protein phosphatase calcineurin, which exposes a nuclear localization sequence and thus enables the protein to translocate to the nucleus where it regulates gene expression (Müller and Rao, 2010; Wu et al., 2007). In immune cells, NFAT activation is tightly linked to Ca2+ entry through store-operated Ca2+ release-activated Ca2+ (CRAC) channels (Feske et al., 2006; Hogan et al., 2010), and NFAT reporter gene expression is dependent on Ca2+ influx (Kar et al., 2012b; Negulescu et al., 1994). CRAC channels open following a fall in Ca2+ from within the ER and control a plethora of cellular functions including secretion, gene expression, and regulation of growth and proliferation (Parekh and Putney, 2005). The channels are gated by stromal interaction molecule 1 (STIM1) and STIM2, which are ER Ca2+ sensors (Liou et al., 2005; Zhang et al., 2006). As the ER loses Ca2+, STIM proteins form multimeric complexes, which then migrate across bulk ER to occupy specialized regions of ER located below the plasma membrane (Hogan et al., 2010; Lewis, 2007; Parekh, 2010). At these sites, STIM proteins bind to and directly activate plasmalemmal Orai1 proteins, which comprise the pore-forming subunit of the CRAC channel (Prakriya et al., 2006; Vig et al., 2006; Yeromin et al., 2006). In RBL-1 cells, a model system for mast cell research, and HEK293 cells, NFAT1 translocation to the nucleus in response to physiological levels of stimulation of endogenous cysteinyl leukotriene type I (cysLT1) receptors is driven by spatially restricted Ca2+ signals, called Ca2+ microdomains, near open CRAC channels rather than by a global Ca2+ rise (Kar et al., 2011, 2012b). The microdomains extend just a few nanometers from the CRAC channel mouth but have privileged access to the NFAT pathway because of the formation of a store-dependent membrane complex that brings calcineurin and a pool of cellular NFAT very close to the Ca2+ channel (Kar et al., 2014). NFAT1 and NFAT4 are often co-expressed in cells, occupy the same cytoplasmic location, and are activated by intracellular Ca2+. Nevertheless, they have recently been found to exhibit marked differences in nuclear translocation dynamics. NFAT4 migration into, and export from, the nucleus was reported to be ∼5–10 times faster than the corresponding rates for NFAT1 (Yissachar et al., 2013). Because NFAT1 and NFAT4 are strongly expressed in immune cells, these differences in isoform dynamics might increase the temporal bandwidth for information processing in the immune system. However, the mechanistic basis for the different nuclear kinetics between the two transcription factor isoforms is unknown. The sub-cellular profile of the Ca2+ signal is critical in selective activation of downstream targets. Here, we compared the dependence of NFAT1 and NFAT4 nuclear accumulation on both local and global cytoplasmic Ca2+. We find that NFAT1 is tightly coupled to Orai1, whereas NFAT4 relies on a form of coincidence detection, requiring both local Ca2+ entry and a nuclear Ca2+ rise. Our study uncovers a mechanism whereby co-existing transcription factor isoforms are activated by distinct sub-cellular Ca2+ signals."
published_061,"Accurate localisation of radioactive materials is crucial for nuclear safety and security. The growing threat of attacks using radioactive substances, as well as an increasing number of nuclear plants requiring characterisation to support decommissioning, necessitate development of a portable system capable of detecting radiation sources in real-time. Radioactive substances are used in medicine, non-destructive testing, and power generation. For example, nuclear fuel used in power generation produces nuclear waste that needs to be carefully disposed of as it remains radioactive for a time period. If the source of radiation is not accurately identified and localised, radiation emitted by these materials can pose a health risk for personnel in close proximity to these materials. Localisation is particularly important during nuclear decommissioning and decontamination, since the actual location of the radioactive source within nuclear waste is often unknown. Highly-penetrating radiation may be released in a form of X- or gamma-ray photons or free neutrons, emitted as a result of nuclear processes occurring within the atomic structure of radioisotopes. All three types of radiation mentioned can be detected using different gas filled tubes, which are sensitive to a specific type (Jahoda et al., 2006; Vanier, 2004); examples are ionisation chambers, proportional counters and Geiger-Mueller counters. Scintillators can also be used in the same way as primary detectors to detect each type of radiation (Tous et al., 2011; Woodring et al., 2003; Nakamura et al., 2009). They transform energy detected into light pulses that can be counted by photosensitive electronic components e.g. photodiodes and photomultiplier tubes (PMTs). Although the detectors mentioned above generally provide good radiation sensitivity, by themselves, they offer no specific information about the location of the radioactive source. Preliminary results of a recent study by Schuster and Brubaker (Schuster and Brubaker, 2015) suggest that the anisotropic response of crystalline scintillation materials to neutron interactions may reveal some directional information about the source location. This type of information is of vital importance for nuclear decommissioning applications. Substances used in the types of detectors mentioned can be difficult to source. 3He, for instance, is mainly obtained as a byproduct from the radioactive decay of tritium; BF3, on the other hand, is a hazardous gas and as such not desirable (Kouzes et al., 2010). Radiographic film represents a safe implementation of radiation detector. For example, it has been employed in medical devices to perform skeletal body scanning with X-rays, successfully identifying fracture location (Lilley, 2001). A modification to the film detector, in the form of an imaging plate, allows detection of radioactive sources, where it is particularly useful for nuclear decommissioning applications (Hirota et al., 2011; Masumoto et al., 2002). Radiation imaging methods do not only detect an occurrence of a radioactive event but also provide the location of the incident. As such their potential has led to the development of radiation cameras – based on the operation principle of a general pinhole camera (Mortimer et al., 1954). Pinhole cameras, capable of identifying radioactive substances from a distance, are frequently used in nuclear decommissioning. CARTOGAM is an example of such gamma-ray pinhole imager (Gal et al., 2000, 2001; Cattle et al., 2004). The size of the pinhole determines the angular resolution and signal-to-noise ratio (SNR) of a specific camera; the former increases in a camera with a small opening, whereas the latter decreases when the diameter of a pinhole is decreased. It follows that an ideal single pinhole camera, would require an infinitely small pinhole to obtain the highest angular resolution. Conversely, the highest SNR would be achieved in a camera with an infinitely long diameter of the opening. Hence, in practical applications, a compromise must always be found between SNR and the angular resolution for the single pinhole cameras. In order to address this problem scatter-hole cameras were developed, where multiple holes of small diameter were distributed randomly on the aperture offering significant improvements in SNR, while maintaining good angular resolution (Dicke, 1968). However, reconstruction of the original image was difficult due to the random distribution of the pinholes. The issue was addressed by coded-aperture imaging, which was introduced with ‘encoding‘ and ‘decoding‘ arrays to simplify the process of image reconstruction (Fenimore and Cannon, 1978). Coded-aperture imaging is a technique adopted in X-ray cameras and telescopes, widely utilised in numerous space exploration missions (Gehrels et al., 2004; Hong et al., 2004; Tumerl et al., 1997; Del Monte et al., 2007; Feroci et al., 2007). Due to similar characteristics to X-rays, coded-aperture method has been incorporated into gamma-ray imagers for medical applications (Alnafea et al., 2006, 2007), non-destructive testing (Damato et al., 2007), explosives detection (Faust et al., 2009), as well as for nuclear decommissioning and decontamination; these are GAMPIX (Gmar et al., 2011), RADCAM (Woodring et al., 2003) and High-Energy Radiation Visualiser (HREV) (Sudarkin et al., 1996, 1998). Coded-aperture methods, for neutron source localisation, have been applied in the field of national security (Marleau et al., 2009; Woolf et al., 2012) and nuclear material detection at fuel cycle facilities (Hausladen et al., 46191). The scintillator based coded-aperture neutron imaging systems mentioned target fast neutron localisation. In each case pulse shape discrimination (PSD) methods are exploited to separate gamma-rays from neutron events. Multiple channel real-time PSD has been successfully implemented laying the foundations for high resolution (high number of pixels) organic scintillator based coded-aperture imagers (Joyce et al., 2014). However, identified neutron interactions must be localised within the scintillator so that the decoding process can be performed. The decoding process, which may contribute to the timing overhead of the localisation procedure, is required to infer the actual location of the radioactive substances. In this paper existing coded-aperture imaging systems are critically reviewed with relevance to radiation detection and localisation. The advantages of the coded-aperture technique over other methods are recognised, when used for X- and gamma-ray detection, as well as its potential for neutron imaging. Additionally, the coded-mask itself, as well as sensitive neutron detectors for such an application are also highlighted. The potential of employing coded-aperture based techniques for neutron imaging in nuclear decommissioning is evidenced by a simple model generated in Monte Carlo N-Particle eXtended (MCNPX) simulation software package."
published_062,"Of the different photovoltaic (PV) modules, the highest efficiency of transforming solar energy into electric energy is shown by monocrystalline photovoltaic modules. Transformation efficiency greatly depends on the proper temperature of photovoltaic modules; therefore, one of the simplest and most effective methods of increasing performance is to cool these modules during the warm summer period. Based on the global radiation map of the world, it can be concluded that the yearly amount of energy from the sun ranges between 800–2800 kW h m−2 on the horizontal plane, due to the various geographical locations. In Europe, this amount of energy typically ranges between 800–2000 kW h/m−2. In European terms, the natural endowments of Hungary are better than average, since the yearly amount of energy from the sun ranges between 1200–1360 kW h/m−2 on the horizontal plane. Based on the data of the Photovoltaic Geographical Information System, 1280 kW h electricity can be used in a year in the examined country with a 1 kWp photovoltaic system feeding back to the grid. These data are based on monthly measured climatic readings [1–4]. There are constant endeavours to exploit renewable energy sources and the amount of energy produced from these sources increases on a yearly basis, in parallel with the energy demand of the population. Of these sources, solar energy is available to the greatest extent and it is clean, inexhaustible and sustainable [5,6]. The yearly amount of solar energy reaching the surface of the Earth is 120,000 TW, which is more than the yearly energy need of the global population (around 15 TW) [7]. There has been a rapid increase recently in energy production with photovoltaic modules, mainly due to quick technological development, decreasing costs and government support being introduced in numerous countries. This phenomenon is represented by the following data: according to the Renewables 2015 Global Status Report, the total installed capacity of photovoltaic systems was 23 GW in 2009. This capacity increased to 177 GW by 2014, which represents more than a sevenfold increase [8–10]. As a result of further installation, 53–57 GW extra capacity is expected in 2015. Currently, it is one of the greatest challenges find a way to exploit this rather promising energy source to the greatest possible extent and to develop a solution to effectively store this energy [11]. In general, it can be stated that the currently available crystalline photovoltaic modules are capable of transforming 20% of solar radiation into electricity. As a result, the significant amount of solar radiation is transformed into heat without utilisation, which deteriorates the efficiency of photovoltaic modules and this can be reduced with cooling [12] Experiments of continuous flow cooling systems resulted in a relatively slight increase in efficiency, while evaporation loss was also observed, along with the need to mobilise a significant amount of water due to recirculation. For this reason, this research focused on cooling methods which either make use of the cooling energy of evaporation or those which perform cooling in a closed loop system without any water loss. In both cases, the obtained findings were evaluated against non-cooled monocrystalline photovoltaic modules, both from technical and economic aspects. During economic calculations, public and small scale plant photovoltaic module systems were evaluated on the basis of Hungarian consumer prices. Further measurements and calculations were also performed in relation to how air temperature and the impact of the sun affect the operation of non-cooled systems, in order to determine the specific air temperature at which photovoltaic modules are best for cooling."
published_063,"Computational and mathematical modelling of biological data is not a new approach in plant science. For example, many multi-component modelling strategies are used in agriculture to support decision-making and predict crop yields, and evolution and ecology are two fields where modelling has had a significant impact for many decades. However, it is only relatively recently that modelling approaches have been used to understand molecular signaling pathways underlying plant defence responses and in many cases the significance, or accuracy, of insights from the models have not been tested. In this review we highlight recent advances in modelling of plant defence signaling and the types of questions that can be addressed (Table 1). Where appropriate we have outlined techniques that have been applied to other areas of plant science, typically abiotic stress responses. For greater detail on specific modelling methods please see alternative reviews [1,2]."
published_064,"Oxygen limitation occurs when the oxygen uptake rate (OUR), which depends on the specific oxygen uptake rate and the biomass concentration, exceeds the oxygen transfer rate (OTR) (Eq. (1)). This situation was previously difficult to avoid in shaken flasks, since monitoring in small-scale bioreactors required the development of suitable technologies [3,8]. Nowadays, this limitation has been overcame, and technologies like the RAMOS device for OTR online recording [14,15] or the optical oxygen probes for DOT in-situ measurements [6,9,16], have proven its worth for evaluating the role of oxygen availability during screening experiments, along with a deeper study of the mass transfer phenomena and successful scale-up strategies. New mixing technologies should be evaluated as an alternative to solve oxygen transfer limitations in orbital shaken flasks. The resonant acoustic mixing (RAM) is a non-contact mixing technology, that induces a low-frequency acoustic field to facilitate mixing through mechanical resonance [17,18]. This newly introduced technology has potential for mixing multiphase systems, as was recently evaluated for powders homogenization using low concentration of active pharmaceutical ingredient (acetaminophen, 3% w/w) and lubricant (magnesium stearate, 1% w/w). A highly efficient mixing performance was achieved by taking less mixing time to reach better blend homogeneity when compared to other batch blenders [18]. Introducing RAM to biotechnology begins with the RAMbio system (RAMbio®, Applikon® Biotechnology, Foster City CA, USA) which was designed to mix microbial cultures grown in flasks (Fig. 1), and provide them with plenty of oxygen when used along with the standard flexible silicone plugs (Oxy-Pump® stoppers by Applikon Biotechnology®). Those stoppers are capable of keeping the media inside the flask, with a membrane at the top through which air and gases are filtered (Fig. 2A). Hence, the mixing and aeration mechanisms differ with those from the orbital mixing (Fig. 1). Scientific information regarding the performance of the RAMbio system is scarce, and just a few technical notes have been published [17]. Performance of Escherichia coli K12 expressing Green Fluorescent Protein (GFP) was compared in shaken flasks (at both 20% and 40% medium volume/nominal volume) using an orbital shaker mixer (and Whatman Bugstopper closures with a shaking diameter of 1.9 cm) against a RAMbio acoustic mixer (and Oxypump® stoppers) for batch growth and protein production in rich media. Moreover, this technical report claimed that the RAM mixer at 20 g and the orbital mixer at 400 rpm are equivalent, lacking any engineered criteria [17]. At the end of cultures, the use of the RAM mixer resulted in an improved growth and biomass production (measured by optical density at 600 nm), as well as in an increased protein production (monitored by flow cytometry), versus traditional orbital shaken flask cultures. The aim of this paper is to quantitatively compare the mass transfer phenomena in shaken flasks, particularly the oxygen transfer coefficient (kLa), in both mixers, the resonant acoustic mixer and the orbital mixer, as a function of the main operational conditions, including the shaking frequency, the flask geometry and the filling volume. Moreover, to compare the two mixing approaches based on the same initial kLa, E. coli cultures were carried out to evaluate growth kinetics, glucose consumption, dissolved oxygen tension (DOT), and acetate production profiles. In the present work, we introduce the first approximation to the understanding of the oxygen transfer phenomena in resonant acoustic shaken flasks."
published_065,"Navigation, in its broadest sense, is to travel safely, efficiently and independently from one point to another (Burns, 1999; May, Ross, & Osman, 2005). It may involve two distinct but often interrelated themes: pre-trip planning and way-finding. Pre-trip planning encompasses the navigational preparations many (but not all) people make before undertaking a journey, whereas way-finding can be defined as the on-trip decision-making process the driver is required to undertake to reach their destination (Burns, 1999). Navigation involves multi-level cognitive processing and attracts much theoretical and practical interest (Ishikawa, Fujiwara, Imai, & Okabe, 2008). Older adults often experience difficulty in navigating because age-related decline in cognitive, perceptual and motor skills (including spatial learning and memory) can impede the mental representation of a spatial environment (Forlizzi, Barley, & Seder, 2010; Kim, Hong, Li, Forlizzi, & Dey, 2012; May et al., 2005; Yamamoto & DeGirolamo, 2012). Therefore, older adults may face challenges in navigating a journey from memory. Moreover, they will often want to avoid certain situations like heavy traffic, driving at night or driving on unfamiliar routes – a process called self-regulation (Burns, 1999; Charlton, Oxley, Fildes, Oxley, & Newstead, 2003; Rabbitt, Carmichael, Jones, & Holland, 1996). Older drivers often plan alternative ways of achieving their goal or adapt their behaviour in order to reach a required destination (Banister & Bowling, 2004; Metz, 2000, 2003). Bryden, Charlton, Oxley, and Lowndes (2013) conducted a large-scale survey with 534 drivers aged 65 and over to examine their way-finding strategies and navigational behaviour. From this sample, 60% of the participants reported difficulties with way-finding. When reviewing strategies for an unfamiliar journey the researchers found that using a map while driving was the most popular (62%), reading a map while on the roadside was second (55%) and reliance on memory (38%) third. 33% of older drivers would create written instructions or a drawn map to assist them. Only 10% would use an in-vehicle navigation system (IVNS). Moreover, this research found that when older drivers have difficulties with way-finding they are likely to ask a passenger to assist them in the navigation task."
published_066,"Noble metal nanostructures, mainly nanoporous gold (NPG) containing three-dimensional network structures, currently have attracted significant attention in the field of electrocatalysis and chemical sensors [1,2]. Hitherto, such NPG structures with increased active surface area have been prepared using de-alloying [3,4], templates [5] and anodization [6,7] methods. More recently, some authors have demonstrated that small amounts of Pt presented over the NPG surface increased the electrocatalytic activity, however this required a two-steps method, where NPG was first prepared and subsequently Pt was deposited over the NPG surface [8–11]. We have recently introduced a simple one-step method to fabricate self-supported NPG films (NPGF), in which a Au electrode was held at a positive potential (2.0 V) for subsequent forward scan to build a stable oxide film and further reverse scan to form NPG structures [12]. In this report, a similar approach was adopted in which the Pt precursor (K2PtCl6) was added after the Au oxide reduction during the reverse scan to fabricate the Pt-NPG structure. In this process, electro-generated oxygen bubbles in the oxygen evolution region can act as a template for building such high surface area porous structures. To the best of our knowledge, this is the first report on the one-step fabrication of Pt-NPGF by single cyclic voltammetry in absence of binary alloys, chemical reducing agents, and stabilizers. It is well reported that platinum is the best catalyst for direct methanol fuel cells (DMFCs), but owing to the expensiveness, limited availability and loss of activity due to CO-poisoning, the development of new materials to retain the activity with a reduced amount of Pt for methanol oxidation is always desired [13,14]. In this paper we show our efforts to use a Pt-NPGF electrode with enhanced electrocatalytic activity and stability towards methanol oxidation. The usefulness of such proposed platform was also demonstrated by using a Pt-NPGF microelectrode for detection of H2O2 as a model analyte, because of the importance of such compound in physiological and pathological functions as a signaling molecule to control diverse biological processes [11]. Thus, H2O2 detection using microelectrodes is particularly essential due to unique properties such as high mass transport, low IR-drop and ease of miniaturization. Sensors with such features have been used by our research group for H2O2 monitoring [15,16], but the presence of dissolved O2 in the samples may be a problem. Denuault et al. have reported the fabrication of mesoporous Pt microelectrodes with high surface area, and such sensors revealed excellent amperometric responses for H2O2 detection [17]. Hence, here an attempt has been made to modify a Au microelectrode surface through the preparation of a Pt-NPG film for H2O2 detection and we will demonstrate that such device can be used for both reduction and oxidation of H2O2, hence problems with dissolved O2 are alleviated."
published_067,"Accurate DNA replication is essential prior to cell division if daughter cells are to inherit a genome free from mutation. The proliferating cell nuclear antigen (PCNA) has a central role during DNA replication, acting to recruit enzymes to the DNA replication fork, and to modify enzymatic activity and processivity. At replication sites PCNA recruits proteins required throughout the complex processes of chromosomal duplication, including (but not limited to): DNA polymerase delta (for lagging strand synthesis) [1], DNA ligase 1 (Lig1) and Flap endonuclease 1 (Fen1) (for Okazaki fragment maturation) [2–4], RNaseH2B (for ribonucleotide removal) [5–7], polymerase eta (Polη) (for translesion synthesis) [8], DNA methyltransferase 1 (DNMT1) (for DNA methylation maintenance) [9], and chromatin assembly factor 1 (for chromatin assembly) [10,11]. PCNA also regulates entry into S-phase via interactions with the cell cycle regulator p21 [12] and the pre-replication complex component Cdt1 [13]. These PCNA partners all utilise a similar peptide motif to associate with PCNA, the PCNA-interacting-protein (PIP) box. This short motif interacts via a combination of charge-pair and hydrophobic interactions with the interdomain connecting loop (IDCL) of PCNA [14,15]. Proteins that bind PCNA in this manner therefore compete for the same interaction surface, perhaps to enable dynamic associations essential for progressing through the multiple stages of chromosomal replication [16]. It has often been postulated that PCNA might coordinate DNA replication, the trimeric nature of PCNA lending itself to a “toolbelt model” where, by binding multiple proteins sequentially, PCNA ensures appropriate selection of enzymes at the replication fork [17]. However, this model has recently been challenged by work showing that PCNA with only a single interaction surface is still functional [18]. As well as its essential role in DNA replication, PCNA is also required for DNA repair. PCNA is involved in the processes of nucleotide excision repair (NER), long-patch base excision repair (BER) and mismatch repair (MMR) [16]. During these processes PCNA interacts directly with repair proteins, including (but not limited to): xeroderma pigmentosum A and G (XPA, XPG) (for NER) [19,20], DNA glycosylases (UNG2 and MPG), endonucleases (APE1 and APE2) and polymerase beta [21–25] (for BER), and MutS homologs 3 and 6 (for MMR) [26]. It is also possible that PCNA is required for DNA repair synthesis during homologous recombination [27]. As PCNA is a crucial component of important pathways for DNA metabolism, it is not surprising that the encoding gene is essential in yeast and mice [28–30]. The gene is highly conserved from yeast to humans; S. cerevisiae, S. pombe and M. musculus PCNAs are 35%, 51% and 97% identical to the human protein, respectively [EMBOSS Needle]. Site-specific mutations of the S. cerevisiae protein result in a variety of phenotypes, including cold sensitivity, sensitivity to DNA damaging agents and alterations to telomere position effects [31–34]. In mice the only characterised PCNA variant is the site directed mutation of lysine-164 to arginine, which results in infertility and in alterations to the somatic hypermutation spectrum due to the requirement for ubiquitination on PCNA Lys164 for the recruitment of Polη [35]. The PCNA protein is not invariant in the human population, but its variation is very low. There are only seven missense coding SNPs reported in the 1000 genomes browser (rs140522967, Ser223Pro; rs369958038, Ser228Ile; rs376351202, Met139Val; rs141842220, Ala67Thr; rs144468297, Asn65Thr; rs1050525, Ser39Arg; and rs375496467, Val15Leu) all with minor allele frequencies of less than 0.01 (where reported). Of these only one very rare allele (rs369958038, S228I) is reported pathogenic in the homozygous state [36]. We previously described four individuals from the Ohio Amish population who are homozygous for this S228I sequence alteration and affected by PCNA-associated DNA repair disorder (PARD), characterised by short stature, hearing loss, premature aging, telangiectasia, neurodegeneration and photosensitivity [36]. A further PARD affected Amish individual from Wisconsin homozygous for the same S228I founder mutation has since been identified, she presented aged 4 years with short stature, sun sensitivity, progressive gait instability and hearing concerns. On examination, there was no evidence of ocular or cutaneous telangiectasia, which appear to be a later manifestation of the disease. We previously showed that PCNAS228I protein has altered binding to a number of client proteins, in particular XPG, Lig1 and Fen1, and that cells from PARD affected individuals were more sensitive to UV damage [36]. We here show data that PCNAS228I also causes repair independent consequences in cells from PARD affected inidividuals and present in-depth characterisation of the protein binding capability of PCNAS228I, showing that the effect on binding varies significantly across a range of PCNA interactors, dependent on the sequence of the PIP-box. These consequences for cellular functions will shed light on the complex pathology of this disorder."
published_068,"The HMM rests on the architectural claim that the human brain is a hierarchically organised system of neurocognitive mechanisms that interact in a dynamic, reciprocal fashion. Under this view, the lowest or most peripheral levels of the cortical hierarchy comprise relatively segregated, highly specialised neural mechanisms responsible for sensorimotor processing (‘domain-specific’ systems), while its higher, deeper or more central layers consist of developmentally plastic, highly integrated (‘domain-general’) mechanisms. The latter are widely distributed subsystems that respond flexibly to input received from multiple lower levels, feed information downstream for further processing, and underlie the executive cognitive functions unique to humans [1]. There are two important distinctions here. First, although there are many interpretations of the neural hierarchy, here we refer to a fractal or nested modular hierarchy, which entails the repeated encapsulation of smaller (neuronal) elements in larger ones across different spatial, temporal, topological, and functional neural scales (i.e., ‘modules within modules’; [17–19]). The second is that neurocognitive mechanism is defined as a neural subsystem that operates at any spatiotemporal scale, ranging from a particular neuronal population through to macroscopic brain regions. Such mechanisms involve a dynamic, bidirectional relationship between specialised functional processing mediated by dense, short-range connections intrinsic to that scale (i.e., its local integration); and their global (functional) integration with other neural subsystems via relatively sparse, long-range (e.g., extrinsic cortico-cortical) connections [20]).2 Accordingly, the HMM implies a complementary relationship between functional segregation and integration: all neurocognitive mechanisms involve a sub-population of cells that have a common, specialised function, but they are also functionally integrated because of their distal connections with other subsystems [20,30]. At the same time, it also recognises that some neural subsystems will be more integrated than others. The type of neural architecture described here echoes a growing consensus that human cognition and behaviour emerge from the integrated dynamics of hierarchical networks of (functionally segregated and differentially integrated) neural processing mechanisms [20,25,31–40]. There is nothing controversial about this claim. The idea that the brain exhibits a hierarchical structure that progresses from relatively ‘domain-specific’ systems through to highly integrated, ‘domain-general’ regions is far from new, having long been recognised by influential perspectives such as global neuronal workspace theory [41,42] and the dual process theory of reasoning [43,44]. More recently, sophisticated structural and functional imaging studies in network neuroscience have furnished extensive evidence that human cortical networks exhibit a nested, fractal-like structure; extending from cellular microcircuits in cortical columns at the lowest level, to cortical areas at intermediate levels, through to distributed clusters of highly interconnected brain regions at the global level [45–47]; see Fig. 1a.3 Notably, a hierarchical neural structure is also central to the theory of predictive processing, an increasingly popular scheme that describes the brain as a Bayesian ‘inference machine’ that minimises discrepancies between incoming sensory inputs and top-town predictions (see Fig. 1b). Since the literature on the brain's hierarchical organisation has already been reviewed elsewhere (e.g., [18,21,31,33,49,50]), we will not dwell on it here. Instead, we will now concentrate on the more contentious issue of why the brain is structured in this way."
published_069,"Osteoarthritis is the predominant form of arthritis and remains a leading cause of disability [1]. Arthritic joints are characterized by lesions in hyaline cartilage that result in severe pain, loss of motion and eventually require surgical intervention [2]. Cartilage tissue engineering has emerged as a treatment method for articular cartilage lesions and this approach has employed a variety of scaffold materials which act as a carrier of and delivery vehicle for autologous chondrocytes and/or progenitor cells capable of cartilage formation [3–5]. Such strategies incorporate scaffold materials that range from autologous periosteal membranes to highly fibrillar materials derived from both biopolymers and degradable polymers including collagen, hyaluronic acid and aliphatic polyesters [6,7]. The materials used are predominantly isotropic in regards to functional characteristics including mechanical performance, cellular distribution, and cellular interaction with the matrix. Articular cartilage is a highly organized tissue that provides a low-friction and wear-resistant bearing surface and exhibits anisotropic mechanical properties as a result of depth-dependent differences in the density and structural arrangement of its extracellular matrix (ECM). Articular cartilage is comprised of four main zones: the superficial, middle, deep and calcified cartilage zone [8,9]. Each zone varies in regards to biochemical content, morphology and biomechanical function, with increased proteoglycan concentration and stiffness with depth, while conversely cellular density decreases; the total collagen concentration is unchanged with depth [8,10]. From a functional perspective, the cartilage portion of the osteochondral gradient can be simplified into two main regions: the superficial zone which exhibits a high tensile strength and low coefficient of friction to maintain smooth articulation; and a dense ECM region rich in proteoglycan molecules which contribute to the compressive mechanical properties by producing a high osmotic pressure within the tissue [5,11]. The superficial, middle and deep zones possess distinct gene and microRNA expression profiles, as thoroughly summarized by Grogan et al. [12]. When chondrocytes are expanded in vitro to obtain sufficient cell quantities for implantation, they dedifferentiate, losing their characteristic gene expression profile of collagen II and aggrecan by over an order of magnitude at the first passage, and begin expressing high levels of collagen I [13]. Dedifferentiation can be reversed by culturing chondrocytes on 3-D matrices, supplementation of the media with transforming growth factor (TGF)-β3 and dexamethasone, and physiological mechanical loading [14]. The redifferentiation protocols have been studied by gene expression as well as microRNA analysis, and in the case of microRNA expression can be almost entirely restored with the proper physical and chemical cues [13]. The use of zonally derived chondrocyte populations has merits in creating zonal constructs for implantation; however, the lack of efficient automated cell-sorting techniques limits their clinical applicability, leaving some to question whether zonal properties can be derived from an expanded redifferentiated chondrocyte population by simpler methods [15]. Beyond zonal populations, the use of purified progenitors isolated from cartilage using the classic stage-specific embryonic antigen-4 (SSEA-4) marker for undifferentiated stem cells has also proven of insufficient difference to warrant the additional complexity [16]. For this reason, a mixed population of passage-2 chondrocytes, cultured in a redifferentiation medium, remains the most widely investigated cell population for clinically applicable cartilage tissue engineering. Current strategies for scaffold fabrication which have been shown to support cartilage formation include electrospinning of fibrous materials, particulate-leaching, gas-foaming and phase separation [9,17–21]. Each of these techniques offer specific advantages, yet none can fully encompass all requirements for optimal scaffold performance including zonal organization, adequate mechanical properties, full cellular ingress and physiological levels of ECM formation. Electrospinning is a facile technique which produces flexible, densely packed fibre networks with tuneable mechanical, physical and biological properties based on polymer selection, fibre size, network orientation and overall thickness [22,23]. Previous research has shown that aligned electrospun fibres are able to mimic the highly oriented morphological and tensile properties of the superficial zone of articular cartilage, and have been shown to support in vitro cartilage formation and chondrogenesis [18]. Nevertheless, electrospun scaffolds are generally limited in regards to dimensional thickness and lack of cellular infiltration due to the dense packing of fibres [22,24]. One exception is melt electrospinning, which can completely eliminate these issues, but concedes fine (<10 μm) fibre sizes for control, forming scaffolds intermediate between solution electrospinning and fused deposition modelling [25]. Particulate-templated scaffolds offer their own unique advantages including greater control of pore size, interconnectivity, geometry and overall scaffold porosity based on the size, shape and loading of the particulate [20]. Electrospinning directly onto a foam substrate produces a bilayered scaffold that possesses the tensile strength and fibrous morphology of the electrospun membrane and the high porosity of the underlying foam. The two-zone approach has been previously applied to vascular and bladder scaffolds which utilize the layered design to mimic the structural variations within their respective tissues [26–28]. As previously stated, articular cartilage also possesses a depth-dependent multi-zone morphology, with varied mechanical properties and function. A scaffold that mimics the structure of the native cartilage may offer a superior template for in vitro cartilage formation and ECM organization. Some approaches to multi-layer articular cartilage scaffolds include multi-layer hydrogels [29], multilayer electrospun fibres [18] and anisotropic fused deposition modelling [9]. This investigation is the first to study a zonally organized articular cartilage scaffold with an electrospun fibre articulating surface deposited onto a particulate-leached foam. Herein, we generate bilayered poly(ε-caprolactone) (PCL) scaffolds comprised of an aligned fibre zone, which mimics the morphology and mechanics of the superficial zone of articular cartilage, laminated to a bulk porous particulate-templated scaffold which allows for full cellular infiltration and extensive ECM deposition. We hypothesized that the addition of aligned electrospun fibres would increase tensile properties, reduce surface roughness, and provide morphological similarities to native articular cartilage. Additionally, we varied the particulate size (0.03 vs. 1 mm3) in the particulate-templated scaffolds and hypothesized that smaller pores would support in vitro cartilage formation by bovine chondrocytes. Our results demonstrate that the electrostatic deposition of fibres with aligned orientation onto particulate-templated foams produces scaffolds which mimic key mechanical and functional characteristics of native cartilage and are able to support in vitro cartilage formation, indicating their promise in regenerative medicine strategies to repair articular cartilage lesions."
published_070,"The cochlear implant (CI) device is widely accepted and has been considered one of the most important therapeutic options for patients with severe and/or profound bilateral sensorineural hearing loss, in cases who did not achieve satisfactory auditory perception benefits with the use of individual sound amplification device (ISAD). The CIs have been indicated at progressively younger ages due to advances in early audiological diagnosis and new technologies in the CI devices.1 Over the last decades, bilateral CI surgery began to be performed, a procedure that can be performed simultaneously or sequentially. The simultaneous technique is used when the patient receives the two internal components in a single surgical procedure, and the sequential one, when the patient receives the two internal components in different surgical procedures. Patients who require a CI have been increasingly choosing the bilateral procedure. Some studies have shown that these patients benefit from improved speech in the perception with noise2 and improved sound location.3,4 The process known as “Programming” or “Mapping” of the CI speech processor is performed at regular intervals postoperatively. The Mapping process aims to determine the appropriate dynamic range of electrical stimulation for each electrode channel. The dynamic range is the difference between the detection perception threshold (T-level) and loudness – maximum comfort (level C).5 The measurement of the electrode impedance telemetry can provide an indication of the electrode interface status in the tissues, as well as the appropriate electrode function. Significant changes in these measures can be indicative of changes in the surrounding tissue and/or changes in electrode function. Initial changes in electrode impedance can be expected due to physical changes in the electrode-tissue interface.6 The CI has the capacity to measure the electrically evoked compound action potential (ECAP) of the auditory nerve. The system applies an electrical pulse to a specific intracochlear electrode and the evoked neural response is recorded in an adjacent electrode. A measure called Neural Response Telemetry (NRT) is used to assess this potential. The system elicits a valid neural response and robust recordings. These responses are recorded and returned to the programming interface system for clinical analysis.7,8 The ECAP provides a relatively direct measurement of the auditory nerve response after electrical stimulation and it is measured in current units (CUs).9 The ECAP waveform typically consists of an initial negative peak followed by a positive peak, called N1 and P1, respectively.7,8 The NRT threshold (T-NRT) is defined as the smallest amount of electric current that can evoke these physiological responses. Studies have shown that the T-NRT measured intraoperatively or at postoperative intervals can be correlated with the psychophysical detection of the threshold (T level) and the maximum level of comfort (C level) in patients with CIs. The amplitude of the response (measured between N1 and P1) varies with increasing stimulus intensity and it is measured in millivolts (μV).10–12 Considering the abovementioned facts, this study aims to analyze the ECAP through NRT in children who received bilateral cochlear implants. The ECAP will be analyzed in relation to the T-NRT visual threshold and the amplitude of N1-P1 peak during the first year of CI use."
published_071,"Because of the similarity in their natural history, cancers that affect the oral cavity, oropharynx, larynx, and hypopharynx are usually described as “squamous cell carcinoma of the upper aerodigestive tract (UADT)”, “head and neck cancer,” or, as reported in lay language as “cancer of the mouth and throat.” Even though they are different diseases and affect different anatomical regions, considering them together may be relevant in an epidemiological context and for the development of appropriate public health actions.1 UADT carcinoma is the sixth most prevalent type of cancer in the world with 9.2% of cases, and is globally responsible for 4.6% of cancer-related deaths.2 In Brazil, for the 2018/19 period, the National Cancer Institute (INCA)3 estimated 22,370 new cases of UADT carcinoma (C00-10, C32), of which 17,590 cases were found in males and 4780 in females, corresponding respectively to the 3rd most prevalent cancer in males and the 12th in females. Still according to INCA, the cases of UADT carcinoma in both genders account for an estimated risk of 21.51 new cases per 100,000 inhabitants. This is a serious public health problem, not only because of its incidence, but because the cancer stage is advanced in 75% to 80% of the cases at the time of diagnosis in Brazil, with a mean mortality rate of 46% in 5 years, which has remained unchanged in the last two to three decades.4–6 The main risk factors7 for UADT carcinoma include the smoking habit, alcohol consumption, sun exposure for lip cancer and Human Papilloma Virus (HPV), especially in the oropharynx. This is an asymptomatic or oligosymptomatic disease in its initial course, often presenting as the main pre-cancerous lesions (PCLs) or precursor lesions erythroplakia, erythroleukoplakia and leukoplakia.7,8 Several barriers to an early diagnosis have been described, which seem to be related to the population's lack of knowledge about risk factors, poor social support, low levels of schooling and per capita income, demographic/geographical aspects, and poor access to health services.9,10 Considering the Brazilian reality, there is no possibility to act on all the above mentioned factors, and for this reason it has been suggested that the early finding of a Pre-Cancerous Lesion (PCL) in the Upper Aerodigestive Tract (UADT) is the most effective form of successful prevention and clinical intervention,11 which would be possible with prevention campaigns. However, prevention campaigns against cancer of the oral cavity and larynx initiated in Brazil are mostly sporadic, carried out in individuals with/without risk factors, with ages outside the age group, with higher prevalence due to the broad appeal in the community and inattention to pre-cancerous lesions. In part, the target population of these campaigns do not exclusively consist of the at-risk group, partly explaining the low number of cancer detection cases.12 However, one should not disregard the element of public awareness and information against the several types of cancer, but a better understanding of the biological and epidemiological processes for UADT carcinoma prevention is necessary. Unlike conventional prevention campaigns, one can attract the individuals who fit the high-risk profile, inviting them to medical care through an active search.13 This strategy was used in Kerala (India) in a campaign against cancer of the oral cavity, where screening results were considered satisfactory.14 Subsequently, a strategy focused on the visualization and therapeutic approach of PCLs may be feasible in reducing UADT carcinoma incidence and mortality.15 Consistent with the literature, equipped propaedeutics through video-laryngoscopy (VL) or Video-nasolaryngoscopy (VNL) is a safe and effective technique to visualize lesions or alterations in the UADT epithelium.16"
published_072,"Salix L. is the largest genus of Salicaceae family comprising ca. 300–450 species. These deciduous tree and shrub species are generally native to the cooler regions of the Northern Hemisphere, but they have been introduced worldwide (Julkunen-Tiitto and Virjamo, 2016; Lauron-Moreau et al., 2015), due to their useful features, such as fast growth and pleasing appearance, as well as considerable value as medicines and wood products, including biomass for energy (Isebrands and Richardson, 2014). Salicinoids are signature metabolites for Salicaceae family (Boeckler et al., 2011), and are well-known for their role in the development of aspirin (Desborough and Keeling, 2017). However, many other phenolic glycosides (e.g. salidroside, triandrin and picein) are also abundant in Salix sp., along with several other classes of common plant secondary metabolites, such as cinnamic acid derivatives, condensed tannins, proanthocyanidins and flavonoids (Nyman and Julkunen-Tiitto, 2005; Wiesneth et al., 2018). In our continuing studies on novel phytochemistry in the Salicaceae, in particular those genotypes contained in the 1500+ National Willow Collection (NWC), maintained as a short-rotation coppice plantation at Rothamsted Research, we recently reported on the occurrence of salicin-7-sulfate, a new salicinoid of potential pharmacological interest (Noleto-Dias et al., 2018). Here, we extend the study of sulfated metabolites in Salix and now report the isolation, structure determination and radical scavenging activity of four new sulfated flavonoids from the polar extract of a willow hybrid S. × alberti L. (S. integra Thunb. × S. suchowensis W.C. Cheng ex G.Zhu)."
published_073,"Heart failure (HF) is a complex clinical syndrome and not a disease. It prevents the heart from fulfilling the circulatory demands of the body, since it impairs the ability of the ventricle to fill or eject blood. It is characterized by symptoms, such as breathlessness, ankle swelling and fatigue that may be accompanied by signs, for example elevated jugular venous pressure, pulmonary crackles, and peripheral edema, caused by structural and/or functional cardiac or non-cardiac abnormalities. HF is a serious condition associated with high morbidity and mortality rates. According to the European Society of Cardiology (ESC), 26 million adults globally are diagnosed with HF, while 3.6 million are newly diagnosed every year. 17–45% of the patients suffering from HF die within the first year and the remaining die within 5 years. The related to HF management costs are approximately 1–2% of all healthcare expenditure, with most of them linked with recurrent hospital admissions [1–3]. The increased prevalence, the escalated healthcare costs, the repeated hospitalizations, the reduced quality of life (QoL) and the early mortality have transformed HF to an epidemic in Europe and worldwide and highlight the need for early diagnosis (detection of the presence of HF and estimation of its severity) and effective treatment. In clinical practice, medical diagnosis, including carefully history and physical examination, is supported by ancillary tests, such as blood tests, chest radiography, electrocardiography and echocardiography [4]. The combination of data produced by the above procedure of diagnosis resulted in the formulation of several criteria (e.g. Framingham, Boston, the Gothenburg and the ESC criteria) determining the presence of HF [5]. Once the diagnosis of HF is established, the experts classify the severity of HF using either the New York Heart Association (NYHA) or the American College of Cardiology/American Heart Association (ACC/AHA) Guidelines classification systems, since this classification allows them to determine the most appropriate treatment (medication treatment, guidelines regarding nutrition and physical activity exercising) to be followed [6]. Although there is a significant progress in understanding the complex pathophysiology of HF, the quantity and complexity of data and information to be analyzed and managed convert the accurate and efficient diagnosis of HF and the assessment of therapeutic regimens to quite challenging and complicated tasks. Those factors, in combination with the positive effects of early diagnosis of HF (which allows experts to design an effective and possibly successful treatment plan, prevents condition worsening, affects positively the patient's health, improves patient's QoL and contributes to decrease of medical costs) are the reasons behind the enormous increase of the application of machine learning techniques to analyze, predict and classify medical data. Classification methods are among the data mining techniques that have gained the interest of research groups. Accurate classification of disease stage or etiology or subtypes allows treatments and interventions to be delivered in an efficient and targeted way and permits assessment of the patient's progress. Focusing on HF, different data mining techniques have been employed to differentiate the patients with HF from controls, to recognize the different HF subtypes (e.g. HF with reduced ejection fraction, HF with preserved ejection fraction) and to estimate the severity of HF (NYHA class) (Fig. 1). Additionally, data mining techniques can be advantageous even if HF is being diagnosed at a late stage, where the therapeutic benefits of interventions and the prospect of survival are limited, since they allow the timely prediction of mortality, morbidity and risk of readmission. Data recorded in the subjects' health record, expressing demographic information, clinical history information, presenting symptoms, physical examination results, laboratory data, electrocardiogram (ECG) analysis results, are employed. An extended review of the studies reported in the literature addressing the above mentioned issues (HF detection, severity estimation, prediction of adverse events) through the utilization of machine learning techniques is presented in this paper. The systematic literature review was based on sources like i) PubMeD, ii) Scopus, iii) ScienceDirect, iv) Google Scholar, v) Web of Science (WoS) using as keywords the phrases “detection of HF”, “severity estimation of HF”, “HF subtypes classification”, “prediction of HF destabilizations”, “prediction of HF relapses”, “prediction of HF mortality”, “prediction of HF re-hospitalizations”. The studies reported in the literature were selected based on the following criteria: i) focus on heart failure and no any other heart disease, ii) are written in English language, iii) are published from 2000 (inclusive) until present, iv) cover different geographical locations, v) are employing machine learning techniques, vi) employ Electronic Health Records, published databases, observational, trial, etc. for the development and validation, vii) provide information regarding the evaluation measures and the validation method that was followed and, viii) the response feature is either differentiation of subjects to normal and HF or differentiation of subjects to different HF subtypes or estimation of the severity of HF or estimation of the destabilization or estimation of re-admission or estimation of mortality. There is no restriction regarding the time frame of the prediction. Furthermore, studies addressing both aspects of HF management (e.g. detection and severity estimation of HF) were also included in this review. Studies not fulfilling more than one of the above mentioned criteria were excluded."
published_074,"Plants produce a variety of low molecular weight organic compounds that are usually ramified into two large classes: primary and secondary metabolites. Primary metabolites are essential for plant growth and development, while secondary metabolites act as defense molecules and protect plants in various adverse conditions. Many secondary metabolites have high pharmaceutical value and have been used for the treatment of wide range of diseases [1]. For example, vinblastine and nicotine are used for treatment of tumor [2] and asthma [3] diseases, respectively. Artemisinin, another essential secondary metabolite, is extensively employed as antimalarial drug [4]. Furthermore, two additional pharmaceutically important secondary metabolites, taxol and ginsenosides, are generally used for the treatment of breast, ovarian and non-small cell lung cancers [5, 6]. Because of their beneficial effects to health, many researchers have been focusing on the comprehensive study of regulation and production of secondary metabolites. The biosynthesis and proper accumulation of secondary metabolites are strictly controlled in a spatial and temporal manner and influenced by a number of biotic and abiotic factors. The spatiotemporal transcriptional regulation of metabolic pathways is controlled by a complex network involving many regulatory proteins known as transcription factors (TFs). TFs are sequence-specific DNA-binding proteins that interact with the regulatory regions of the target genes and modulate the rate of transcriptional initiation by RNA polymerase [7]. Many TFs have been characterized for their roles in regulating biosynthetic pathways at the transcriptional level. TFs include a wide number of proteins that initiate and regulate the transcription of several genes. These proteins regulate gene transcription depending on tissue type and in response to external and internal signals [8]. Jasmonic acid (JA) that presents in the scent of jasmine flowers (Jasminum grandiflorum) is known as an essential signaling molecule which can coordinate a lot of cellular activities [9]. JA and its cyclic precursors and derivatives (such as methyl jasmonate, MeJA) are collectively referred to as jasmonates (JAs). JAs act as ubiquitous and conserved elicitors for the production of secondary metabolites across the plant kingdom, from gymnosperms to angiosperms [10, 11]. The promising depiction of transcriptional regulation of secondary metabolism suggests that, in response to phytohormones (JAs), TFs form dynamic regulatory networks that fine-tune the timing, amplitude and tissue-specific expression of pathway genes and the subsequent accumulation of secondary metabolites [8]. Here, we give an updated review on JA-responsive signaling pathway and speculate on JA-mediated transcriptional regulation of secondary metabolism (e.g., vinblastine, nicotine, artemisinin, taxol and ginsenoside) in a range of medicinal plant species."
published_075,"Hepatic fibrosis is the outcome of chronic liver injury in response to various etiologies. It is characterized by excess accumulation of abnormal extracellular matrix (ECM) and disruption of hepatic architecture.1,2 During liver fibrosis, ECM is converted from a collagen IV/VI-rich matrix to a collagen I/III-rich one, resulting in liver stiffness and distortion of the sinusoidal architecture. Long-term fibrosis leads to cirrhosis and subsequent liver dysfunction and hepatocellular carcinoma (HCC).3 Activation of hepatic stellate cells (HSCs) plays a critical role in liver fibrosis and is characterized by the conversion from vitamin A-storing cell into myofibroblast that expresses excess profibrogenic genes, such as alpha-smooth muscle actin (α-SMA), collagen type I alpha 1 chain (Col1a1), collagen type I alpha 2 chain (Col1a2), tissue inhibitor of metalloproteinase 1 (Timp1), and fibronectin 1 (Fn1). α-SMA is a marker for activated HSCs, and it also enhances the contraction force of HSCs by incorporating into stress fibers.4,5 The contractility of HSCs leads to increased liver stiffness and hepatic sinusoidal vasoconstriction, two key events that cause liver complications, like portal hypertension.6,7 Identification of molecules that regulate HSC activation and liver fibrogenesis may provide novel therapeutic targets. To date, whether microRNAs (miRNAs) have a role in these processes remains poorly understood. miR-125b is frequently dysregulated in human diseases, especially in cancers. Recent reports have shown that miR-125b is also implicated in different liver diseases. For instance, miR-125b attenuates paracetamol- and FAS-induced toxicity in hepatocytes and prevents acute liver failure (ALF) by directly targeting kelch-like ECH-associated protein 1 (Keap1);8 upregulation of miR-125b protects female mice from non-alcoholic fatty liver disease through repressing lipid accumulation in hepatocytes,9 and downregulation of miR-125b in cholangiocyte enhances proliferation of cholangiocytes during bile duct ligation (BDL)-induced cholestatic liver injury.10 In addition, miR-125b promotes apoptosis and inhibits proliferation and metastasis of HCC cells.11,12 Some studies have also revealed that miR-125b level is increased in mouse fibrotic livers13 and in the serum of patients with liver fibrosis.14 However, the function and its underlying mechanism of miR-125b in HSC activation and liver fibrosis remain unknown. Here, we showed that miR-125b was significantly upregulated in murine activated HSCs, but not in hepatocytes, during liver fibrogenesis. Inhibition of miR-125b dramatically suppressed the activation of murine HSCs in vitro and attenuated hepatic fibrosis in vivo. Moreover, miR-125b enhanced the activity of RhoA by directly targeting StAR-related lipid transfer (START) domain containing 13 (Stard13) expression, consequently increasing the level of α-SMA and the contractile ability of HSCs. This study reveals a promotive function of miR-125b in HSC activation and hepatic fibrosis, and this may be beneficial for developing miR-125b-based therapy for liver fibrosis."
published_076,"Climate services have long been held up as development tools with tremendous potential to reduce risk and vulnerability, and build resilience, for agrarian communities in the Global South (Dessai et al., 2009; Fröde et al., 2013; Pervin et al., 2013; USAID Global Climate Change Office, 2014). The ongoing development and refinement of climate service-based tools, such as weather based index insurance, provides opportunities to stabilize and protect people’s livelihoods by establishing new forms of safety nets, strengthening existing safety nets, and supporting the general improvement of risk management mechanisms (Carter et al., 2014; Hess and Syroka, 2005; Jensen et al., 2015; Mburu et al., 2015). For example, climate advisories and information offer opportunities to inform farmer management of climate related risk (Boyd et al., 2013; Carr et al., 2015c; Hansen, 2012; Hellmuth et al., 2011; Ingram et al., 2002), such as by supporting farmer decisions with regard to intensifying production, investing in new technologies, or taking measures to protect their households and livelihoods in the case of adverse predictions (Carr et al., 2015a,c; Hansen, 2012). The climate services community was initially dominated by a focus on delivering climate information, especially information focused on identifying and/or predicting weather and climate shocks and stresses to which particular users were exposed (for discussion, see Carr and Owusu-Daaku, 2016; Hansen et al., 2009; Millner and Washington, 2011; Roncoli, 2006; Shankar et al., 2011). Such challenges are broadly related to agroecological context (Akponikpè et al., 2010; Carr et al., 2015c; Ingram et al., 2002; Leclerc et al., 2013; Patt et al., 2005; Phillips et al., 2002; Roncoli et al., 2009; Silvestri et al., 2012) and the livelihoods of the intended users (Akponikpè et al., 2010; Bone et al., 2011; Carr et al., 2015a,c; Green and Raygorodetsky, 2010; Roncoli et al., 2001, 2002, 2009, 2011). The place- and activity-specific impacts of most weather and climate shocks and stressors has resulted in a contemporary climate services literature which recognizes that users’ vulnerability to weather- and climate-related stress is not only a function of their exposure, but also their sensitivity and adaptive capacity. As climate services have ever more closely examined the intended users of these services, it has become increasingly clear that within agroecological zones, communities, or even households, various social cleavages can shape the weather- and climate-related vulnerabilities individuals face, and therefore the particular type of climate information they need, or if they need new or additional information at all. This growing literature has drawn out the ways in which gender (Carr et al., 2016; Carr and Owusu-Daaku, 2016; Ingram et al., 2002; Patt et al., 2005; Roncoli et al., 2001; Tschakert, 2007; Tschakert et al., 2010; Ziervogel, 2004), age (Akponikpè et al., 2010; Carr et al., 2015b; Carr and Owusu-Daaku, 2016; Ingram et al., 2002; Roncoli et al., 2001; Tschakert, 2007; Waiswa et al., 2007), wealth (Akponikpè et al., 2010; Carr et al., 2015c; Roncoli et al., 2001; Tschakert et al., 2010), religion (Orlove et al., 2010; Roncoli et al., 2009, 2011), ethnicity (Carr et al., 2015c; Roncoli et al., 2009), education (Akponikpè et al., 2010; Waiswa et al., 2007), and even the identities that emerge at the intersection of two or more of these cleavages (Carr et al., 2015a,b; Carr and Owusu-Daaku, 2016; Orlove et al., 2010; Peterson et al., 2010; Roncoli et al., 2009, 2011; Ziervogel et al., 2006) produce different roles and responsibilities that result in different exposures, sensitivities, and adaptive capacities within populations, communities, and even households. This literature makes it clear that effective climate services are those directed at particular users, to address their particular vulnerabilities. We interpret this body of critical, empirically-informed work on climate services as a powerful call for more serious attention to the social science of climate services for development. If we are to design and implement climate services that truly help users address their weather- and climate-related vulnerabilities, we must understand who our users are in all of their diversity, what challenges these different users face, whether or not weather and climate information can address any of these challenges, and what information best addresses these challenges for different members of the same user population. Simply put, it is possible to design climate services that, in the context of a specific stressor for a specific group of people, work brilliantly, but when applied to a wider group of users for new purposes, fail dramatically. Our work assessing Mali's l'Agence Nationale de la Météorologie's (Mali Meteo) Agrometeorological Advisory Program serves to illustrate this important lesson, one which can inform the design and scaling-up of climate services for development, as well as the monitoring and evaluation of such services, going forward. We begin with a review of the history and function of the Agrometeorological Advisory Program. We then turn to the results of our efforts to assess the use and efficacy of this program for farmers in southern Mali. Our assessment found very low, highly gendered rates of advisory use. On the surface, these results appear deeply disheartening for those who view climate services as an important tool for adaptation, vulnerability reduction, and resilience building. However, our findings are not necessarily indicators of either a failed project or the limitations of climate services for development, as those using the advisories followed their advice very closely. Instead, this contradiction presents an interesting question: why, if this climate service is so useful to some farmers, it is not used by more? As we demonstrate through ethnographic analysis, when these results are placed back into the context of the stated goals that informed the program’s design in the early 1980s, the low, highly gendered rates of use reflect a project well-designed to address a particular problem (food insecurity linked to drought) through a particular solution (boosting yields of staple grains through better farming decisions). It is only when this program is abstracted from that specific context, and applied more broadly to questions of agricultural development, climate change adaptation, and resilience building, that these results became indicators of failure. Thus, this example of climate services for development is an extraordinarily clear example of the need to design climate services for specific goals, and when monitoring and evaluating a climate service to carefully assess which goals, and whose goals, that service is meeting."
published_077,"Laron syndrome (LS) is a rare, autosomal recessive, hereditary disorder caused by loss-of-function mutations in the growth hormone receptor (GHR) gene (https://www.omim.org/entry/600946), initially described as a syndrome of primary growth hormone (GH) resistance or insensitivity ([1]; reviewed in [2,3]). As a consequence, LS patients have low levels of insulin-like growth factor 1 (IGF1) and – due to the lack of feedback inhibition of GH secretion – high levels of GH [3]. A few hundred cases of LS have been reported world-wide, caused by a variety of GHR mutations (reviewed in [4]). Among them is an isolated, more homogeneous population of GHR deficient patients in Ecuador with only two distinct mutations of the GHR gene [5–7]. The main clinical feature is short stature. In addition, LS patients may exhibit reduced muscle strength and endurance, hypoglycemia in infancy, delayed puberty, obesity, and distinct facial features, including a protruding forehead, sunken bridge of the nose, and blue sclerae (reviewed in [3,8]). The standard treatment of LS is long-term application of recombinant IGF1, which increases growth velocity and improves adult height, but it may lead to a spectrum of side effects, in particular hypoglycemia ([9,10]; reviewed in [11]). A particularly interesting observation in LS patients is their reduced incidence of malignancies ([12,13]; reviewed in [8]). In addition, LS patients from the cohort in Ecuador have been shown to be protected against the development of type 2 diabetes despite severe obesity [7]. Although mechanistic studies have been performed in cell lines derived from LS patients and healthy controls [7,14], animal models are of pivotal importance for understanding the pathophysiology of LS in vivo. In particular, GHR-deficient mice [15] have provided new insights into the consequences of GH insensitivity for body and organ growth, body composition, endocrine and metabolic functions, and reproduction, as well as aging and life expectancy (reviewed in [16]). More recently, inducible/tissue-specific Ghr knockout (KO) mouse models have helped to define the specific roles of GHR in liver, muscle, and adipose tissue and revealed interesting differences compared with constitutive Ghr KO mice [17,18]. However, due to their small size, short life expectancy and physiological differences compared with humans, findings from mouse models may be difficult to extrapolate to the clinical situation of LS patients. In general, genetically tailored pig models are useful to bridge the gap between proof-of-concept studies in rodent models and clinical studies in patients (reviewed in [19,20]). Thus, we have developed a GHR-deficient (GHR-KO) pig model and show that it resembles important aspects of LS pathophysiology and reveals altered activation of signaling cascades in the liver."
published_078,"Bipolar disorder (BD) is characterized by episodes of mania, hypomania and depression and the wellbeing state between the episodes called euthymia (American Psychiatric Association, 2000). The lifetime prevalence is 0.6% for BD type I (BDI), 0.4% for BD type II and extends to 2.4% considering the whole spectrum of BD (Merikangas et al., 2011). BD is arguably one of the most heritable of the Axis I disorders with a heritability estimate to be as high as 93% (Kieseppa et al., 2004). One way to investigate vulnerability factors for highly heritable diseases like BD is an endophenotype based approach (Hasler et al., 2006). An endophenotype is a biomarker, which is heritable, associated with illness, state-independent, co-segregates with illness in families and is found at a higher rate in non-effected family members compared to general population (Gottesman and Gould, 2003). A wide variety of endophenotypes have been proposed in the psychiatric literature and advanced tools of neuroimaging such as structural magnetic resonance imaging (sMRI) promise to expand these further (Gottesman and Gould, 2003). sMRI in BD has been the focus of attention for >20 years with the majority of studies in the literature examining gray matter volume. So far, the most consistent findings in BD patients are reduced volume in insula (Bora et al., 2010; Ellison-Wright and Bullmore, 2010; Selvaraj et al., 2012), anterior cingulate cortex (ACC) (Bora et al., 2010; Ellison-Wright and Bullmore, 2010), inferior frontal cortex (IFC) (Houenou et al., 2011; Selvaraj et al., 2012) and enlargement in lateral ventricles (Arnone et al., 2009; Kempton et al., 2008) compared to healthy controls (HC). Gray matter volume changes in the relatives of BD patients are inconclusive but there are few replicated findings which includes increased volume in IFC, amygdala, caudate, parahippocampal cortex and decreased volume in orbitofrontal cortex (OFC), insula and cerebellum compared to HC (Nery et al., 2013). Gray matter volume is a function of two different morphometric structures called as cortical thickness (CT) and surface area (SA) (Hanford et al., 2016a; Winkler et al., 2010). While SA is reflective of number of columns, CT measures reflect the number of cells within each column (Hanford et al., 2016a). CT and SA are both heritable, globally and regionally independent and genetically uncorrelated (Sugranyes et al., 2017). It has been argued that these indices should be studied separately and may be preferred over gray matter volume in terms of brain imaging endophenotype (Winkler et al., 2010). Recent advances in neuroimaging analysis such as surface based techniques enable researchers to measure these two different cortical indices accurately (Winkler et al., 2010). To date, studies on CT in BD patients have revealed some consistent results including cortical thinning in ACC, OFC, paracingulate, dorsolateral, ventrolateral, superior frontal and superior temporal cortex (STC) compared to HC (Hanford et al., 2016a). In terms of SA in BD; although a number of studies did not find significant results (Elvsashagen et al., 2013; Fornito et al., 2008; Janssen et al., 2014; Rimol et al., 2012; Roberts et al., 2016), other studies reported increased SA in STC, precuneus, insula, temporal pole, supramarginal, postcentral and superior frontal cortex and decreased SA in frontotemporal cortices and posterior cingulate cortex in BD patients compared to HC (Abe et al., 2016; Fung et al., 2015; Hartberg et al., 2011). In addition to these small sample sized studies on cortical abnormalities in BD, a mega analysis by ENIGMA consortium, which was comprised of 2447 BD patients and 4056 HC, was conducted recently. This largest study to date showed decreased CT in pars opercularis, fusiform and rostral middle frontal cortex in BD patients compared to HC but didn't show any differences in SA between groups (Hibar et al., 2018). Regarding CT and SA in relatives of BD patients, 6 studies have been conducted. Two heritability studies revealed that cortical thickening in supramarginal cortex and rolandic operculum, SA expansion in supramarginal, inferior parietal and posterior cingulate cortex and cortical thinning in STC, IFC, OFC, inferior temporal, fusiform and lingual cortex are associated with genetic liability to develop BD in unaffected relatives (Bootsman et al., 2015; Fears et al., 2014). The rest four studies showed cortical thinning in IFC, STC, parahippocampal, middle and inferior temporal, middle frontal, fusiform and supramarginal cortex, cortical thickening in postcentral cortex (Hanford et al., 2016b; Papmeyer et al., 2015; Roberts et al., 2016) and no differences in CT or SA (Roberts et al., 2016; Sugranyes et al., 2017) in the relatives of BD compared to HC. We recently reported gray matter volume results from the same cohort with a significant main effect of the group in cerebellum, IFC, parahippocampal, lingual, posterior cingulate and supramarginal cortex. Larger IFC and smaller cerebellum were demonstrated in both BDI and FR (Saricicek et al., 2015). In the present study, we primarily aimed to investigate gray matter volume determinants, CT and SA, between BDI patients, their FR and HC. Based on the studies in the literature and our previous findings, ACC and IFC were considered as strong candidates to be a BDI endophenotype and were selected as regions of interest (ROIs) for this study. We also aimed to conduct an exploratory analysis of SA and CT of other brain regions apart from the defined ROIs between groups to provide information for future research. Considering the summarized literature above, we hypothesized that BD and FR will have decreased CT and/or SA of the defined ROIs and will differ from HC."
published_079,"Eukaryotic organisms use oxygen (O2) as the final electron acceptor in the mitochondrial electron transport chain, producing water (H2O) and driving the production of the high-energy molecule ATP through oxidative phosphorylation (OXPHOS). The OXPHOS system is located in the mitochondrial inner membrane and is composed of five complexes which couple the pumping of H+ to the transfer of electrons from different substrates, such as NADH (oxidised by complex I) and succinate (oxidised by complex II). The difference in charges and pH generated across the mitochondrial inner membrane establish the mitochondrial membrane potential (ΔΨmt) and the pH gradient (ΔpH), respectively. Both parameters determine the protonmotive force (Δµmt) essential to drive OXPHOS. A series of reactive oxygen species (ROS) is also formed from the incomplete reduction of O2 during respiration [1,2]. ROS can oxidise the majority of cellular components including nucleic acids, lipids and proteins, and are known to be associated with cell damage, particularly in conditions of oxidative stress [3]. Mitochondrial ROS are involved in many pathological scenarios [4] such as stroke [5], cancer [6], Parkinson's [7], Alzheimer's [8] or cardiovascular diseases, where its overproduction may contribute to disease progression. However, it is acknowledged that mitochondrial ROS also act as second messengers in cell signalling processes in a variety of physiological conditions [9–15]. Among the five complexes comprising OXPHOS, complex I is the largest and performs a reversible NADH-ubiquinone oxidoreductase reaction coupled to pumping four H+ across the mitochondrial inner membrane. Complex I is formed by a hydrophilic arm which incorporates one flavin mononucleotide (FMN) and eight iron-sulfur clusters involved in electron transfer across this structure. The hydrophilic domain is attached to a hydrophobic arm involved in the H+-pumping function of the complex. Energy transfer to the hydrophobic domain occurs through the stabilization of the oxidised quinone in the ubiquinone-binding site which allows a series of conformational rearrangements necessary for H+ pumping [16]. Complex I can also undergo a deactivation process named active/‘deactive’ transition (A/D transition) which implies a switch from a NADH-ubiquinone oxidoreductase activity to a Na+/H+ antiporter through its hydrophobic arm [17,18]. Importantly, deactivation includes a series of conformational changes in which the Cys39 of the complex I subunit ND3 becomes exposed. This exposure has been used as a marker of deactivation [19,20]. In addition, complex I can be modulated by proteins and lipids [21] whose deregulation can lead to pathophysiological scenarios. Among them, a genetic variant of Parkinson's disease involves the mutation of the PTEN-induced putative kinase (PINK1) gene which has been associated with lower complex I activity and increased ROS production [22,23]. Mitochondrial complex I is also a major site of superoxide anion production in the mitochondria [1,24] through both forward and reverse reactions (electron transfer from NADH to ubiquinone, or from reduced ubiquinone to NAD+, respectively). The reverse reaction or reverse electron transfer (RET) needs a large pool of reduced ubiquinone which is normally generated from succinate oxidation through mitochondrial complex II, can be inhibited by rotenone and is dependent on high ΔΨmt [25,26]. RET has been implicated in exacerbated ROS production in reperfusion after ischemia [27,28]. Cells are frequently subjected to changes in oxygen availability and must adapt in order to survive. A decrease in oxygenation (hypoxia) induces a series of acute and long-term cellular, tissue-specific and systemic adaptive responses [29]. Both types of responses have been linked to the production of ROS. Whether ROS generation increased or decreased in hypoxia was strongly debated for years [30,31]. We have recently described that superoxide anion is produced in the first minutes of hypoxia by the mitochondria in different cell types, and correlates in endothelial cells with the oxidation of protein thiols [32,33]. More recently, it has been described that complex I is involved in the specialized acute response to hypoxia that takes place in the carotid body [34], where it triggers a ROS signal that activates ion channels provoking the release of neurotransmitters and hyperventilation [35]. Herein, we describe that complex I is involved in the ROS burst produced in acute hypoxia, in endothelial cells but also in brain tissue, and the mechanism by which complex I may be involved in triggering this response."
published_080,"The advent of and increased access to antiretroviral therapy (ART) has transformed HIV from a deadly disease to a chronic condition for many people living with HIV.1 With ART, people living with HIV can now have a near-normal life expectancy.1 However, unhealthy behaviours such as tobacco use threaten to undermine some of the gains that have been made.2 Smoking increases the risk of death among people living with HIV.3,4 A study3 among 924 HIV-positive women on ART in the USA reported an increased risk of death due to smoking with a hazard ratio (HR) of 1·53 (95% CI 1·08–2·19). A prospective cohort4 of 17 995 HIV-positive individuals from Europe and North America receiving ART found a mortality rate ratio of 1·94 (95% CI 1·56–2·41) for smokers when compared with non–smokers. The average years of life lost by HIV-positive smokers compared with HIV-positive non-smokers have been estimated as 12·3 years, which is more than twice the number of years lost by HIV infection alone.5 People living with HIV are more susceptible to tobacco-related illnesses such as cardiovascular disease, cancer, and pulmonary disease when compared with those who are HIV-negative or with the general population.6–8 Furthermore, smoking among people living with HIV increases susceptibility to infections such as bacterial pneumonia, oral candidiasis, and tuberculosis.9–11 A case-control study12 among 279 ART-naive HIV-positive men in South Africa found that current smoking tripled the risk of pulmonary tuberculosis (adjusted odds ratio [OR] 3·2; 95% CI 1·3–7·9, p=0·01). Smoking also increases the risk of developing AIDS (HR 1·36; 95% CI 1·07–1·72) among people living with HIV.3 This increased susceptibility has been mainly attributed to biochemical mechanisms including the immunosuppressive effects of smoking and its negative impact on immune and virological response even when on ART.13 Behavioural mechanisms have also been suggested—for example, an association between smoking and non-adherence to antiretroviral therapy.14,15 Few estimates are available of population-level prevalence of tobacco use among people living with HIV from low-income and middle-income countries (LMICs), where the burden of HIV and tobacco-related illnesses is greatest.16–18 Our study aimed to address this evidence gap for all forms of tobacco use among people living with HIV, using data from 28 nationally representative household surveys. Our study also compared tobacco use prevalence among people living with HIV to that among HIV-negative individuals. Evidence before this study We searched MEDLINE (OVID platform) for articles published from inception until Dec 31, 2015, that included the terms “smoking” or “tobacco use” or “smokeless” or “cigarette” AND “HIV” or “human immunodeficiency virus” or “AIDS” or “Acquired Immune Deficiency Syndrome” in the title and were from low-income and middle-income countries (LMICs). We updated this search on Sept 30, 2016. We identified six primary research articles on the prevalence of tobacco smoking among people living with HIV that covered eight countries, of which only one article was based on national-level Demographic and Health Survey data from one country. We did not identify any multi-LMIC comparisons on the topic. Added value of this study Our study is the largest to our knowledge to report country-level and overall prevalence estimates for tobacco smoking, smokeless tobacco use, and any tobacco use among people living with HIV from 28 LMICs using nationally representative data that is comparable across countries. Our study is also the first to compare the country-level prevalence estimates for tobacco use for people living with HIV with those for HIV-negative individuals in the respective countries where such data are available. Implications of all the available evidence Findings from our study and all other identified studies confirm that for LMICs, the prevalence of tobacco use is higher among people living with HIV than among those without HIV, for both men and women. Policy, practice, and research action on tobacco cessation among people living with HIV is urgently needed to prevent the excess morbidity and mortality due to tobacco-related diseases and to improve the health outcomes in this population. This action could include exploring effective and cost-effective tobacco cessation interventions for people living with HIV that are appropriate and scalable in low-resource settings; the integration of tobacco use services within HIV programmes in LMICs including proactive identification and recording of tobacco use, as well as the provision of tobacco use cessation interventions; increasing health-care providers' awareness and competencies on provision of tobacco use cessation services among people living with HIV; increasing awareness of the harms due to tobacco use and the benefits of quitting among people living with HIV; and implementing smoke-free policies within HIV services."
published_081,"Carbon dioxide capture and storage (CCS) occupies an ambiguous role in literatures on climate change mitigation as both a key technology for emissions reduction and a source of concerns relating to its feasibility and public acceptability. CCS refers to the process of capturing carbon dioxide (CO2) from power plants and other industrial sources, transporting it by pipeline, compressing it and then burying it in deep geological formations. CCS is thus intended to permanently prevent CO2 from reaching the atmosphere and contributing to climate change. CCS boasts numerous strengths as a CO2 abatement technology. It can be built into new thermal power plants or retrofitted onto older facilities, and is viewed by some as offering a means to significantly reduce emissions in advance of more complex transitions to renewable energy systems [1]. Moreover, CCS is the only currently available technology for decarbonising fossil-fuel-intensive industries such as cement, fertiliser and steel manufacture [1]. Assessments conducted for the IPCC and other bodies have identified CCS as a low risk and cost-effective emissions reduction technology [2–4]. Scenario modelling focussed on limiting global average temperature rises to below 2 °C suggests that CCS could contribute one-sixth of total emissions reductions by 2050 [5]. Projected costs for maintaining a 2 °C limit were found to be 40% higher in scenarios where CCS was unavailable [5]. While the feasibility of such plans remain highly contested, it has also been suggested that combining CCS with bio-energy may represent a relatively benign means of generating electricity, while removing and permanently separating CO2 from the atmosphere [6,7]. Given the hitherto slow progress on global emissions reductions, ‘negative emissions’ provided by bio-energy with CCS (BECCS) may provide a means of reducing atmospheric concentrations of CO2 in scenarios where cumulative emissions over-shoot recommended levels [8]. Indeed, the recently stated goal of the Paris Agreement to ‘pursue efforts’ to limit temperature increases to 1.5 °C means that BECCS has gained salience in mitigation planning and may be essential if more ambitious targets are to be met [9]. Since the early 2000 s, a rich literature has emerged aiming to examine how various publics interpret CCS and engage in issues surrounding it [10]. Rationales for this expansion have varied. Chief among those cited have been previous socio-technical controversies that may be analogues for poorly-implemented CCS deployments, and high profile project cancellations in Germany and the Netherlands where vocal public opposition played a key role [11]. More significantly, the growth of this literature represents the realization that: ‘CCS enters the energy and climate change arena with several disadvantages from the perception point of view: it is related to fossil fuels, which are at the heart of the problem, it is new and not fully understood, it involves waste disposal, and it is presently high-cost.’ [12] Whilst various elements of CCS systems have been found to be problematic, it is generally thought that the storage component raises the most significant concerns among lay publics. Such concerns have often been attributed to perceptions of risk, such as concerns that CO2 injection and storage could induce seismic disturbances, cause explosive gas releases, or pose toxic hazards that may contaminate nearby freshwater deposits or ecosystems [13,14]. Other studies have focused on public concerns over the trustworthiness and competence of project developers [15], or on longer-term ethical considerations relating to the sustainability, distributional and inter-generational effects of long term geological storage [16]. It has been suggested that situating geological storage offshore, away from population centres, may reduce the potential for public concern and anxiety relating to CCS [17]. However, studies explicitly examining this issue have suggested that sub-seabed storage does not eliminate concerns regarding the unsustainable, fossil-fuel-driven nature of many proposed CCS projects, nor the desire to protect future generations and non-human living systems from unforeseen long-term consequences of CCS deployment [18,19]. Other researchers have provided thorough overviews of the existing research into perceptions of CCS and this study does not aim to replicate their efforts [cf. 10]. Consistent findings indicate that awareness of CCS among lay publics is generally low [20–22], and that CCS tends to be perceived less favourably than renewable alternatives unless carefully contextualised within wider processes of decarbonisation and energy system change [23–25]. Acceptance of CCS is often contingent on early engagement with relevant communities, and on local perceptions of its relative risks and benefits. These in turn can be mediated by a wide range of contextual factors including: Context-specific characteristics of a project associated engagement processes [11,12,26]; Trust in government and industrial organisations [24,27,28]; Environmental values and beliefs [23,29,30]; Self-identity and worldviews [30–32]. Where relevant we refer to this literature below, however in this article we focus on a more specific set of issues that remain underexplored in light of recent upheavals surrounding the technology in the UK. These include the cancellation of the UK government’s £1 billion CCS competition in 2015 and the subsequent abandonment of CCS demonstration projects at power plants in Aberdeenshire and North Yorkshire. Moreover, given the newfound centrality of BECCS in many scenarios for meeting ambitious CO2 targets [7,9], relatively little research exists into how BECCS may be perceived. At present there appears to be no strong body of research examining perceptions of CCS in communities where projects have been cancelled [although see: 33] however, there is some evidence suggesting that the suspension and subsequent downgrading of CCS demonstrations has reduced community support for deployments in Illinois [34]. Given that trust in project actors has consistently been found to be a mediating factor in CCS acceptance [28,35,36], further examination of cases of policy instability are necessary in order to understand how cancellations may affect perceptions of the sincerity and competence of CCS proponents."
published_082,"Global standardized quantitative inventories of inorganic-Hg (IHg) and methylmercury (MeHg) concentrations in the aquatic network are currently lacking but are undoubtedly needed to report the effectiveness of the Minamata Convention. However, determining IHg and MeHg concentrations at low, but environmentally relevant levels remain challenging. Ambient low-level Hg samples are susceptible to contamination from many sources, including improperly cleaned equipment, improper sample-collection techniques, contaminated reagents, and atmospheric inputs from dust, dirt, and rain. Clean procedures are thus necessary to minimize contamination of samples at a typical ambient Hg concentration, which commonly is at the nanogram-per-liter level. Because contamination problems during sampling and storage have been reported widely, much attention has been paid to decontamination of the material, cleaning procedures, and appropriate storage of the samples [1,2]. Rigorous cleaning procedures have been established for all laboratory ware including sampling vials and other equipment that comes into contact with samples. There are various different cleaning procedures available, but they all generally include several acid baths and intermediate rinses with Hg-free deionized water or double distilled water, with storage in a designated Hg-free area, preferably sealed in clean plastic bags [1]. The importance of the choice of the material for sampling storage has been previously discussed [2]. For example, water samples can be stored in polytetrafluoroethylene (PTFE, Teflon®), glass or polyethylene terephthalate (PET) bottles (see ref [1] and references therein). Quartz or borosilicate glass bottles minimize the loss of Hg(II) [3] and are suitable for MeHg solution storage [4]. For extremely low concentrations, as found in seawater, PTFE presents the lowest risk for contamination [1]. However, PTFE bottles are more expensive than glass bottles and not all laboratories can afford them. Here, we provide a complete list of materials needed to achieve effective sampling of the typically low ambient Hg levels found in freshwaters. To prevent contamination, we describe a protocol for equipment cleaning and sample processing. As there is no consensus on reporting IHg and MeHg concentration from filtered or unfiltered water fractions, we give details for collecting both water fractions: filtered (dissolved) and unfiltered (total). Although detailed protocols for collection of water with low Hg levels are available (e.g. https://water.usgs.gov/owq/FieldManual/chapter5/pdf/5.6.4.B_v1.0.pdf), here we describe an updated, simple and didactic protocol for collecting samples from freshwater ecosystems for Hg analyses. Furthermore, as Hg binds strongly to organic matter, we include a procedure to collect samples for determining dissolved and total organic carbon concentration (DOC and TOC) as well as DOM composition. Consequently, this protocol can be extremely helpful for non-experts in Hg sampling and analysis, as well as for all Hg research focusing on the role of organic matter composition related to Hg concentrations in the water [5–9]. It is noteworthy to mention that this protocol does not provide guidelines on the sampling design. The users of this protocol are encouraged to plan their sampling considering the optimal site location (e.g. deepest point of lake, distance from shoreline, etc.), depth (e.g. hypolimnion, epilimnion, etc), number of replicates, number of times the sampling will be performed in a year, etc. It is also important to highlight that because the choice of material used to sample with is glass (instead of PTFE), this protocol is not suitable for seawaters and water with ultra trace Hg levels (i.e. below 0.01 ng L−1 for IHg and 0.002 ng L−1 for MeHg). Four different water samples will be collected following the protocol described here: Total-Hg, IHg and MeHg (unfiltered): 250 mL amber borosilicate glass bottle. Total-Hg, IHg and MeHg (filtered): 250 mL amber borosilicate glass bottle. TOC (unfiltered): 60 mL glass vial. DOC, fluorescence spectroscopy, anions and cations (filtered): 60 mL glass vial."
published_083,"De-icing salt-frost scaling is a type of superficial damage that occurs on concrete structures. This happens when water with a low concentration of de-icing salt freezes in contact with a concrete surface. The outermost layer of the concrete structure protects the reinforcement bars. Thus, if the concrete is not salt-frost scaling resistant, the protective concrete layer will scale off as a result of repeated freezing and thawing. This will in turn shorten the period of time until the reinforcement bars start to corrode; thus, the service life of the concrete structure is shortened. There are various methods to assess the salt-frost scaling resistance of concrete [1–7]. All of these methods use a low-concentration salt solution. This solution is in contact with the concrete test surface during the test. The salt solution and concrete are then exposed to a freeze–thaw cycle. The conditions of the freeze–thaw cycle are different (cycle duration, max and min temperature) for each salt-frost scaling test method. Two important features of the proposed method are the freeze–thaw cycle and the sample setup. The method was designed to enable the testing of high-performance concrete, which generally has a high salt-frost scaling resistance. Hence, the selected freeze–thaw cycle should result in a large mass of scaled material. The method should also be able to detect how different preconditioning processes affect the salt-frost scaling behaviour with various binders. Since a large mass of scaled material can result in leakage of salt solution if the solution is on top of the surface, the test surface was instead submerged into a salt solution inside a cup. The sample size was chosen to allow a large number of samples to be tested at the same time."
published_084,"Foam fractionation is a physical process in which rising foam serves as the medium to separate surface-active compounds from their diluted solutions [24]. Due to its low cost, free pollution and high efficiency at low concentrations, foam fractionation is widely used to separate not only various organic and inorganic chemical compounds, but also biological materials such as proteins and microorganisms [20,5,6]. A typical example for its industrial application is that it is successfully used by Tianjin Kangyi Biotechnology Company in China to separate nisin (a polypeptide) from the fermentation broth. By replacing membrane separation, foam fractionation has effectively decreased the price of nisin from 1200 RMB/kg at 2004 to 500 RMB at 2006 and then to 300 RMB/kg at 2015 (the data were obtained from Tianjin Kangyi Biotechnology Company). The successful application gives foam fractionation a great potential in the large-scale separation of proteins from their highly diluted aqueous solutions [6]. Foam fractionation has been troubled by protein aggregation induced by the gas–liquid interface [7]. For clearly describing how protein aggregation was induced by the gas–liquid interface, a schematic diagram is presented in Fig. 1. It is presented that protein molecules suffer structural unfolding as they are adsorbed at the gas–liquid interface [1]. Because a large gas–liquid interfacial area is used in foam fractionation, the number of protein molecules that suffer denaturation at the interface will be large. In the desorption of the unfolded molecules from the interface, some of them cannot return their native structures so that they readily interact with each other to form aggregates [3], as presented in Fig. 1. In the foam fractionation of proteins, foam drainage is often enhanced to obtain a high enrichment ratio. However, the enhanced foam drainage reduces the gas–liquid interfacial area to intensify the desorption of adsorbed protein molecules in the rising foam. In this case, protein aggregates will flow into the bulk solution with the drained liquid and may reduce the protein recovery. Furthermore, the enhanced foam drainage increases the relative content of the adsorbed protein molecules in the foam so that protein aggregation in the defoaming process will also be intensified [17]. As a result, protein aggregation induced by the gas–liquid interface will significantly decrease the foam fractionation performances [18,19]. Then it is necessary to give a molecular-level understanding of the gas–liquid interface-induced protein aggregation to reduce its bad effect on foam fractionation of proteins. pH is a common parameter that significantly affects both protein aggregation and foam fractionation performances due to its effects on the zeta potentials and structures of proteins [29,15]. So it is necessary to study the role of pH in protein aggregation in foam fractionation at the molecular level. In general, the net charge of a protein increases as pH deviates from its isoelectric point. The increased charge enhances the electrostatic repulsion between protein molecules to reduce their aggregation in the aqueous solution and reduce the protein surface excess at the gas–liquid interface [10,13]. The decreased surface excess will result in an unstable foam with large bubbles and low volumetric liquid fraction. Then, a high protein enrichment ratio will be obtained so that a high-level protein aggregation in the defoaming process will be caused [17]. Furthermore, a pH far away from the protein isoelectric point also can unfold the protein structure to enhance protein aggregation [9]. Thus pH will have a complicated effect on the protein aggregation in foam fractionation. Bovine serum albumin (BSA) is a typical globular protein which has a similar structure to human serum albumin (HSA), so it has wide applications in the pharmaceutical field [26,16]. Due to its low price, high purity and good foamability, BSA is also widely used in the fundamental studies of foam fractionation [14]. Thus in this work, it will be selected as a model protein for the molecular-level study of the role of pH in protein aggregation in foam fractionation. At present, there are many references reporting the effects of pH on the structure and the foam fractionation of BSA [25,30,2]. However, few of them have reported the effect of pH on the aggregation of BSA in foam fractionation. In the current work, the effects of pH on the zeta potential, secondary structure, tertiary structure and molecular size of BSA will be firstly investigated in the pH range from 7.0 to 3.0. Then, the effect of pH on the aggregation of BSA in the aqueous solution will be investigated. Subsequently, the effects of pH on the BSA surface excess and bubble size will be studied. Finally, the effects of pH on the relative contents of the BSA aggregates in the foamate will be investigated. Based on the above results, the role of pH in the aggregation of BSA in foam fractionation will be discussed at the molecular level."
published_085,"In this decade, we have witnessed the increased use of antibodies, recombinant proteins, and peptides in preclinical and clinical studies.1–5 As a powerful tool in cell biology, the transfection of functional proteins acts as a chemical cue for biological purposes, and it offers considerable potential for clinical applications. Protein transfection has also been useful for obtaining information on protein-protein interactions and protein trafficking within cells. Moreover, it is an essential prerequisite to antibody therapy and vaccinations against intracellular proteins. Therefore, the attention of the biomedical community has been focused on seeking efficient and nontoxic methods of protein transfection.1,6 Current delivery systems for the transfer of proteins through the cell membrane to the cytosol include physical (e.g., microinjection and electroporation) and chemical (e.g., macromolecular carriers) systems.7–11 In general, the efficiency of delivery into cells depends on the numerous biochemical properties of the proteins (including structure, composition, size, molecular weight, and charge).12,13 Commercially available protein transfection chemical reagents include (1) lipid-based, (2) cationic-polymer-based, and (3) cell-penetrating peptide-based formulations.7,14 Unfortunately, access to such transfection reagents as PULSin,15 ProteoJuice,16 Xfect,17 and BioPorter18 is limited by their high cost, which hampers the scalability of the process. For various proteins, endocytosis is crucial to their cytoplasmic delivery.19,20 The mechanisms involved in endocytosis can be classified into groups (i.e., macropinocytosis and caveolin-mediated, clathrin-mediated, clathrin-independent, and caveolin-independent endocytosis), according to the protein machinery employed.19,21,22 The treatment of cells with endocytosis inhibitors capable of blocking a specific endocytotic pathway is useful for determining which and how many uptake pathways are utilized by a specific particle or complex.17,20,23–25 In addition to endocytotic uptake, cytosolic delivery is modulated by subsequent endosomal escape, the promotion of which can also improve biological therapy by ensuring cytosolic delivery.12 Commonly used in several biochemical reactions, 4-(2-hydroxyethyl)-1-piperazineethanesulfonic acid (HEPES) is employed as a zwitterionic buffering agent in cell culture media. Indeed, HEPES-buffered solutions are used to introduce plasmid DNA to monolayer cell cultures during calcium phosphate transfection.26 The application of HEPES for cellular functions could be as previously described. Possibly as the result of the zwitterionic property of HEPES, HEPES buffer resulted in less diffusion of PrP106-126 peptide in mica-supported lipid bilayers.27 Additionally, HEPES repressed the contractility of arterial smooth muscle, and it may promote alkaline-initiated contraction in non-mammalian vessels, indicating the functional effects of HEPES in cells.28,29 However, it is still unknown whether it was necessarily the HEPES in these studies that promoted protein delivery in cells. In this study, we demonstrated for the first time that the HEPES method can be used for efficient protein transfection. After being combined with a pure HEPES solution, proteins with various molecular weights, including antibodies, recombinant proteins, and small peptides, were successfully delivered into the cytosol of various cultured cell lines without notable cytotoxicity. Stress-induced phosphoprotein 1 (STIP1) is upregulated in certain cancers, and suppressing the function of STIP1 inhibits tumor progression.30,31 The HEPES method successfully delivered STIP1 antibodies to cancer cells, leading to protein degradation of STIP1. Overall, our data demonstrate that HEPES is a readily available, simple, and efficient reagent for promoting protein transfection and intracellular targeting."
published_086,"Temperature is a major driver in climate adaptation in ectotherms (Angilletta Jr, 2009). Thermal adaptation influences the behaviour and physiology of ectotherms (Dawson, 1975; Brown et al., 2004). Small ectotherms including many insects have limited physiological thermoregulatory mechanisms and react rapidly to changes in environmental temperature (Porter and Gates, 1969). Many insects are climate specialists restricted to specific thermal conditions (Hoffmann, 2010). These species will actively seek places with adequate thermal conditions for developing and avoiding harmful temperatures, which dictates the habitats they occupy and restricts their distribution (Dillon et al., 2009). One approach to characterising the thermal needs of an ectotherm is to assess its preferred temperature (Tp), the temperature selected when a range of temperatures is available and thermoregulation incurs no costs (Dillon et al., 2009). It is a commonly used proxy for thermal adaptation in insects as well as reptiles (Hertz et al., 1993; Sayeed and Benzer, 1996; Dillon et al., 2012). Theoretically, the preferred temperature is expected to be in a similar range as the optimum temperature of maximum physiological performance (e.g., fecundity), but empirically, Tp can be lower than the optimum temperature (Dillon et al., 2012). Another approach to assessing thermal vulnerability is to acquire knowledge about a species' particular habitat requirements. The standard meteorological approach for characterising macrohabitats is measuring environmental factors at a height of 1.2–2.0 m above the ground (e.g., air temperature). Such data are available for long time spans (e.g., WorldClim, Hijmans et al., 2005). The spatial resolution of climate data varies from metres to kilometres. Overly coarse resolution can be problematic in identifying a species’ requirements (Araújo et al., 2005; Potter et al., 2013). To overcome this common mismatch in spatial scales (Potter et al., 2013), measurements on the parameters of microhabitats are required to characterise the environment of small ectotherms (Andersson et al., 2010). Information on the microhabitat is often difficult to obtain, and thus, fine-scale data are rare. Moreover, weather data do not always reflect the environmental conditions experienced by animals (Bakken, 1992; Woods et al., 2015). The operative environmental temperature (Te) is the animal body temperature in equilibrium with the thermal habitat (Bakken, 1992; Angilletta Jr, 2009). Te depends on environmental factors (e.g., air temperature, radiation) but also on morphological characteristics (e.g., size, colour) as well as on convection and conduction. It reflects the temperature experienced without physiological heating or cooling (Huey, 1991; Angilletta Jr, 2009). Two alternative approaches are typically used to quantify Te: First, mathematical models can be used to estimate Te based on microclimatic and morphological aspects (Kingsolver, 1983) and, second, mock-ups of an animal can be positioned in the habitat to measure the temperature inside the mock-up (Kingsolver and Moffat, 1982). In nature, the exact position preferred by ectotherms is influenced by factors such as microhabitat climate and biological interactions (Huey, 1991; Angilletta Jr, 2009). For practical reasons, a species' thermobiology is often characterised using experimental approaches in the laboratory; for example, Tp is commonly examined by exposing individuals to an artificial temperature gradient and observing which position they prefer (Dillon et al., 2012). Laboratory approaches have the major advantage of reducing complexity to the factor(s) considered relevant and of monitoring under controlled conditions (Harshman and Hoffmann, 2000; Dillon et al., 2012). However, the ecological relevance of laboratory approaches has been questioned (Harshman and Hoffmann, 2000; Dillon et al., 2012), such as due to changed behaviour in the laboratory as compared with the field (Huey, 1991; Harshman and Hoffmann, 2000). The relationship of laboratory results about a species’ thermal preference to its thermobiology in natural habitats has been analysed multiply for vertebrate ectotherms (Huey and Bennett, 1987; Hertz et al., 1993; Webb and Shine, 1998). In Drosophila research, the laboratory Tp values of two species relative to each other were found to be in line with distribution-based thermal expectations (Matute et al., 2009), but to our knowledge, laboratory-based thermal preference and thermal behaviour in nature have never been directly compared for Drosophila. Here, we used the fly Drosophila nigrosparsa, an alpine-habitat specialist (Bächli and Burla, 1985; Kinzner et al. 2016, 2018; Cicconardi et al. 2017a, 2017b), to address the question if dimension-reduced laboratory results from Tp experiments reflect thermal behaviour in nature. In more detail, we examined links among five aspects of thermobiology in this fly. We related (i) the laboratory-inferred preferred temperature (Tp), field meteorological data on the (ii) macrohabitat and (iii) on microhabitats representative of the range of conditions available in the macrohabitat, (iv) temperatures in microhabitats in which flies were located (hereafter termed tracking-based temperatures, Ttb), and (v) experienced-temperature models built using the operative-temperature approach (Te). We show inconsistency of Tp in the laboratory with microhabitat data and Ttb in the field. Based on this result, we highlight challenges associated with using laboratory preference to infer performance in the field."
published_087,"Mean peak values obtained from intact cells (IC), detergent-soluble extracts (SD) and detergent-insoluble extracts (ID) associated with immature (IC2, SD2, ID2) and mature epididymal spermatozoa (IC9, SD9, ID9) are shown in Supplementary Table 1. Only 172m/z peak values for which the fold-changes were >2 and which presented at least one significant difference between epididymal samples (p<0.05) are reported in this table. Some of them are identified. The variation index was calculated as the product of the absolute difference between the maximum (max) and minimum (min) peak intensity values multiplied by the fold-change (fold) between the two extreme values. The variations are expressed as linear decrease (LD) or linear increase (LI) when all the mean values for the four epididymal samples were significantly different from each other (p<0.05), expressed as increase (I) or decrease (D) when at least one of the mean values was not significantly different from the others, and intermediate (inter.) when at least one value was different. In the first column, m/z values in green correspond to m/z peaks observed only in IC analysis (a total of 135m/z). Rows with color in IC, SD and ID columns correspond to specific peaks for the sample preparation. A list of endogenous biomolecules identified by top-down MS is shown in Supplementary Table 2. This table shows the raw file name associated to the NCBInr accession number, the gene name, the protein description, the location, the characterized post-translational modifications, the number of b ions, y ions and total ions, the delta mass (Da and ppm) between the ProSight theoretical mass M and the mass M observed by nano-ESI-FTMS, the mass M (Da) observed by nano-ESI-FTMS, the ProSight theoretical mass M (Da), the ProSight PDE score, the E-value identification probability, the p score, the precursor m/z and mass type with corresponding charge state (z), the fragmentation mode, the database used for identification, the signal/noise, the ProSight search type mode, the precursor mass type, the precursor mass tolerance (Da or ppm), the fragment mass type, the fragment mass tolerance (ppm), the delta M mode and disulfide activation/deactivation, the minimum of matching fragment between observed and theoretical fragmentation mass spectra, the include modified forms activation/deactivation, the sequence, the calculated theoretical average and monoisotopic mass [M+H]+ (Da) and the mass [M+H]+ (Da) observed previously by MALDI-MS."
published_088,2-D gel reference image of OSA RBC cytoplasmic fraction depleted from hemoglobin was shown. Graphics representing the identified variations for the different PRDX2 and Catalase proteoforms between groups and conditions were highlighted as examples. The 31 proteins spots identified by MALDI/TOF/TOF MS are displayed in detail in the Table. Fold-change histograms and pathway analysis between patients’ groups and conditions were shown. Material and methods are provided in detail as much as possible to be reproduced elsewhere.
published_089,Reported data represents the observed association between pre-existing use of injectable insulin before breast cancer diagnosis and the T-helper 1 and 2 produced cytokine profiles upon cancer diagnosis in women with both breast cancer and diabetes mellitus (Table 1). Data in Table 2 includes the observed correlations between T-helper 1 and 2 cytokines stratified by diabetes mellitus pharmacotherapy and controls.
published_090,"In this Data in Brief article, we present the salivary profiling of 28 plasma biomarkers from 33 patients (12 healthy individuals (control), 10 Chronic Periodontitis patients (CP) and 11 patients with Aggressive Periodontitis (AP)). Subjects were between 18 and 75 years of age. Saliva samples were processed in triplicate and analyzed in duplicate by LC-MRM. Quantitative results obtained for the 28 protein biomarkers studied were validated and are presented together in Table 1. Absolute quantification was obtained by spiking labeled proteotypic peptides containing heavy isotope equivalents of arginine ([13C6] or [13C6, 15N4]) or lysine ([13C6] or [13C6, 15N2]) residues. The analytical performance obtained for each biomarker is indicated in Table 1, based on the following criteria: Linear concentration range (pg/mL), Linear response (R2), Limit of detection (LOD, pg/mL), Limit of quantification (LOQ, pg/mL), precision (CV %), clinical range in saliva samples (pg/mL). The three disease status groups were compared after normalization for albumin concentration to adjust for analytical variability (Fig. 1). The significance of any difference between the groups was statistically analyzed (Mann-Whitney test). Four proteins (Beta-2-glycoprotein I (APOH); α-fibrinogen (FIBA); Hemopexin (HEMO); Plasminogen (PLMN)) were found to be present at statistically different levels between groups, and are promising candidates to facilitate screening and diagnosis of periodontal diseases."
published_091,"Rhamnolipid were first confined from Pseudomonas aeruginosa and portrayed by Jarvis and Johnson in 1949 [3]. These molecules are usually built from the association of rhamnose sugar and hydroxyl (3-hydroxy) unsaturated fats. Rhamnolipid with one sugar particle are alluded to as mono-rhamnolipid, whereas those with two sugar atoms are termed as di-rhamnolipid. In this work, we portray data on production and evaluation of rhamnolipid produced by a non-pathogenic bacterium P. aeruginosa VM011 (Table 1). The biotechnological potential and application of rhamnolipid was evaluated by emulsification activity (Table 2)."
published_092,"The band densities quantification of Western Blotting was shown in the graph below. In Fig. 1a, Representative blots of p-p38, p-p44/42, p44/42, and beta-actin of the cells after exposing to MG (400 μM) after the treatment time intervals between 1 h and 24 h. Phosphorylation of p38 and p44/42 were induced by MG in RAW264.7 cells. Endogenous p44/42 and beat-actin were used as the internal control. Data shown was corresponding to 5 repeated experiments. In Fig. 1b, the mRNA expression of osteoclast biomarkers: TRACP5, OSCAR, and CTSK still increased in MG group and the effects of MG did not countered by the transfection. Before 400 μM MG treatment, RAW264.7 cells underwent transfection under treatment of 10 nM siRNA for 24 h. The cells were collected after 24 h MG treatment. However, the mRNA expression of the osteoclast bone biomarkers persisted to increase under p38 and p44/42 inhibition in MG-treated macrophages. Beta-actin acted as mRNA internal control. Data shown was corresponding to 5 repeated experiments. These data implied that MG activated the p38 and p44/42, which was reported to regulate proliferation and differentiation of osteoclast. However, the decreasing MAPK though siRNA knockdown did not change expression of those target markers, TRACP5, OSCAR, and CTSK, in mRNA level. The effects of MG to other osteoclast markers through p38 and p44/42 would be worth to be investigated."
published_093,"Type 1 diabetes (T1D) is a major global health issue, and its incidence is increasing. T1D is a T cell-mediated autoimmune disease that reduces the population of pancreatic islet β cells, which limits insulin production and interferes with glucose homeostasis. The immune dysfunction in T1D is complicated, with effects both in pancreatic islets and outside the pancreas. Different components of the immune system [e.g., CD4+, CD8+ T cells, Tregs, B cells, dendritic cells (DCs), monocyte/macrophages (Mo/Mϕs), natural killer T cells (NKTs)] contribute to autoimmune responses in T1D, complicating efforts to develop successful treatments or a cure that will work across most or all individuals with the disease. Several recent clinical trials (Bach, 2011; Wherrett et al., 2011) highlight the challenges in conquering T1D, but their failures provide some valuable lessons about the limitations of conventional immune therapy and the future direction of the quest. Specifically, they point to the need for an approach that produces comprehensive immune modulation at both the local pancreatic and systematic levels rather than targeting the pancreatic effects of one or a few components of the immune system. The Stem Cell Educator therapy takes this broader approach (Zhao and Mazzone, 2010; Zhao et al., 2012; Zhao, 2012; Zhao et al., 2013; Li et al., 2015). Physiologically, the human immune system constantly protects the body against a variety of pathogens that may be encountered. Following the recognition and eradication of pathogens through adaptive immune responses, the majority (90–95%) of T cells undergo apoptosis with the remaining cells forming a pool of memory T cells, designated central memory T cells (TCM), effector memory T cells (TEM), and resident memory T cells (TRM) (Clark, 2015). In comparison to conventional T cells, these memory T cells are long-lived with distinct phenotypes, such as expression of specific surface markers, rapid production of different cytokine profiles, capability of direct effector cell function, a different potential for proliferation, and unique homing distribution patterns. As a group, memory T cells display quick reactions upon re-exposure to their cognate antigens in order to eliminate the reinfection of pathogens and restore balance and harmony of the immune system. Nevertheless, increasing evidence establishes that autoimmune memory T cells become the “stumbling blocks” and hinder most attempts to treat or cure autoimmune diseases, including T1D, multiple sclerosis (MS), rheumatoid arthritis (RA), and system lupus erythematosus (SLE) (Ehlers and Rigby, 2015; Clark, 2015; Devarajan and Chen, 2013). Therefore, novel and more comprehensive approaches are needed to fundamentally correct the inordinate dominance of autoimmune T cell memory and overcome the complexities of autoimmune responses. We previously characterized a new type of stem cell from human cord blood, designated a cord blood-derived multipotent stem cell (CB-SCs) (Zhao et al., 2006; Zhao and Mazzone, 2010). CB-SCs display a unique phenotype with both embryonic and hematopoietic markers that distinguish them from other known stem cell types, including hematopoietic stem cells (HSCs), mesenchymal stem cells (MSCs), and monocytes/macrophages (Mo/Mϕ) (Zhao et al., 2003). SCE therapy functions as an “artificial thymus” that circulates a patient's blood through a blood cell separator, briefly treats the patient's lymphocytes with CB-SCs in vitro, induces immune tolerance through the action of autoimmune regulator (AIRE, expressed by CB-SCs), returns the educated autologous lymphocytes to the patient's circulation, and restores immune balance and homeostasis (Zhao and Mazzone, 2010; Zhao et al., 2012; Zhao, 2012). This approach was piloted in clinical studies for the treatment of diabetes and other autoimmune diseases in China with patients of Chinese origin (Zhao et al., 2012; Zhao, 2012; Zhao et al., 2013; Li et al., 2015). Our clinical data demonstrates that the SCE therapy provides long-lasting reversal of autoimmunity that induces the regeneration of pancreatic islet β cells and improvement of metabolic control in individuals with longstanding T1D (Zhao et al., 2012). Findings from recent autoimmune-caused Alopecia Areata (AA) trial provide visible evidence that SCE therapy can control autoimmunity and lead to the regeneration of tissues like hair regrowth (Li et al., 2015). Here, we explored the expansion of the therapeutic potential of the SCE therapy to the treatment of Caucasian T1D subjects in Spain."
published_094,"Randomized controlled trials afford reliable approaches to understand the causal relevance of drug treatments and can also help our understanding of disease mechanisms by relating changes in biomarkers with incidence of disease or with surrogate markers of disease. Advances in molecular methods now permit use of high-throughput functional genomics strategies in clinical trials. Application of such approaches has been under-utilized to date, with previous studies focusing on comparisons of transcriptomes to understand mechanisms and identify novel biomarkers (Beck et al., 2014). Animal and experimental models of disease pathogenesis have limited ability for translation into humans, particularly in the context of complex diseases (Seok et al., 2013, Takao and Miyakawa, 2015), while incomplete knowledge of mechanisms contributes to inconclusive findings in randomized trials and the current high failure rate in late stage drug development. Hence, there is an urgent need to demonstrate the value of combining functional genomic approaches, including genome-wide genotyping and gene expression profiling, together with measurements of biochemical and clinical markers in clinical trials to enhance our understanding of the pathophysiological processes and mechanisms of action of novel drug treatments. Such approaches may also allow high-throughput assessment of cellular and molecular responses at group and individual levels that can also be integrated with effects on clinical outcomes data. Hence, integrated analysis may yield clinically relevant insights about treatment that could guide the design of large outcome trials. This study applied functional genomics methods to investigate the molecular response to vitamin D supplementation. In addition to the established role of vitamin D in calcium metabolism and bone disease, accumulating evidence suggests a possible role of vitamin D in immune function and inflammatory diseases (Bouillon et al., 2008). Previous studies have investigated the associations of vitamin D with gene expression (Carlberg et al., 2013, Hossein-Nezhad et al., 2013, Ramagopalan et al., 2010), but these have typically been cross-sectional, in experimental models, involved relatively small sample sizes or lacked placebo controls. Moreover, no previous studies have assessed the impact of genome-wide genetic variation on responses to vitamin D supplementation. The aim of the work described here was to examine the molecular responses to vitamin D supplementation in a randomized, placebo-controlled trial. To achieve this, we investigated changes in response to treatment after 12 months in whole blood transcriptomes and plasma levels of cytokines, in addition to genetic determinants of individual responses on circulating 25-hydroxy vitamin D (25[OH]D) and genome-wide gene expression, by comparing a total of 305 individuals allocated to daily treatment with vitamin D at either 4000 IU, 2000 IU or placebo in the BEST-D trial (Hin et al., 2017)."
published_095,"Alkyds are polyesters derived from the reaction of polyols and dicarboxylic acids or their anhydrides modified by the addition of fatty acids [1]. They can also be defined as products of a polycondensation reaction between polybasic acids and polyhydric alcohols modified with fatty acids [2]. The presence of the fatty acid confers a propensity to form plastic coating during application [1] (see Scheme 1). Alkyd resin is one of the most indispensable raw materials in the coating and paint industries; it is the main binder accounting for a large volume of coatings and paints used for decorative purposes [3, 4]. They are inexpensive owing to the inexpensive materials from which they are produced and therefore have found applications more than other binders. In addition, they are easy to manufacture and dissolve in a cheap solvent like xylene and white spirit. Other additional qualities include ease of application under variable environmental conditions, gloss and gloss retention, durability, drying abilities, film flexibility, good adhesion, etc [5]. Alkyd resins applications are not only limited to decorative paints but are also applied in air-drying paints, machine tool finishes, inks, matt and semi-matt varnishes of wood furniture, protection of surface from chemical attack, mechanical stress and environmental effect, etc [5]. The presence of oil and glycerol as parts of alkyd resins confers the quality of them being eco-friendly compared to conventional petroleum-based polymers that constitute environmental pollution and degradation; they have attracted particular attention because of their inherent non-toxic nature and biodegradability [6, 7, 8, 9, 10]. Several types of seed oils such as linseed oil, soybean oil, castor oil and tall oil have been utilised for the syntheses of polymeric resins like alkyds depending on the nature of unsaturated fatty acid present [11, 12]. These fats and oils have the ability to slowly absorb oxygen forming dry, tough, transparent and durable films when spread and exposed to air. These drying oils contain a variety of polyunsaturated fatty acids like linoleic and linolenic acid and their triglycerides [13]. Natural rubber (Heveabrasiliensis) is a high utility base resource with potential for several industrial utilizations such as putty, soap, biodiesel, alkyd resins, etc. Currently, in some southern states of Nigeria, there is an abundance of natural rubber plantation that can serve as a source of rubber seed oil (RSO), a practical raw material for the production of alkyd resins on an industrial scale. It has been stated by a previous survey that about 42,980 metric tons of rubber seeds could be generated annually from a natural rubber plantation in Nigeria. However, conditions such as abnormal leaf disease, genetics, phytophora disease, weather and powdery mildew disease usually influence the quantity of rubber seed that could be produced in any given year [14]. The unique nature of rubber seed oil (a semi-drying oil) is because of its level of unsaturation, relative abundance and possession of similar properties with linseed oil (a drying oil) which is widely utilized in the synthesis of alkyd resins [15]. In Nigeria, there is a high demand for vegetable oil such as palm oil, soybean oil, coconut oil linseed oil, olive oil, etc., for nutritional, cosmetic and pharmaceutical purposes [16]. The use of rubber seed oil for non-edible purposes increases its availability and relative abundance renders it a reasonable substitute for linseed oil (Linumusitatissimum L.) which is costing the country a deal to import [17]. In this study, alkyd resins were synthesised via the monoglyceride process from rubber seed oil (RSO), linseed oil (LSO) and their blends. The physicochemical properties, drying performance and chemical resistance were then evaluated to investigate the effect of blending on the properties of RSO and LSO."
published_096,"According to recent reports, Tob/BTG family proteins display antiproliferative activity in a variety of cell types and are involved in the regulation of tumorigenesis [1, 2, 3, 4, 5, 6, 7, 8]. To the best of our knowledge, protease-like activity has never been reported in the Tob/BTG family of proteins or in small synthetic peptides. However, we have previously found the auto-proteolytic activity and the proteolytic activity of the synthetic nona-peptide JAL-TA9 (YKGSGFRMI) derived from the Box A region of the Tob1 protein against Aβ42 and its fragment peptides, Aβ1-20 and Aβ11-29 [9, 10, 11, 12, 13]. Nuclear magnetic resonance (NMR) study proved that the stereo-structure of JAL-TA9 is very compact [14]. Furthermore, we reported the similar proteolytic activity of 5-mer synthetic peptides derived from JAL-TA9 [15]. Therefore, we have termed the shorter proteolytic peptides like JAL-TA9 as Catalytide (Catalytic peptide) [11, 12, 13]. Tob/BTG family proteins are consisted of BTG1, BTG2/Tis21/PC3, ANA/BTG3, BTG4/PC3B, Tob1, and Tob2. Three kinds of homologous regions, Box A, Box B and Box C in the N-terminus region of the Tob/BTG family, are highly conserved among the Tob/BTG family of proteins, but the function of theses region are not clarified (SIFig. 1) [16]. The proteolytic activity of JAL-TA9 suggests that Tob/BTG family proteins may possess the proteolytic activity. Alzheimer's disease (AD) is the most common age-related neurodegenerative disorder. It is well known that aggregation and accumulation of Aβ42 causes AD due to the strong neurotoxicity of Aβ42 oligomers. This makes Aβ42 an effective target for drug therapies [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. Mainly, two strategies have been developed against Aβ42. The first utilizes inhibitors against β- or γ-secretases that control the production of soluble Aβ42 [22]. The other uses inhibitors of Aβ42 oligomerization [17]. Many trials have been conducted to develop drugs for the treatment of AD, but the results have not been encouraging [25, 26, 27, 28, 29, 30]. Thus, the development of new and effective drugs is an urgent necessity for treating AD. Catalytides such as JAL-TA9 are attractive candidates as peptide drugs with a novel strategy for prevention and treatment of AD. In this study, we focus on ANA-TA9 (SKGQAYRMI) derived from Box A region of the ANA/BTG3 protein corresponding to JAL-TA9 to find new Catalytide. Herein, we demonstrate the proteolytic activity of ANA-TA9 and its components, ANA-SA5 (SKGQA) and -YA4 (YRMI)."
published_097,"Human Polo-like kinase 1 (PLK1) regulates multiple events that lead to accurate chromosome segregation during cell division (reviewed in Bruinsma et al., 2012). It comprises an N-terminal kinase catalytic domain, and a C-terminal segment with tandem motifs that form the Polo-box domain (PBD). The PBD engages pSer/pThr phosphopeptide substrates to recruit PLK1 to mitotic structures (Elia et al., 2003a), direct PLK1 kinase activity (Kang et al., 2006), and mediate reaction-diffusion mechanisms for centrosome assembly (Mahen et al., 2011). Homologous PBDs, in which key residues implicated in phosphopeptide substrate recognition are conserved, also occur in the related human Polo-like kinases PLK2, PLK3, and PLK4 (reviewed in Archambault and Glover, 2009). PLK1's essential role in mitosis, and its dysregulation in several different forms of human cancer, has prompted efforts to create small-molecule inhibitors (reviewed in Liu, 2015). Potent ATP-competitive small-molecule inhibitors of PLK1 kinase activity are in clinical development (reviewed in Gjertsen and Schoffski, 2015). Moreover, compounds that inhibit PBD binding to phosphopeptide substrates are reported to induce mitotic arrest and apoptosis in cancer cell lines at micromolar concentrations (Reindl et al., 2008; Scharow et al., 2015; Watanabe et al., 2009; Yuan et al., 2011); but several have recently been identified as non-specific protein alkylators (Archambault and Normandin, 2017). Mutations activating the small GTP-binding protein KRAS, which occur frequently in human cancers, have proven largely refractory to therapeutic targeting (reviewed in McCormick, 2015). Inhibitors unique to the G12C KRAS mutant have been described (Ostrem et al., 2013), as have inhibitors of an essential interaction between KRAS and the prenyl-binding protein, PDEδ (Chandra et al., 2012; Zimmermann et al., 2013). An RNAi screen for factors whose depletion inhibits the growth of mutant KRAS G12D-expressing cancer cell lines identified PLK1 and components of the APC/C ubiquitin-conjugating holoenzyme (Luo et al., 2009). Genetic modulation of the pathway linking PLK1 to APC/C activation curtailed the proliferation of mutant KRAS G12D-expressing cancer cells, while ATP-competitive PLK1 inhibitors suppressed their growth as xenografts. Here, we identify compounds that modulate protein-protein interactions of the structurally related PLK kinases via the PBD domain, and demonstrate their utility in strategies to target mutant KRAS. Poloppin (Polo protein-protein interaction inhibitor) kills cells expressing mutant KRAS in two-dimensional or organoid cultures. An optimized analog (Poloppin-II) is effective against KRAS-expressing cancer xenografts after systemic oral administration. Notably, Poloppin sensitivity persists in cancer cells resistant to an ATP-competitive PLK1 inhibitor, and Poloppin sensitizes mutant KRAS-expressing cells to clinically used inhibitors of the MET tyrosine kinase, opening opportunities for combination therapy."
published_098,"Growth is one of the most important traits in fish aquaculture (Gjedrem, 2005). Heritability is the measure of the relative proportion of genetic versus environmental factors that determine the total variation of a specific trait. Heritability is an important genetic parameter in selective breeding programs (Gjedrem & Baranski, 2010). Specifically, narrow sense heritability, which is the proportion of total phenotypic variation due to additive genetic factors, is important in predicting how a trait will respond to selection. Heritabilities have been estimated for a number of traits in several aquaculture fish species, such as growth in the common carp (Cyprinus carpio, Vandeputte et al., 2004), and Asian seabass (Lates calcarifer, Wang et al., 2008, Domingos et al., 2013); resistance to columnaris disease in the Atlantic salmon (Salmo salar, Evenhuis, Leeds, Marancik, LaPatra, & Wiens, 2015) and others (Gjedrem, Robinson, & Rye, 2012). In aquaculture species the estimated heritabilities for growth traits range from 0 to 0.83 (Gjedrem, 1983; Gjedrem & Baranski, 2010). Heritability estimates are affected by a number of factors, such as, genetic background, genetic variation, sample size, culturing conditions and the number of generations of selection that the population has gone through (Gjedrem, 2005; Visscher, Hill, & Wray, 2008). In aquaculture species the genetic gain for growth rate is greater than 12% per generation (Gjedrem & Robinson, 2014). However, for newly emerging species, information about heritabilities for important traits and increase of growth is still limited. The Asian seabass (Lates calcarifer) is an important commercial fish in the Indo-Pacific region. It is considered an ideal aquaculture species as it is in high demand, grows rapidly and can grow in salinities ranging from fresh to sea water and thus can be raised in monsoonal areas (Jerry, 2013; Yue, Li, & Orban, 2001). According to the United Nations Food and Agricultural Organisation's, there is a recognized need for genetic selection programmes for Asian seabass to target faster growth and disease resistance. However to date little efforts have been made (Rimmer, 2006). A unique study describing significant heritabilities and high phenotypic variation in body weight and length of Asian seabass at very early stages (21 dph to 80 dph) suggested that it would be possible to improve fish growth rate through selection at an early stage (Chandra, Kailasam, Thirunavukkarasu, & Abraham, 2000). Since 2004, our laboratory, in collaboration with the Marine Aquaculture Center, Agri-Food and Veterinary Authority of Singapore, has been carrying out an Asian seabass selective breeding programme to improve growth and disease resistance using an initial broodstock composed of more than 500 adults from Malaysia, Indonesia, Thailand and Singapore (Wang et al., 2008; Yue et al., 2009). Using the growth traits from the first two batches (1704 individuals) of the first generation progeny (i.e. F1) produced from the broodstock, a genetic study was carried out focused on estimating the narrow-sense heritabilities of Asian seabass growth traits (body weight, standard length and Fulton's condition factor based on standard length) at 90 dph (days post hatch), and the correlation of each trait between 90 dph and 270 dph (Wang et al., 2008). This study concluded that the heritabilities for growth traits at 90 dph ranged from 0.15 to 0.31 and that maternal and dominance effects were negligible and that the correlation of each trait between 90 dph and 270 dph was strong. These findings supported the hypothesis that fish selection could be performed at an early age (i.e. 90 dph), hence saving maintenance costs (Wang et al., 2008). Further support to the previous findings came from a recent study on the Australian strains of Asian seabass, in which a different set of growth traits, wider range of ages and habitats and estimating the genotype by environment (G × E) effects were considered and found that G × E effects were mostly negligible (Domingos et al., 2013). Both studies used the first generation (F1) of Asian seabass raised in captivity to estimate the heritabilities of the initial broodstock (F0). It is well known that in breeding programs, heritability estimates vary (are reduced) from generation to generation due to the reduction of genetic variation (Gjedrem, 2005). It is essential to estimate heritability for a selected species in each generation to ensure enough selective response. The purpose of this study was to estimate narrow-sense heritabilities (including realised heritability) for growth traits at 90 dph and 270 dph from F1 to F2 generation of Asian seabass to facilitate future genetic improvement. To our knowledge, this is the first study that considers the second generation (i.e. F2) of selectively bred Asian seabass to estimate the heritabilities of growth traits in F1, and comparing these heritabilities to F0."
published_099,"Carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) are the three major gases leading to global warming (Lashof & Ahuja, 1990). The concentrations of CH4 and N2O in the atmosphere are much less than that of CO2, while their global warming potential (GWP) is approximately 25 and 298 times greater over a time span of 100 years, than that of CO2, respectively (Zhang et al., 2016). A man-made wetland, the rice field is considered to be a main source of CH4 and N2O emissions due to water and nutrient management practices (Akiyama, Yagi, & Yan, 2005; Hadi, Inubushi, & Yagi, 2010; Linquist et al., 2012; Zou et al., 2009). Globally, the gross CH4 emissions from rice paddies were accounted at approximately 20–40 Tg/yr, while the total N2O was assessed to be 1.7–4.8 Tg/yr from overall farmland area (Yan et al., 2009; Yao et al., 2010). China is one of the major rice producers in the world, accounting for 16% of the global rice planting area (Xu et al., 2015). Evaluations of total CH4 and N2O emissions from Chinese rice fields were 6–10 Tg/yr and 32–51 Gg/yr, respectively (Liu et al., 2015). The emissions of CH4 and N2O are comprised of production, conversion, and transportation processes. Methanogenic archaea can produce CH4 by breaking down organic matter in oxygen-poor conditions (Conrad, 2006; Huang et al., 2005). At the same time, CH4 transformed in the environment can be partly oxidized to CO2 by methanotrophs in aerobic zones (Brune, Frenzel, & Cypionka, 2000; Chen et al., 2013a). Then, it is expelled into the atmosphere mostly through plants, aerenchyma, rarely through ebullition and diffusion from the sites of production (Aulakh, Wassmann, & Rennenberg, 2001; Schutz, Seiler, & Conrad, 1989). Nitrification and denitrification processes occurring in appropriate soil regions simultaneously can produce N2O (Hu et al., 2013). Nitrification takes place where sufficient oxygen (O2) exists, while denitrification requires anaerobic conditions. It is commonly assumed that only small amounts of N2O are emitted from rice fields under waterlogged conditions, because N2O as an obligate intermediate, would be ultimately reduced to nitrogen (N2) as a result of oxides limitation (Kampschreur et al., 2009; Li et al., 2009). Furthermore, because it is easily soluble in water, N2O has more opportunities to be reduced (Tumendelger et al., 2016; Weiss & Price, 1980). N2O mainly diffuses from soil to atmosphere if there is no water layer. On the contrary, rice plants emit N2O when the soil is flooded (Yan et al., 2000). Many reports have indicated that the fluxes of CH4 and N2O from rice fields are implicated in many factors, such as fertilization, water regime, tillage, and cultivation (Cai et al., 1997; Xu et al., 2015; Zhang et al. 2014, 2016; Zhou et al., 2015). Rice-fish culture is a typical model combining planting of rice and rearing if fish in the rice field with a long history in China (Feng et al., 2016; Frei et al., 2007b; Li, 1988; Mirhaj et al., 2013; Mohanty et al., 2010; Saiful Islam, Barman, & Murshed-e-Jahan, 2015). O2 deficiency and other conditions that increase organic carbon levels are characteristic of rice-fish cultivation system, which promote the CH4 emission and depress the N2O emission (Bhattacharyya et al., 2013; Datta et al., 2009; Frei & Becker, 2005; Frei et al., 2007a). The Chinese mitten crab, Eriocheir sinensis (hereafter referred to simply as crab), is one of the most important commercial crustacean species and a popular food in Southeast Asia (Zeng et al., 2013). The total production of crab reached 796,622 metric tons in 2014 in China (Yuan et al., 2017). After the successful application of rice-fish integrated culture, farmers in northern area attempted to introduce the crabs into rice fields and gained good economic benefits (Li et al., 2007). In order to adapt to the growth cycle of rice, two models have been used in rice fields: one is rearing megalopa (June to October), the other is rearing juvenile crab (June to September). Many researches have focused on the ecology, feeding habits, yield, and food web structure in rice-crab culture, and came to the conclusion that crab rearing in rice field could improve the ecological environment and bring greater economic benefits (Guo et al., 2015; Li et al. 2007, 2013b; Xu, Ma, & Wang, 2014). However, little is known about CH4 and N2O emissions in rice-crab paddy fields. This study conducted an experiment to (1) estimate the effect of crab rearing on the amount of CH4 and N2O emitted from rice fields; (2) determine the dependence of CH4 and N2O emissions on soil and water characters in rice-crab culture system; and (3) assess the effect of the rice-crab systems on the environment in comparison with their economic benefits for the farmers."
published_100,"Basically, the rare-earth ions doped glasses have extensive applications in the field of developing up-conversion (UC) lasers, optical amplifiers, color displays, biomedical diagnostics, temperature sensors, laser remote sensing, telecommunications, atmosphere transmissions, Raman laser, eye-safe LIDAR applications, and solar concentrators, etc. [1–4]. Nowadays, many researchers show a lot of interest in the up-conversion (UC) luminescence phenomenon because of its several potential applications in bio-imaging, display monitors, solar cells, medical diagnostics and photodynamic therapy [5,6]. In the up-conversion luminescence (UCL) process (also known as anti-Stokes luminescence process) the RE ions absorp 2 or more photons of longer- wavelength and give luminescence emission in the low wavelength region via multi phonon relaxation (MPR). Among all the RE ions Tm2O3, Ho2O3, Er2O3, and Yb2O3, etc elements are predicted to act as act as an effective sensitizers to enhance the up-conversion emission because of their sufficient energy levels from NIR to the visible region. When these RE ions excited with IR light, then the intensity of visible light emission increases from 2–3 orders of magnitude. Recently, triply RE ion-doped up-conversion nanoparticles perform an important role in the field of biomedicine, especially in the early diagnosis and cancer treatment [7,8]. Due to a large number of energy levels and long-life metastable state 4I13/2 of Er2O3, these Er3+ ions are used as the most attractive active ion in a host glass matrix. Recently Li feng et al., proposed that as to increase the up-conversion photoluminescence (UCL) efficiency, the Yb3+ RE ions are always used as active ions due to the reason of large absorption around 977 nm and a beneficial energy transfer takes place between the Yb3+ ions and Er3+ ions (Yb3+→ Er3+) [1]. The spectroscopic measurements of Er3+ ions in various glasses have much practical interest because of its emission at 1.532 μm can be used for the optical amplification. The gain of NPbPEr glasses covers all the C + S+ L communication bands which are used to design and develop the EDFA (Erbium-Doped Fibre Amplifiers). Nowadays, these EDFA devices may have extensive interest in wavelength division multiplexing network systems [9]. Further, the visible UC emission of Er3+ ion at 547 nm (4S3/2 → 4I15/2) has many advantages in the field of solid-state laser and hence emission at 4S3/2 → 4I15/2 has the capability of high optical storage and green emission applications [10]. To show the required applications of RE ions, the selection of the host matrix is important. Among these glass matrices, phosphate glasses have recently promising applications in both academic and industrial purposes. The addition of PbO to this phosphate glasses has more advantages because the structural role of PbO is unique and plays a dual role if Pb–O is ionically named as a network modifier and if Pb–O is covalent then it is called as glass former in glass matrices [11]. Also, the inclusion of lead to phosphate glass matrices may improve the chemical durability of glasses [12]. Nowadays, many researchers show interest in alkali-sulfo phosphate glasses because of its relatively high electrical conductivity (purely ionic); these glasses have extensive applications in micro-batteries, smart cards, medical applications [13]. From the above facts, the present study reports the XRD, optical properties, visible and NIR emission, up-conversion emission, decay properties, CIE color coordinates, absorption (σabs(λ)) and emission (σemi(λ)) cross-sections along with the gain coefficient G(λ,γ) of Er3+ ion-doped NPbP glasses. These results are examined at different concentrations of Er2O3 and compared with other reported glasses."
